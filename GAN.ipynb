{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771ada7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:37.877630Z",
     "iopub.status.busy": "2025-04-28T07:32:37.877323Z",
     "iopub.status.idle": "2025-04-28T07:32:44.444128Z",
     "shell.execute_reply": "2025-04-28T07:32:44.443083Z"
    },
    "papermill": {
     "duration": 6.575318,
     "end_time": "2025-04-28T07:32:44.447028",
     "exception": false,
     "start_time": "2025-04-28T07:32:37.871710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from math import log10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4bb691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:44.457360Z",
     "iopub.status.busy": "2025-04-28T07:32:44.456891Z",
     "iopub.status.idle": "2025-04-28T07:32:44.563164Z",
     "shell.execute_reply": "2025-04-28T07:32:44.562050Z"
    },
    "papermill": {
     "duration": 0.113328,
     "end_time": "2025-04-28T07:32:44.564996",
     "exception": false,
     "start_time": "2025-04-28T07:32:44.451668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "#check cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using CUDA!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available. Using CPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097c9f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:44.574450Z",
     "iopub.status.busy": "2025-04-28T07:32:44.574214Z",
     "iopub.status.idle": "2025-04-28T07:32:44.579041Z",
     "shell.execute_reply": "2025-04-28T07:32:44.578150Z"
    },
    "papermill": {
     "duration": 0.01215,
     "end_time": "2025-04-28T07:32:44.581247",
     "exception": false,
     "start_time": "2025-04-28T07:32:44.569097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#metrics\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    return 20 * log10(1.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(img1, img2, multichannel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7cd73b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:44.591748Z",
     "iopub.status.busy": "2025-04-28T07:32:44.591347Z",
     "iopub.status.idle": "2025-04-28T07:32:44.599957Z",
     "shell.execute_reply": "2025-04-28T07:32:44.599098Z"
    },
    "papermill": {
     "duration": 0.015868,
     "end_time": "2025-04-28T07:32:44.602169",
     "exception": false,
     "start_time": "2025-04-28T07:32:44.586301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path, device):\n",
    "    model.load_state_dict(torch.load(path, map_location=device,weights_only=True))\n",
    "    return model\n",
    "\n",
    "def display_images(images, titles=None):\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, img in enumerate(images):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        if titles:\n",
    "            plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28764492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:44.627549Z",
     "iopub.status.busy": "2025-04-28T07:32:44.626936Z",
     "iopub.status.idle": "2025-04-28T07:32:44.637732Z",
     "shell.execute_reply": "2025-04-28T07:32:44.636948Z"
    },
    "papermill": {
     "duration": 0.017489,
     "end_time": "2025-04-28T07:32:44.639579",
     "exception": false,
     "start_time": "2025-04-28T07:32:44.622090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeblurDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((448, 448)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.blur_paths = []\n",
    "        self.sharp_paths = []\n",
    "\n",
    "        for subdir, dirs, files in os.walk(root_dir):\n",
    "            if os.path.basename(subdir) == \"blur\":\n",
    "                for fname in sorted(os.listdir(subdir)):\n",
    "                    blur_path = os.path.join(subdir, fname)\n",
    "                    sharp_path = blur_path.replace('blur', 'sharp')\n",
    "                    if os.path.exists(sharp_path):\n",
    "                        self.blur_paths.append(blur_path)\n",
    "                        self.sharp_paths.append(sharp_path)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.blur_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        blur_img = Image.open(self.blur_paths[idx]).convert(\"RGB\")\n",
    "        sharp_img = Image.open(self.sharp_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            blur_img = self.transform(blur_img)\n",
    "            sharp_img = self.transform(sharp_img)\n",
    "\n",
    "        return blur_img, sharp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3901bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:44.648803Z",
     "iopub.status.busy": "2025-04-28T07:32:44.648434Z",
     "iopub.status.idle": "2025-04-28T07:32:44.662975Z",
     "shell.execute_reply": "2025-04-28T07:32:44.662173Z"
    },
    "papermill": {
     "duration": 0.021312,
     "end_time": "2025-04-28T07:32:44.664739",
     "exception": false,
     "start_time": "2025-04-28T07:32:44.643427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_residual_blocks=6):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            *[ResidualBlock(256) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        bottleneck = self.bottleneck(encoded)\n",
    "        decoded = self.decoder(bottleneck)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652148d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:44.674368Z",
     "iopub.status.busy": "2025-04-28T07:32:44.674040Z",
     "iopub.status.idle": "2025-04-28T07:32:44.683912Z",
     "shell.execute_reply": "2025-04-28T07:32:44.683014Z"
    },
    "papermill": {
     "duration": 0.017243,
     "end_time": "2025-04-28T07:32:44.686143",
     "exception": false,
     "start_time": "2025-04-28T07:32:44.668900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)  \n",
    "        return validity.view(-1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5da6175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:44.723613Z",
     "iopub.status.busy": "2025-04-28T07:32:44.723294Z",
     "iopub.status.idle": "2025-04-28T07:32:44.727232Z",
     "shell.execute_reply": "2025-04-28T07:32:44.726439Z"
    },
    "papermill": {
     "duration": 0.013516,
     "end_time": "2025-04-28T07:32:44.729509",
     "exception": false,
     "start_time": "2025-04-28T07:32:44.715993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5e786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:32:44.745475Z",
     "iopub.status.busy": "2025-04-28T07:32:44.745202Z",
     "iopub.status.idle": "2025-04-28T18:43:12.432523Z",
     "shell.execute_reply": "2025-04-28T18:43:12.431535Z"
    },
    "papermill": {
     "duration": 40227.697827,
     "end_time": "2025-04-28T18:43:12.434851",
     "exception": false,
     "start_time": "2025-04-28T07:32:44.737024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/95], Step [1/132], D Loss: 0.7181, G Loss: 46.9652\n",
      "Epoch [1/95], Step [6/132], D Loss: 0.7024, G Loss: 11.1761\n",
      "Epoch [1/95], Step [11/132], D Loss: 0.6907, G Loss: 9.7001\n",
      "Epoch [1/95], Step [16/132], D Loss: 0.6839, G Loss: 9.1794\n",
      "Epoch [1/95], Step [21/132], D Loss: 0.6781, G Loss: 6.7094\n",
      "Epoch [1/95], Step [26/132], D Loss: 0.6684, G Loss: 6.8438\n",
      "Epoch [1/95], Step [31/132], D Loss: 0.6406, G Loss: 6.4299\n",
      "Epoch [1/95], Step [36/132], D Loss: 0.6667, G Loss: 6.2258\n",
      "Epoch [1/95], Step [41/132], D Loss: 0.5764, G Loss: 6.6782\n",
      "Epoch [1/95], Step [46/132], D Loss: 0.6304, G Loss: 8.0384\n",
      "Epoch [1/95], Step [51/132], D Loss: 0.6444, G Loss: 10.2530\n",
      "Epoch [1/95], Step [56/132], D Loss: 0.6188, G Loss: 6.1273\n",
      "Epoch [1/95], Step [61/132], D Loss: 0.6223, G Loss: 6.5601\n",
      "Epoch [1/95], Step [66/132], D Loss: 0.6731, G Loss: 6.4315\n",
      "Epoch [1/95], Step [71/132], D Loss: 0.6560, G Loss: 5.8938\n",
      "Epoch [1/95], Step [76/132], D Loss: 0.6465, G Loss: 6.0217\n",
      "Epoch [1/95], Step [81/132], D Loss: 0.6814, G Loss: 7.4404\n",
      "Epoch [1/95], Step [86/132], D Loss: 0.6797, G Loss: 6.5187\n",
      "Epoch [1/95], Step [91/132], D Loss: 0.6403, G Loss: 5.7050\n",
      "Epoch [1/95], Step [96/132], D Loss: 0.6593, G Loss: 8.3503\n",
      "Epoch [1/95], Step [101/132], D Loss: 0.6535, G Loss: 6.8992\n",
      "Epoch [1/95], Step [106/132], D Loss: 0.6524, G Loss: 5.4573\n",
      "Epoch [1/95], Step [111/132], D Loss: 0.6494, G Loss: 6.9991\n",
      "Epoch [1/95], Step [116/132], D Loss: 0.6607, G Loss: 7.8499\n",
      "Epoch [1/95], Step [121/132], D Loss: 0.6268, G Loss: 7.0671\n",
      "Epoch [1/95], Step [126/132], D Loss: 0.6554, G Loss: 6.2288\n",
      "Epoch [1/95], Step [131/132], D Loss: 0.6671, G Loss: 5.6358\n",
      "Epoch [1/95], Training Loss: 7.9315\n",
      "1 epoch time: 7.783729767799377\n",
      "Epoch [2/95], Step [1/132], D Loss: 0.6571, G Loss: 5.5384\n",
      "Epoch [2/95], Step [6/132], D Loss: 0.6407, G Loss: 5.5622\n",
      "Epoch [2/95], Step [11/132], D Loss: 0.6532, G Loss: 5.4727\n",
      "Epoch [2/95], Step [16/132], D Loss: 0.6390, G Loss: 6.3997\n",
      "Epoch [2/95], Step [21/132], D Loss: 0.6593, G Loss: 6.9591\n",
      "Epoch [2/95], Step [26/132], D Loss: 0.6514, G Loss: 5.7816\n",
      "Epoch [2/95], Step [31/132], D Loss: 0.7070, G Loss: 7.7291\n",
      "Epoch [2/95], Step [36/132], D Loss: 0.6109, G Loss: 5.2368\n",
      "Epoch [2/95], Step [41/132], D Loss: 0.6731, G Loss: 5.9051\n",
      "Epoch [2/95], Step [46/132], D Loss: 0.6295, G Loss: 6.2324\n",
      "Epoch [2/95], Step [51/132], D Loss: 0.6299, G Loss: 5.1487\n",
      "Epoch [2/95], Step [56/132], D Loss: 0.6225, G Loss: 8.4972\n",
      "Epoch [2/95], Step [61/132], D Loss: 0.6030, G Loss: 5.7489\n",
      "Epoch [2/95], Step [66/132], D Loss: 0.7297, G Loss: 7.2019\n",
      "Epoch [2/95], Step [71/132], D Loss: 0.5818, G Loss: 5.0084\n",
      "Epoch [2/95], Step [76/132], D Loss: 0.6331, G Loss: 4.7283\n",
      "Epoch [2/95], Step [81/132], D Loss: 0.6312, G Loss: 5.2854\n",
      "Epoch [2/95], Step [86/132], D Loss: 0.6162, G Loss: 6.2349\n",
      "Epoch [2/95], Step [91/132], D Loss: 0.5356, G Loss: 4.9628\n",
      "Epoch [2/95], Step [96/132], D Loss: 0.5358, G Loss: 5.7301\n",
      "Epoch [2/95], Step [101/132], D Loss: 0.6897, G Loss: 5.9179\n",
      "Epoch [2/95], Step [106/132], D Loss: 0.6807, G Loss: 5.6652\n",
      "Epoch [2/95], Step [111/132], D Loss: 0.6692, G Loss: 5.3665\n",
      "Epoch [2/95], Step [116/132], D Loss: 0.6606, G Loss: 5.4493\n",
      "Epoch [2/95], Step [121/132], D Loss: 0.5738, G Loss: 5.3213\n",
      "Epoch [2/95], Step [126/132], D Loss: 0.5627, G Loss: 5.3732\n",
      "Epoch [2/95], Step [131/132], D Loss: 0.5184, G Loss: 4.9974\n",
      "Epoch [2/95], Training Loss: 6.0022\n",
      "1 epoch time: 7.047238405545553\n",
      "Epoch [3/95], Step [1/132], D Loss: 0.5414, G Loss: 5.8692\n",
      "Epoch [3/95], Step [6/132], D Loss: 0.5085, G Loss: 5.3988\n",
      "Epoch [3/95], Step [11/132], D Loss: 0.5082, G Loss: 5.9722\n",
      "Epoch [3/95], Step [16/132], D Loss: 0.6728, G Loss: 7.3354\n",
      "Epoch [3/95], Step [21/132], D Loss: 0.6610, G Loss: 6.2423\n",
      "Epoch [3/95], Step [26/132], D Loss: 0.5730, G Loss: 5.1373\n",
      "Epoch [3/95], Step [31/132], D Loss: 0.7220, G Loss: 6.3466\n",
      "Epoch [3/95], Step [36/132], D Loss: 0.5834, G Loss: 4.9962\n",
      "Epoch [3/95], Step [41/132], D Loss: 0.5248, G Loss: 5.8460\n",
      "Epoch [3/95], Step [46/132], D Loss: 0.5352, G Loss: 4.9559\n",
      "Epoch [3/95], Step [51/132], D Loss: 0.5063, G Loss: 5.1034\n",
      "Epoch [3/95], Step [56/132], D Loss: 0.5059, G Loss: 4.7469\n",
      "Epoch [3/95], Step [61/132], D Loss: 0.5127, G Loss: 7.1612\n",
      "Epoch [3/95], Step [66/132], D Loss: 0.5203, G Loss: 6.0947\n",
      "Epoch [3/95], Step [71/132], D Loss: 0.5738, G Loss: 5.5132\n",
      "Epoch [3/95], Step [76/132], D Loss: 0.5133, G Loss: 6.3546\n",
      "Epoch [3/95], Step [81/132], D Loss: 0.5040, G Loss: 7.2695\n",
      "Epoch [3/95], Step [86/132], D Loss: 0.5039, G Loss: 4.8961\n",
      "Epoch [3/95], Step [91/132], D Loss: 0.5040, G Loss: 4.6609\n",
      "Epoch [3/95], Step [96/132], D Loss: 0.5040, G Loss: 7.2135\n",
      "Epoch [3/95], Step [101/132], D Loss: 0.5040, G Loss: 5.1150\n",
      "Epoch [3/95], Step [106/132], D Loss: 0.5037, G Loss: 4.6108\n",
      "Epoch [3/95], Step [111/132], D Loss: 0.5037, G Loss: 5.7646\n",
      "Epoch [3/95], Step [116/132], D Loss: 0.5037, G Loss: 5.7188\n",
      "Epoch [3/95], Step [121/132], D Loss: 0.5040, G Loss: 6.0160\n",
      "Epoch [3/95], Step [126/132], D Loss: 0.5036, G Loss: 4.9599\n",
      "Epoch [3/95], Step [131/132], D Loss: 0.5038, G Loss: 4.7355\n",
      "Epoch [3/95], Training Loss: 5.4912\n",
      "1 epoch time: 7.043442678451538\n",
      "Epoch [4/95], Step [1/132], D Loss: 0.5039, G Loss: 5.3362\n",
      "Epoch [4/95], Step [6/132], D Loss: 0.5038, G Loss: 5.3122\n",
      "Epoch [4/95], Step [11/132], D Loss: 0.5035, G Loss: 5.9170\n",
      "Epoch [4/95], Step [16/132], D Loss: 0.5036, G Loss: 5.6798\n",
      "Epoch [4/95], Step [21/132], D Loss: 0.5037, G Loss: 6.2401\n",
      "Epoch [4/95], Step [26/132], D Loss: 0.5048, G Loss: 4.5650\n",
      "Epoch [4/95], Step [31/132], D Loss: 0.5036, G Loss: 5.8561\n",
      "Epoch [4/95], Step [36/132], D Loss: 0.5041, G Loss: 4.7170\n",
      "Epoch [4/95], Step [41/132], D Loss: 0.5036, G Loss: 8.3803\n",
      "Epoch [4/95], Step [46/132], D Loss: 0.5034, G Loss: 5.0025\n",
      "Epoch [4/95], Step [51/132], D Loss: 0.5036, G Loss: 5.6604\n",
      "Epoch [4/95], Step [56/132], D Loss: 0.5035, G Loss: 5.0622\n",
      "Epoch [4/95], Step [61/132], D Loss: 0.5037, G Loss: 5.7541\n",
      "Epoch [4/95], Step [66/132], D Loss: 0.5132, G Loss: 6.2238\n",
      "Epoch [4/95], Step [71/132], D Loss: 0.5036, G Loss: 5.0406\n",
      "Epoch [4/95], Step [76/132], D Loss: 0.5035, G Loss: 5.1554\n",
      "Epoch [4/95], Step [81/132], D Loss: 0.5039, G Loss: 7.6101\n",
      "Epoch [4/95], Step [86/132], D Loss: 0.5033, G Loss: 4.8125\n",
      "Epoch [4/95], Step [91/132], D Loss: 0.5035, G Loss: 4.7018\n",
      "Epoch [4/95], Step [96/132], D Loss: 0.5034, G Loss: 5.6160\n",
      "Epoch [4/95], Step [101/132], D Loss: 0.5034, G Loss: 4.9617\n",
      "Epoch [4/95], Step [106/132], D Loss: 0.5037, G Loss: 5.7261\n",
      "Epoch [4/95], Step [111/132], D Loss: 0.5034, G Loss: 4.3032\n",
      "Epoch [4/95], Step [116/132], D Loss: 0.5034, G Loss: 4.5687\n",
      "Epoch [4/95], Step [121/132], D Loss: 0.5034, G Loss: 4.8530\n",
      "Epoch [4/95], Step [126/132], D Loss: 0.5033, G Loss: 5.9048\n",
      "Epoch [4/95], Step [131/132], D Loss: 0.5034, G Loss: 5.0890\n",
      "Epoch [4/95], Training Loss: 5.4368\n",
      "1 epoch time: 7.029470570882162\n",
      "Epoch [5/95], Step [1/132], D Loss: 0.5033, G Loss: 5.1345\n",
      "Epoch [5/95], Step [6/132], D Loss: 0.5034, G Loss: 5.0051\n",
      "Epoch [5/95], Step [11/132], D Loss: 0.5034, G Loss: 4.6889\n",
      "Epoch [5/95], Step [16/132], D Loss: 0.5034, G Loss: 4.6805\n",
      "Epoch [5/95], Step [21/132], D Loss: 0.5053, G Loss: 5.6808\n",
      "Epoch [5/95], Step [26/132], D Loss: 0.5037, G Loss: 4.8564\n",
      "Epoch [5/95], Step [31/132], D Loss: 0.5034, G Loss: 5.9073\n",
      "Epoch [5/95], Step [36/132], D Loss: 0.5034, G Loss: 4.2935\n",
      "Epoch [5/95], Step [41/132], D Loss: 0.5036, G Loss: 5.8851\n",
      "Epoch [5/95], Step [46/132], D Loss: 0.5034, G Loss: 5.1068\n",
      "Epoch [5/95], Step [51/132], D Loss: 0.5034, G Loss: 4.4113\n",
      "Epoch [5/95], Step [56/132], D Loss: 0.5034, G Loss: 5.1410\n",
      "Epoch [5/95], Step [61/132], D Loss: 0.5033, G Loss: 5.3305\n",
      "Epoch [5/95], Step [66/132], D Loss: 0.5034, G Loss: 5.1453\n",
      "Epoch [5/95], Step [71/132], D Loss: 0.5033, G Loss: 5.0091\n",
      "Epoch [5/95], Step [76/132], D Loss: 0.5035, G Loss: 4.8209\n",
      "Epoch [5/95], Step [81/132], D Loss: 0.5034, G Loss: 6.4202\n",
      "Epoch [5/95], Step [86/132], D Loss: 0.5040, G Loss: 5.6741\n",
      "Epoch [5/95], Step [91/132], D Loss: 0.5034, G Loss: 5.0469\n",
      "Epoch [5/95], Step [96/132], D Loss: 0.5033, G Loss: 5.3496\n",
      "Epoch [5/95], Step [101/132], D Loss: 0.5034, G Loss: 4.6888\n",
      "Epoch [5/95], Step [106/132], D Loss: 0.5035, G Loss: 6.0846\n",
      "Epoch [5/95], Step [111/132], D Loss: 0.5033, G Loss: 5.7876\n",
      "Epoch [5/95], Step [116/132], D Loss: 0.5034, G Loss: 5.8643\n",
      "Epoch [5/95], Step [121/132], D Loss: 0.5033, G Loss: 4.7210\n",
      "Epoch [5/95], Step [126/132], D Loss: 0.5034, G Loss: 5.2211\n",
      "Epoch [5/95], Step [131/132], D Loss: 0.5033, G Loss: 5.0191\n",
      "Epoch [5/95], Training Loss: 5.2220\n",
      "1 epoch time: 7.031790781021118\n",
      "Epoch [6/95], Step [1/132], D Loss: 0.5033, G Loss: 5.1758\n",
      "Epoch [6/95], Step [6/132], D Loss: 0.5034, G Loss: 5.7386\n",
      "Epoch [6/95], Step [11/132], D Loss: 0.5033, G Loss: 5.8545\n",
      "Epoch [6/95], Step [16/132], D Loss: 0.5034, G Loss: 5.4306\n",
      "Epoch [6/95], Step [21/132], D Loss: 0.5033, G Loss: 5.0188\n",
      "Epoch [6/95], Step [26/132], D Loss: 0.5033, G Loss: 6.6853\n",
      "Epoch [6/95], Step [31/132], D Loss: 0.5033, G Loss: 5.1202\n",
      "Epoch [6/95], Step [36/132], D Loss: 0.5033, G Loss: 5.8262\n",
      "Epoch [6/95], Step [41/132], D Loss: 0.5033, G Loss: 5.2044\n",
      "Epoch [6/95], Step [46/132], D Loss: 0.5034, G Loss: 6.2825\n",
      "Epoch [6/95], Step [51/132], D Loss: 0.5033, G Loss: 5.9120\n",
      "Epoch [6/95], Step [56/132], D Loss: 0.5034, G Loss: 5.3512\n",
      "Epoch [6/95], Step [61/132], D Loss: 0.5063, G Loss: 4.5517\n",
      "Epoch [6/95], Step [66/132], D Loss: 0.5051, G Loss: 4.8312\n",
      "Epoch [6/95], Step [71/132], D Loss: 0.5033, G Loss: 4.6093\n",
      "Epoch [6/95], Step [76/132], D Loss: 0.5033, G Loss: 4.8047\n",
      "Epoch [6/95], Step [81/132], D Loss: 0.5033, G Loss: 4.1912\n",
      "Epoch [6/95], Step [86/132], D Loss: 0.5035, G Loss: 4.4417\n",
      "Epoch [6/95], Step [91/132], D Loss: 0.5033, G Loss: 5.2964\n",
      "Epoch [6/95], Step [96/132], D Loss: 0.5033, G Loss: 5.3697\n",
      "Epoch [6/95], Step [101/132], D Loss: 0.5033, G Loss: 4.3476\n",
      "Epoch [6/95], Step [106/132], D Loss: 0.5033, G Loss: 4.7797\n",
      "Epoch [6/95], Step [111/132], D Loss: 0.5033, G Loss: 5.4014\n",
      "Epoch [6/95], Step [116/132], D Loss: 0.5033, G Loss: 4.3392\n",
      "Epoch [6/95], Step [121/132], D Loss: 0.5033, G Loss: 4.4194\n",
      "Epoch [6/95], Step [126/132], D Loss: 0.5034, G Loss: 5.3159\n",
      "Epoch [6/95], Step [131/132], D Loss: 0.5033, G Loss: 4.7757\n",
      "Epoch [6/95], Training Loss: 5.1491\n",
      "1 epoch time: 7.047627047697703\n",
      "Epoch [7/95], Step [1/132], D Loss: 0.5035, G Loss: 6.0338\n",
      "Epoch [7/95], Step [6/132], D Loss: 0.5033, G Loss: 5.0320\n",
      "Epoch [7/95], Step [11/132], D Loss: 0.5033, G Loss: 3.9460\n",
      "Epoch [7/95], Step [16/132], D Loss: 0.5033, G Loss: 4.4286\n",
      "Epoch [7/95], Step [21/132], D Loss: 0.5033, G Loss: 5.3843\n",
      "Epoch [7/95], Step [26/132], D Loss: 0.5037, G Loss: 4.0436\n",
      "Epoch [7/95], Step [31/132], D Loss: 0.5034, G Loss: 4.0422\n",
      "Epoch [7/95], Step [36/132], D Loss: 0.5034, G Loss: 4.1759\n",
      "Epoch [7/95], Step [41/132], D Loss: 0.5033, G Loss: 3.9319\n",
      "Epoch [7/95], Step [46/132], D Loss: 0.5033, G Loss: 4.6355\n",
      "Epoch [7/95], Step [51/132], D Loss: 0.5033, G Loss: 4.8207\n",
      "Epoch [7/95], Step [56/132], D Loss: 0.5035, G Loss: 4.3395\n",
      "Epoch [7/95], Step [61/132], D Loss: 0.5035, G Loss: 6.8788\n",
      "Epoch [7/95], Step [66/132], D Loss: 0.5033, G Loss: 4.4169\n",
      "Epoch [7/95], Step [71/132], D Loss: 0.5034, G Loss: 4.2650\n",
      "Epoch [7/95], Step [76/132], D Loss: 0.5033, G Loss: 4.8773\n",
      "Epoch [7/95], Step [81/132], D Loss: 0.5033, G Loss: 4.4862\n",
      "Epoch [7/95], Step [86/132], D Loss: 0.5033, G Loss: 5.2818\n",
      "Epoch [7/95], Step [91/132], D Loss: 0.5033, G Loss: 4.6965\n",
      "Epoch [7/95], Step [96/132], D Loss: 0.5033, G Loss: 4.7715\n",
      "Epoch [7/95], Step [101/132], D Loss: 0.5033, G Loss: 4.2622\n",
      "Epoch [7/95], Step [106/132], D Loss: 0.5034, G Loss: 5.0072\n",
      "Epoch [7/95], Step [111/132], D Loss: 0.5033, G Loss: 4.5912\n",
      "Epoch [7/95], Step [116/132], D Loss: 0.5206, G Loss: 4.6840\n",
      "Epoch [7/95], Step [121/132], D Loss: 0.7682, G Loss: 4.3249\n",
      "Epoch [7/95], Step [126/132], D Loss: 0.7463, G Loss: 3.8215\n",
      "Epoch [7/95], Step [131/132], D Loss: 0.6451, G Loss: 4.9807\n",
      "Epoch [7/95], Training Loss: 4.8374\n",
      "1 epoch time: 7.039329087734222\n",
      "Epoch [8/95], Step [1/132], D Loss: 0.7103, G Loss: 4.8172\n",
      "Epoch [8/95], Step [6/132], D Loss: 0.6601, G Loss: 5.1147\n",
      "Epoch [8/95], Step [11/132], D Loss: 0.7017, G Loss: 5.1004\n",
      "Epoch [8/95], Step [16/132], D Loss: 0.6707, G Loss: 4.7253\n",
      "Epoch [8/95], Step [21/132], D Loss: 0.6038, G Loss: 4.8658\n",
      "Epoch [8/95], Step [26/132], D Loss: 0.5073, G Loss: 4.6713\n",
      "Epoch [8/95], Step [31/132], D Loss: 0.8105, G Loss: 4.0713\n",
      "Epoch [8/95], Step [36/132], D Loss: 0.7232, G Loss: 4.7901\n",
      "Epoch [8/95], Step [41/132], D Loss: 0.5318, G Loss: 5.4921\n",
      "Epoch [8/95], Step [46/132], D Loss: 0.6658, G Loss: 4.9044\n",
      "Epoch [8/95], Step [51/132], D Loss: 0.5982, G Loss: 4.8437\n",
      "Epoch [8/95], Step [56/132], D Loss: 0.6238, G Loss: 4.5174\n",
      "Epoch [8/95], Step [61/132], D Loss: 0.6235, G Loss: 5.1294\n",
      "Epoch [8/95], Step [66/132], D Loss: 0.8086, G Loss: 5.1456\n",
      "Epoch [8/95], Step [71/132], D Loss: 0.6328, G Loss: 4.6632\n",
      "Epoch [8/95], Step [76/132], D Loss: 0.6480, G Loss: 4.6772\n",
      "Epoch [8/95], Step [81/132], D Loss: 0.5215, G Loss: 4.1159\n",
      "Epoch [8/95], Step [86/132], D Loss: 0.6675, G Loss: 4.7274\n",
      "Epoch [8/95], Step [91/132], D Loss: 0.7164, G Loss: 5.8371\n",
      "Epoch [8/95], Step [96/132], D Loss: 0.7611, G Loss: 4.2792\n",
      "Epoch [8/95], Step [101/132], D Loss: 0.6061, G Loss: 4.8947\n",
      "Epoch [8/95], Step [106/132], D Loss: 0.5395, G Loss: 5.3512\n",
      "Epoch [8/95], Step [111/132], D Loss: 0.6245, G Loss: 4.7537\n",
      "Epoch [8/95], Step [116/132], D Loss: 0.5088, G Loss: 5.1055\n",
      "Epoch [8/95], Step [121/132], D Loss: 0.5034, G Loss: 5.0244\n",
      "Epoch [8/95], Step [126/132], D Loss: 0.5034, G Loss: 4.5361\n",
      "Epoch [8/95], Step [131/132], D Loss: 0.5034, G Loss: 4.8896\n",
      "Epoch [8/95], Training Loss: 4.8129\n",
      "1 epoch time: 7.020547072092692\n",
      "Epoch [9/95], Step [1/132], D Loss: 0.5033, G Loss: 4.4192\n",
      "Epoch [9/95], Step [6/132], D Loss: 0.5034, G Loss: 4.8139\n",
      "Epoch [9/95], Step [11/132], D Loss: 0.5034, G Loss: 4.2967\n",
      "Epoch [9/95], Step [16/132], D Loss: 0.5033, G Loss: 4.6719\n",
      "Epoch [9/95], Step [21/132], D Loss: 0.5037, G Loss: 4.1513\n",
      "Epoch [9/95], Step [26/132], D Loss: 0.5033, G Loss: 5.2233\n",
      "Epoch [9/95], Step [31/132], D Loss: 0.5034, G Loss: 5.1798\n",
      "Epoch [9/95], Step [36/132], D Loss: 0.5037, G Loss: 4.9660\n",
      "Epoch [9/95], Step [41/132], D Loss: 0.5033, G Loss: 4.4717\n",
      "Epoch [9/95], Step [46/132], D Loss: 0.5032, G Loss: 5.1526\n",
      "Epoch [9/95], Step [51/132], D Loss: 0.5033, G Loss: 4.2047\n",
      "Epoch [9/95], Step [56/132], D Loss: 0.5032, G Loss: 4.4424\n",
      "Epoch [9/95], Step [61/132], D Loss: 0.5341, G Loss: 4.2147\n",
      "Epoch [9/95], Step [66/132], D Loss: 0.5034, G Loss: 4.6042\n",
      "Epoch [9/95], Step [71/132], D Loss: 0.5033, G Loss: 4.8976\n",
      "Epoch [9/95], Step [76/132], D Loss: 0.5032, G Loss: 5.2178\n",
      "Epoch [9/95], Step [81/132], D Loss: 0.5032, G Loss: 5.1264\n",
      "Epoch [9/95], Step [86/132], D Loss: 0.5040, G Loss: 4.7286\n",
      "Epoch [9/95], Step [91/132], D Loss: 0.5032, G Loss: 6.3537\n",
      "Epoch [9/95], Step [96/132], D Loss: 0.5033, G Loss: 5.2860\n",
      "Epoch [9/95], Step [101/132], D Loss: 0.5033, G Loss: 4.2057\n",
      "Epoch [9/95], Step [106/132], D Loss: 0.5037, G Loss: 4.7759\n",
      "Epoch [9/95], Step [111/132], D Loss: 0.5033, G Loss: 4.4176\n",
      "Epoch [9/95], Step [116/132], D Loss: 0.5033, G Loss: 4.8098\n",
      "Epoch [9/95], Step [121/132], D Loss: 0.5040, G Loss: 4.1122\n",
      "Epoch [9/95], Step [126/132], D Loss: 0.5033, G Loss: 4.8678\n",
      "Epoch [9/95], Step [131/132], D Loss: 0.5033, G Loss: 4.8249\n",
      "Epoch [9/95], Training Loss: 4.7466\n",
      "1 epoch time: 7.081000522772471\n",
      "Epoch [10/95], Step [1/132], D Loss: 0.5033, G Loss: 3.8268\n",
      "Epoch [10/95], Step [6/132], D Loss: 0.5058, G Loss: 5.0416\n",
      "Epoch [10/95], Step [11/132], D Loss: 0.7010, G Loss: 5.0161\n",
      "Epoch [10/95], Step [16/132], D Loss: 0.6235, G Loss: 4.5442\n",
      "Epoch [10/95], Step [21/132], D Loss: 0.6051, G Loss: 5.1834\n",
      "Epoch [10/95], Step [26/132], D Loss: 0.5099, G Loss: 4.0046\n",
      "Epoch [10/95], Step [31/132], D Loss: 0.5034, G Loss: 4.5480\n",
      "Epoch [10/95], Step [36/132], D Loss: 0.5042, G Loss: 4.6660\n",
      "Epoch [10/95], Step [41/132], D Loss: 0.5034, G Loss: 5.2627\n",
      "Epoch [10/95], Step [46/132], D Loss: 0.5033, G Loss: 5.1405\n",
      "Epoch [10/95], Step [51/132], D Loss: 0.5034, G Loss: 3.8739\n",
      "Epoch [10/95], Step [56/132], D Loss: 0.5033, G Loss: 5.0634\n",
      "Epoch [10/95], Step [61/132], D Loss: 0.5033, G Loss: 4.7991\n",
      "Epoch [10/95], Step [66/132], D Loss: 0.5033, G Loss: 5.4138\n",
      "Epoch [10/95], Step [71/132], D Loss: 0.5032, G Loss: 4.5441\n",
      "Epoch [10/95], Step [76/132], D Loss: 0.5033, G Loss: 4.5349\n",
      "Epoch [10/95], Step [81/132], D Loss: 0.5047, G Loss: 4.3293\n",
      "Epoch [10/95], Step [86/132], D Loss: 0.5033, G Loss: 4.1941\n",
      "Epoch [10/95], Step [91/132], D Loss: 0.5032, G Loss: 4.4598\n",
      "Epoch [10/95], Step [96/132], D Loss: 0.5033, G Loss: 4.5812\n",
      "Epoch [10/95], Step [101/132], D Loss: 0.5033, G Loss: 4.7494\n",
      "Epoch [10/95], Step [106/132], D Loss: 0.5034, G Loss: 4.8764\n",
      "Epoch [10/95], Step [111/132], D Loss: 0.5039, G Loss: 5.0525\n",
      "Epoch [10/95], Step [116/132], D Loss: 0.5032, G Loss: 5.5675\n",
      "Epoch [10/95], Step [121/132], D Loss: 0.5033, G Loss: 4.1249\n",
      "Epoch [10/95], Step [126/132], D Loss: 0.5033, G Loss: 4.0416\n",
      "Epoch [10/95], Step [131/132], D Loss: 0.5033, G Loss: 4.3411\n",
      "Epoch [10/95], Training Loss: 4.6087\n",
      "1 epoch time: 7.035286796092987\n",
      "Epoch [11/95], Step [1/132], D Loss: 0.5054, G Loss: 5.4453\n",
      "Epoch [11/95], Step [6/132], D Loss: 0.5032, G Loss: 5.2162\n",
      "Epoch [11/95], Step [11/132], D Loss: 0.5032, G Loss: 4.6267\n",
      "Epoch [11/95], Step [16/132], D Loss: 0.5032, G Loss: 5.5583\n",
      "Epoch [11/95], Step [21/132], D Loss: 0.5032, G Loss: 4.7838\n",
      "Epoch [11/95], Step [26/132], D Loss: 0.5032, G Loss: 6.3815\n",
      "Epoch [11/95], Step [31/132], D Loss: 0.5032, G Loss: 4.2702\n",
      "Epoch [11/95], Step [36/132], D Loss: 0.5032, G Loss: 3.9811\n",
      "Epoch [11/95], Step [41/132], D Loss: 0.5035, G Loss: 5.2401\n",
      "Epoch [11/95], Step [46/132], D Loss: 0.5033, G Loss: 4.9066\n",
      "Epoch [11/95], Step [51/132], D Loss: 0.5033, G Loss: 5.1514\n",
      "Epoch [11/95], Step [56/132], D Loss: 0.5032, G Loss: 5.4024\n",
      "Epoch [11/95], Step [61/132], D Loss: 0.5034, G Loss: 4.8771\n",
      "Epoch [11/95], Step [66/132], D Loss: 0.5032, G Loss: 4.5585\n",
      "Epoch [11/95], Step [71/132], D Loss: 0.5033, G Loss: 3.9212\n",
      "Epoch [11/95], Step [76/132], D Loss: 0.5034, G Loss: 4.3471\n",
      "Epoch [11/95], Step [81/132], D Loss: 0.5033, G Loss: 5.1616\n",
      "Epoch [11/95], Step [86/132], D Loss: 0.5032, G Loss: 4.7819\n",
      "Epoch [11/95], Step [91/132], D Loss: 0.5032, G Loss: 4.8432\n",
      "Epoch [11/95], Step [96/132], D Loss: 0.5032, G Loss: 3.8880\n",
      "Epoch [11/95], Step [101/132], D Loss: 0.5033, G Loss: 4.0899\n",
      "Epoch [11/95], Step [106/132], D Loss: 0.5032, G Loss: 3.9355\n",
      "Epoch [11/95], Step [111/132], D Loss: 0.5033, G Loss: 4.1605\n",
      "Epoch [11/95], Step [116/132], D Loss: 0.5032, G Loss: 5.0458\n",
      "Epoch [11/95], Step [121/132], D Loss: 0.5032, G Loss: 4.4028\n",
      "Epoch [11/95], Step [126/132], D Loss: 0.5032, G Loss: 5.3028\n",
      "Epoch [11/95], Step [131/132], D Loss: 0.5032, G Loss: 4.5227\n",
      "Epoch [11/95], Training Loss: 4.7636\n",
      "1 epoch time: 7.02825276851654\n",
      "Epoch [12/95], Step [1/132], D Loss: 0.5404, G Loss: 5.0328\n",
      "Epoch [12/95], Step [6/132], D Loss: 0.6717, G Loss: 4.4957\n",
      "Epoch [12/95], Step [11/132], D Loss: 0.5989, G Loss: 4.3111\n",
      "Epoch [12/95], Step [16/132], D Loss: 0.5045, G Loss: 4.5977\n",
      "Epoch [12/95], Step [21/132], D Loss: 0.5034, G Loss: 4.3642\n",
      "Epoch [12/95], Step [26/132], D Loss: 0.5036, G Loss: 4.2773\n",
      "Epoch [12/95], Step [31/132], D Loss: 0.5033, G Loss: 4.3538\n",
      "Epoch [12/95], Step [36/132], D Loss: 0.5034, G Loss: 4.9814\n",
      "Epoch [12/95], Step [41/132], D Loss: 0.5033, G Loss: 4.0401\n",
      "Epoch [12/95], Step [46/132], D Loss: 0.5033, G Loss: 4.8359\n",
      "Epoch [12/95], Step [51/132], D Loss: 0.5033, G Loss: 4.8968\n",
      "Epoch [12/95], Step [56/132], D Loss: 0.5034, G Loss: 4.2527\n",
      "Epoch [12/95], Step [61/132], D Loss: 0.5035, G Loss: 5.1494\n",
      "Epoch [12/95], Step [66/132], D Loss: 0.5033, G Loss: 4.1976\n",
      "Epoch [12/95], Step [71/132], D Loss: 0.5033, G Loss: 4.0691\n",
      "Epoch [12/95], Step [76/132], D Loss: 0.5033, G Loss: 3.9481\n",
      "Epoch [12/95], Step [81/132], D Loss: 0.5033, G Loss: 4.2252\n",
      "Epoch [12/95], Step [86/132], D Loss: 0.5033, G Loss: 4.2470\n",
      "Epoch [12/95], Step [91/132], D Loss: 0.5033, G Loss: 4.6775\n",
      "Epoch [12/95], Step [96/132], D Loss: 0.5032, G Loss: 4.2806\n",
      "Epoch [12/95], Step [101/132], D Loss: 0.5032, G Loss: 4.2937\n",
      "Epoch [12/95], Step [106/132], D Loss: 0.5033, G Loss: 5.2964\n",
      "Epoch [12/95], Step [111/132], D Loss: 0.5032, G Loss: 4.7837\n",
      "Epoch [12/95], Step [116/132], D Loss: 0.5032, G Loss: 5.3101\n",
      "Epoch [12/95], Step [121/132], D Loss: 0.5033, G Loss: 4.1324\n",
      "Epoch [12/95], Step [126/132], D Loss: 0.5032, G Loss: 4.0093\n",
      "Epoch [12/95], Step [131/132], D Loss: 0.5032, G Loss: 4.7406\n",
      "Epoch [12/95], Training Loss: 4.4951\n",
      "1 epoch time: 7.059798463185628\n",
      "Epoch [13/95], Step [1/132], D Loss: 0.5033, G Loss: 4.4807\n",
      "Epoch [13/95], Step [6/132], D Loss: 0.5033, G Loss: 4.7610\n",
      "Epoch [13/95], Step [11/132], D Loss: 0.5034, G Loss: 6.3047\n",
      "Epoch [13/95], Step [16/132], D Loss: 0.5034, G Loss: 4.8430\n",
      "Epoch [13/95], Step [21/132], D Loss: 0.5032, G Loss: 4.7857\n",
      "Epoch [13/95], Step [26/132], D Loss: 0.5033, G Loss: 4.7127\n",
      "Epoch [13/95], Step [31/132], D Loss: 0.5033, G Loss: 4.3768\n",
      "Epoch [13/95], Step [36/132], D Loss: 0.5032, G Loss: 4.1554\n",
      "Epoch [13/95], Step [41/132], D Loss: 0.5033, G Loss: 4.4657\n",
      "Epoch [13/95], Step [46/132], D Loss: 0.5032, G Loss: 4.5222\n",
      "Epoch [13/95], Step [51/132], D Loss: 0.5032, G Loss: 4.6760\n",
      "Epoch [13/95], Step [56/132], D Loss: 0.5033, G Loss: 4.4527\n",
      "Epoch [13/95], Step [61/132], D Loss: 0.5032, G Loss: 4.0518\n",
      "Epoch [13/95], Step [66/132], D Loss: 0.5033, G Loss: 3.9933\n",
      "Epoch [13/95], Step [71/132], D Loss: 0.5032, G Loss: 4.4572\n",
      "Epoch [13/95], Step [76/132], D Loss: 0.5033, G Loss: 4.6697\n",
      "Epoch [13/95], Step [81/132], D Loss: 0.5032, G Loss: 4.5128\n",
      "Epoch [13/95], Step [86/132], D Loss: 0.5032, G Loss: 4.7990\n",
      "Epoch [13/95], Step [91/132], D Loss: 0.5033, G Loss: 5.0404\n",
      "Epoch [13/95], Step [96/132], D Loss: 0.5033, G Loss: 4.8563\n",
      "Epoch [13/95], Step [101/132], D Loss: 0.5032, G Loss: 4.7193\n",
      "Epoch [13/95], Step [106/132], D Loss: 0.5032, G Loss: 5.9837\n",
      "Epoch [13/95], Step [111/132], D Loss: 0.5032, G Loss: 4.0726\n",
      "Epoch [13/95], Step [116/132], D Loss: 0.5032, G Loss: 4.8479\n",
      "Epoch [13/95], Step [121/132], D Loss: 0.5032, G Loss: 4.3988\n",
      "Epoch [13/95], Step [126/132], D Loss: 0.5032, G Loss: 4.8552\n",
      "Epoch [13/95], Step [131/132], D Loss: 0.5032, G Loss: 4.4821\n",
      "Epoch [13/95], Training Loss: 4.6204\n",
      "1 epoch time: 7.0479097684224445\n",
      "Epoch [14/95], Step [1/132], D Loss: 0.5032, G Loss: 4.7580\n",
      "Epoch [14/95], Step [6/132], D Loss: 0.5096, G Loss: 4.6235\n",
      "Epoch [14/95], Step [11/132], D Loss: 0.5063, G Loss: 5.0783\n",
      "Epoch [14/95], Step [16/132], D Loss: 0.5032, G Loss: 3.9319\n",
      "Epoch [14/95], Step [21/132], D Loss: 0.5032, G Loss: 3.9731\n",
      "Epoch [14/95], Step [26/132], D Loss: 0.5032, G Loss: 5.0037\n",
      "Epoch [14/95], Step [31/132], D Loss: 0.5032, G Loss: 4.3745\n",
      "Epoch [14/95], Step [36/132], D Loss: 0.5032, G Loss: 4.2142\n",
      "Epoch [14/95], Step [41/132], D Loss: 0.5032, G Loss: 3.9621\n",
      "Epoch [14/95], Step [46/132], D Loss: 0.5033, G Loss: 5.1442\n",
      "Epoch [14/95], Step [51/132], D Loss: 0.5032, G Loss: 6.1898\n",
      "Epoch [14/95], Step [56/132], D Loss: 0.5032, G Loss: 5.1928\n",
      "Epoch [14/95], Step [61/132], D Loss: 0.5033, G Loss: 4.1352\n",
      "Epoch [14/95], Step [66/132], D Loss: 0.5032, G Loss: 5.0053\n",
      "Epoch [14/95], Step [71/132], D Loss: 0.5033, G Loss: 4.4941\n",
      "Epoch [14/95], Step [76/132], D Loss: 0.5032, G Loss: 4.5507\n",
      "Epoch [14/95], Step [81/132], D Loss: 0.5033, G Loss: 4.5723\n",
      "Epoch [14/95], Step [86/132], D Loss: 0.5033, G Loss: 4.8907\n",
      "Epoch [14/95], Step [91/132], D Loss: 0.5037, G Loss: 4.0202\n",
      "Epoch [14/95], Step [96/132], D Loss: 0.5032, G Loss: 4.4522\n",
      "Epoch [14/95], Step [101/132], D Loss: 0.5032, G Loss: 4.4935\n",
      "Epoch [14/95], Step [106/132], D Loss: 0.5032, G Loss: 4.3620\n",
      "Epoch [14/95], Step [111/132], D Loss: 0.5032, G Loss: 3.8968\n",
      "Epoch [14/95], Step [116/132], D Loss: 0.5032, G Loss: 4.4173\n",
      "Epoch [14/95], Step [121/132], D Loss: 0.5287, G Loss: 4.4267\n",
      "Epoch [14/95], Step [126/132], D Loss: 0.5390, G Loss: 4.4325\n",
      "Epoch [14/95], Step [131/132], D Loss: 0.5551, G Loss: 4.4227\n",
      "Epoch [14/95], Training Loss: 4.5087\n",
      "1 epoch time: 7.021523416042328\n",
      "Epoch [15/95], Step [1/132], D Loss: 0.7237, G Loss: 4.4547\n",
      "Epoch [15/95], Step [6/132], D Loss: 0.6275, G Loss: 3.7983\n",
      "Epoch [15/95], Step [11/132], D Loss: 0.6930, G Loss: 4.4050\n",
      "Epoch [15/95], Step [16/132], D Loss: 0.6930, G Loss: 4.9865\n",
      "Epoch [15/95], Step [21/132], D Loss: 0.6927, G Loss: 4.2827\n",
      "Epoch [15/95], Step [26/132], D Loss: 0.6867, G Loss: 4.9637\n",
      "Epoch [15/95], Step [31/132], D Loss: 0.6759, G Loss: 5.1172\n",
      "Epoch [15/95], Step [36/132], D Loss: 0.6490, G Loss: 3.5324\n",
      "Epoch [15/95], Step [41/132], D Loss: 0.6606, G Loss: 4.0315\n",
      "Epoch [15/95], Step [46/132], D Loss: 0.6211, G Loss: 4.4028\n",
      "Epoch [15/95], Step [51/132], D Loss: 0.6233, G Loss: 3.7359\n",
      "Epoch [15/95], Step [56/132], D Loss: 0.6823, G Loss: 4.5115\n",
      "Epoch [15/95], Step [61/132], D Loss: 0.7009, G Loss: 3.9978\n",
      "Epoch [15/95], Step [66/132], D Loss: 0.7592, G Loss: 3.9824\n",
      "Epoch [15/95], Step [71/132], D Loss: 0.8034, G Loss: 4.6627\n",
      "Epoch [15/95], Step [76/132], D Loss: 0.5387, G Loss: 3.6408\n",
      "Epoch [15/95], Step [81/132], D Loss: 0.5559, G Loss: 4.4023\n",
      "Epoch [15/95], Step [86/132], D Loss: 0.5034, G Loss: 4.3836\n",
      "Epoch [15/95], Step [91/132], D Loss: 0.5033, G Loss: 4.6601\n",
      "Epoch [15/95], Step [96/132], D Loss: 0.5070, G Loss: 4.0986\n",
      "Epoch [15/95], Step [101/132], D Loss: 0.5069, G Loss: 4.9621\n",
      "Epoch [15/95], Step [106/132], D Loss: 0.5032, G Loss: 4.4770\n",
      "Epoch [15/95], Step [111/132], D Loss: 0.5033, G Loss: 3.7382\n",
      "Epoch [15/95], Step [116/132], D Loss: 0.5032, G Loss: 4.4321\n",
      "Epoch [15/95], Step [121/132], D Loss: 0.5033, G Loss: 4.7090\n",
      "Epoch [15/95], Step [126/132], D Loss: 0.5032, G Loss: 4.0494\n",
      "Epoch [15/95], Step [131/132], D Loss: 0.5032, G Loss: 4.0425\n",
      "Epoch [15/95], Training Loss: 4.3794\n",
      "1 epoch time: 7.042545159657796\n",
      "Epoch [16/95], Step [1/132], D Loss: 0.5032, G Loss: 5.4641\n",
      "Epoch [16/95], Step [6/132], D Loss: 0.6867, G Loss: 5.6528\n",
      "Epoch [16/95], Step [11/132], D Loss: 0.6872, G Loss: 4.5536\n",
      "Epoch [16/95], Step [16/132], D Loss: 0.6684, G Loss: 4.1614\n",
      "Epoch [16/95], Step [21/132], D Loss: 0.6533, G Loss: 4.1365\n",
      "Epoch [16/95], Step [26/132], D Loss: 0.5339, G Loss: 4.7040\n",
      "Epoch [16/95], Step [31/132], D Loss: 0.5041, G Loss: 4.7675\n",
      "Epoch [16/95], Step [36/132], D Loss: 0.5034, G Loss: 3.9196\n",
      "Epoch [16/95], Step [41/132], D Loss: 0.5033, G Loss: 4.3959\n",
      "Epoch [16/95], Step [46/132], D Loss: 0.5034, G Loss: 4.1798\n",
      "Epoch [16/95], Step [51/132], D Loss: 0.5034, G Loss: 4.0783\n",
      "Epoch [16/95], Step [56/132], D Loss: 0.5037, G Loss: 4.2301\n",
      "Epoch [16/95], Step [61/132], D Loss: 0.5033, G Loss: 4.4650\n",
      "Epoch [16/95], Step [66/132], D Loss: 0.5033, G Loss: 4.3949\n",
      "Epoch [16/95], Step [71/132], D Loss: 0.5033, G Loss: 4.1970\n",
      "Epoch [16/95], Step [76/132], D Loss: 0.5036, G Loss: 4.1467\n",
      "Epoch [16/95], Step [81/132], D Loss: 0.5034, G Loss: 3.7148\n",
      "Epoch [16/95], Step [86/132], D Loss: 0.5032, G Loss: 5.6707\n",
      "Epoch [16/95], Step [91/132], D Loss: 0.5032, G Loss: 5.2055\n",
      "Epoch [16/95], Step [96/132], D Loss: 0.5032, G Loss: 4.6160\n",
      "Epoch [16/95], Step [101/132], D Loss: 0.5032, G Loss: 4.5297\n",
      "Epoch [16/95], Step [106/132], D Loss: 0.5033, G Loss: 4.8592\n",
      "Epoch [16/95], Step [111/132], D Loss: 0.5032, G Loss: 4.9670\n",
      "Epoch [16/95], Step [116/132], D Loss: 0.5033, G Loss: 3.7938\n",
      "Epoch [16/95], Step [121/132], D Loss: 0.5033, G Loss: 4.2757\n",
      "Epoch [16/95], Step [126/132], D Loss: 0.5033, G Loss: 3.8748\n",
      "Epoch [16/95], Step [131/132], D Loss: 0.5032, G Loss: 4.4631\n",
      "Epoch [16/95], Training Loss: 4.4072\n",
      "1 epoch time: 6.97845196723938\n",
      "Epoch [17/95], Step [1/132], D Loss: 0.5032, G Loss: 4.7663\n",
      "Epoch [17/95], Step [6/132], D Loss: 0.5032, G Loss: 5.4201\n",
      "Epoch [17/95], Step [11/132], D Loss: 0.5033, G Loss: 4.4115\n",
      "Epoch [17/95], Step [16/132], D Loss: 0.5032, G Loss: 4.0428\n",
      "Epoch [17/95], Step [21/132], D Loss: 0.5033, G Loss: 3.7689\n",
      "Epoch [17/95], Step [26/132], D Loss: 0.5032, G Loss: 4.3164\n",
      "Epoch [17/95], Step [31/132], D Loss: 0.5032, G Loss: 4.3647\n",
      "Epoch [17/95], Step [36/132], D Loss: 0.5035, G Loss: 3.6820\n",
      "Epoch [17/95], Step [41/132], D Loss: 0.5033, G Loss: 4.1383\n",
      "Epoch [17/95], Step [46/132], D Loss: 0.5032, G Loss: 4.0875\n",
      "Epoch [17/95], Step [51/132], D Loss: 0.5996, G Loss: 4.5797\n",
      "Epoch [17/95], Step [56/132], D Loss: 0.5076, G Loss: 3.8621\n",
      "Epoch [17/95], Step [61/132], D Loss: 0.5034, G Loss: 4.5078\n",
      "Epoch [17/95], Step [66/132], D Loss: 0.5032, G Loss: 4.6096\n",
      "Epoch [17/95], Step [71/132], D Loss: 0.5033, G Loss: 4.1946\n",
      "Epoch [17/95], Step [76/132], D Loss: 0.5032, G Loss: 4.6368\n",
      "Epoch [17/95], Step [81/132], D Loss: 0.5436, G Loss: 4.2128\n",
      "Epoch [17/95], Step [86/132], D Loss: 0.8133, G Loss: 4.3201\n",
      "Epoch [17/95], Step [91/132], D Loss: 0.8133, G Loss: 4.1525\n",
      "Epoch [17/95], Step [96/132], D Loss: 0.8133, G Loss: 6.1332\n",
      "Epoch [17/95], Step [101/132], D Loss: 0.8133, G Loss: 3.7381\n",
      "Epoch [17/95], Step [106/132], D Loss: 0.8133, G Loss: 3.9662\n",
      "Epoch [17/95], Step [111/132], D Loss: 0.8133, G Loss: 4.0875\n",
      "Epoch [17/95], Step [116/132], D Loss: 0.8133, G Loss: 3.4001\n",
      "Epoch [17/95], Step [121/132], D Loss: 0.8133, G Loss: 3.6944\n",
      "Epoch [17/95], Step [126/132], D Loss: 0.8133, G Loss: 4.6009\n",
      "Epoch [17/95], Step [131/132], D Loss: 0.8133, G Loss: 4.6618\n",
      "Epoch [17/95], Training Loss: 4.2542\n",
      "1 epoch time: 7.006591232617696\n",
      "Epoch [18/95], Step [1/132], D Loss: 0.8133, G Loss: 3.4383\n",
      "Epoch [18/95], Step [6/132], D Loss: 0.8133, G Loss: 4.3512\n",
      "Epoch [18/95], Step [11/132], D Loss: 0.8133, G Loss: 3.9284\n",
      "Epoch [18/95], Step [16/132], D Loss: 0.8133, G Loss: 3.7445\n",
      "Epoch [18/95], Step [21/132], D Loss: 0.8133, G Loss: 4.4426\n",
      "Epoch [18/95], Step [26/132], D Loss: 0.8133, G Loss: 4.6104\n",
      "Epoch [18/95], Step [31/132], D Loss: 0.8133, G Loss: 3.8431\n",
      "Epoch [18/95], Step [36/132], D Loss: 0.8133, G Loss: 3.8092\n",
      "Epoch [18/95], Step [41/132], D Loss: 0.8133, G Loss: 3.9165\n",
      "Epoch [18/95], Step [46/132], D Loss: 0.8133, G Loss: 4.7676\n",
      "Epoch [18/95], Step [51/132], D Loss: 0.8133, G Loss: 3.3165\n",
      "Epoch [18/95], Step [56/132], D Loss: 0.8133, G Loss: 4.1412\n",
      "Epoch [18/95], Step [61/132], D Loss: 0.8133, G Loss: 4.0760\n",
      "Epoch [18/95], Step [66/132], D Loss: 0.8133, G Loss: 3.7443\n",
      "Epoch [18/95], Step [71/132], D Loss: 0.8133, G Loss: 3.8829\n",
      "Epoch [18/95], Step [76/132], D Loss: 0.8133, G Loss: 3.4811\n",
      "Epoch [18/95], Step [81/132], D Loss: 0.8133, G Loss: 4.3521\n",
      "Epoch [18/95], Step [86/132], D Loss: 0.8133, G Loss: 3.6091\n",
      "Epoch [18/95], Step [91/132], D Loss: 0.8133, G Loss: 3.4599\n",
      "Epoch [18/95], Step [96/132], D Loss: 0.8132, G Loss: 3.7001\n",
      "Epoch [18/95], Step [101/132], D Loss: 0.8132, G Loss: 3.6241\n",
      "Epoch [18/95], Step [106/132], D Loss: 0.8130, G Loss: 3.5775\n",
      "Epoch [18/95], Step [111/132], D Loss: 0.7796, G Loss: 4.1068\n",
      "Epoch [18/95], Step [116/132], D Loss: 0.7192, G Loss: 3.9381\n",
      "Epoch [18/95], Step [121/132], D Loss: 0.5178, G Loss: 4.2721\n",
      "Epoch [18/95], Step [126/132], D Loss: 0.5171, G Loss: 4.7632\n",
      "Epoch [18/95], Step [131/132], D Loss: 0.5091, G Loss: 3.7209\n",
      "Epoch [18/95], Training Loss: 3.9869\n",
      "1 epoch time: 6.99009538491567\n",
      "Epoch [19/95], Step [1/132], D Loss: 0.8067, G Loss: 3.9389\n",
      "Epoch [19/95], Step [6/132], D Loss: 0.6788, G Loss: 4.0301\n",
      "Epoch [19/95], Step [11/132], D Loss: 0.6648, G Loss: 3.9544\n",
      "Epoch [19/95], Step [16/132], D Loss: 0.6771, G Loss: 3.9938\n",
      "Epoch [19/95], Step [21/132], D Loss: 0.5881, G Loss: 4.3879\n",
      "Epoch [19/95], Step [26/132], D Loss: 0.5580, G Loss: 4.4676\n",
      "Epoch [19/95], Step [31/132], D Loss: 0.6549, G Loss: 4.4732\n",
      "Epoch [19/95], Step [36/132], D Loss: 0.8058, G Loss: 4.1590\n",
      "Epoch [19/95], Step [41/132], D Loss: 0.5423, G Loss: 4.4859\n",
      "Epoch [19/95], Step [46/132], D Loss: 0.5033, G Loss: 4.5175\n",
      "Epoch [19/95], Step [51/132], D Loss: 0.5088, G Loss: 4.1889\n",
      "Epoch [19/95], Step [56/132], D Loss: 0.5036, G Loss: 4.0554\n",
      "Epoch [19/95], Step [61/132], D Loss: 0.5033, G Loss: 4.6796\n",
      "Epoch [19/95], Step [66/132], D Loss: 0.5036, G Loss: 3.9246\n",
      "Epoch [19/95], Step [71/132], D Loss: 0.5032, G Loss: 4.5515\n",
      "Epoch [19/95], Step [76/132], D Loss: 0.5033, G Loss: 4.1315\n",
      "Epoch [19/95], Step [81/132], D Loss: 0.5033, G Loss: 4.5721\n",
      "Epoch [19/95], Step [86/132], D Loss: 0.5114, G Loss: 4.2978\n",
      "Epoch [19/95], Step [91/132], D Loss: 0.5032, G Loss: 4.8160\n",
      "Epoch [19/95], Step [96/132], D Loss: 0.5032, G Loss: 4.2496\n",
      "Epoch [19/95], Step [101/132], D Loss: 0.5033, G Loss: 4.3697\n",
      "Epoch [19/95], Step [106/132], D Loss: 0.5415, G Loss: 4.5638\n",
      "Epoch [19/95], Step [111/132], D Loss: 0.7640, G Loss: 4.1773\n",
      "Epoch [19/95], Step [116/132], D Loss: 0.6254, G Loss: 4.5518\n",
      "Epoch [19/95], Step [121/132], D Loss: 0.5725, G Loss: 4.7646\n",
      "Epoch [19/95], Step [126/132], D Loss: 0.5690, G Loss: 3.6497\n",
      "Epoch [19/95], Step [131/132], D Loss: 0.5217, G Loss: 4.1857\n",
      "Epoch [19/95], Training Loss: 4.3236\n",
      "1 epoch time: 6.991721765200297\n",
      "Epoch [20/95], Step [1/132], D Loss: 0.5063, G Loss: 4.3718\n",
      "Epoch [20/95], Step [6/132], D Loss: 0.5033, G Loss: 4.2885\n",
      "Epoch [20/95], Step [11/132], D Loss: 0.5037, G Loss: 4.5474\n",
      "Epoch [20/95], Step [16/132], D Loss: 0.5033, G Loss: 4.3258\n",
      "Epoch [20/95], Step [21/132], D Loss: 0.5033, G Loss: 4.3826\n",
      "Epoch [20/95], Step [26/132], D Loss: 0.5032, G Loss: 4.3497\n",
      "Epoch [20/95], Step [31/132], D Loss: 0.5033, G Loss: 4.0747\n",
      "Epoch [20/95], Step [36/132], D Loss: 0.5032, G Loss: 4.1409\n",
      "Epoch [20/95], Step [41/132], D Loss: 0.5079, G Loss: 4.7307\n",
      "Epoch [20/95], Step [46/132], D Loss: 0.5056, G Loss: 4.0514\n",
      "Epoch [20/95], Step [51/132], D Loss: 0.8021, G Loss: 4.0197\n",
      "Epoch [20/95], Step [56/132], D Loss: 0.5517, G Loss: 4.3125\n",
      "Epoch [20/95], Step [61/132], D Loss: 0.6078, G Loss: 4.2614\n",
      "Epoch [20/95], Step [66/132], D Loss: 0.5033, G Loss: 3.8565\n",
      "Epoch [20/95], Step [71/132], D Loss: 0.5032, G Loss: 4.8292\n",
      "Epoch [20/95], Step [76/132], D Loss: 0.5032, G Loss: 3.9776\n",
      "Epoch [20/95], Step [81/132], D Loss: 0.5032, G Loss: 4.2591\n",
      "Epoch [20/95], Step [86/132], D Loss: 0.5032, G Loss: 3.9456\n",
      "Epoch [20/95], Step [91/132], D Loss: 0.5032, G Loss: 5.8940\n",
      "Epoch [20/95], Step [96/132], D Loss: 0.5033, G Loss: 3.8898\n",
      "Epoch [20/95], Step [101/132], D Loss: 0.5032, G Loss: 4.6851\n",
      "Epoch [20/95], Step [106/132], D Loss: 0.5036, G Loss: 3.8216\n",
      "Epoch [20/95], Step [111/132], D Loss: 0.5032, G Loss: 4.8960\n",
      "Epoch [20/95], Step [116/132], D Loss: 0.5032, G Loss: 4.8959\n",
      "Epoch [20/95], Step [121/132], D Loss: 0.5032, G Loss: 5.0811\n",
      "Epoch [20/95], Step [126/132], D Loss: 0.5032, G Loss: 4.5845\n",
      "Epoch [20/95], Step [131/132], D Loss: 0.5032, G Loss: 4.0222\n",
      "Epoch [20/95], Training Loss: 4.2578\n",
      "1 epoch time: 7.026949481169383\n",
      "Epoch [21/95], Step [1/132], D Loss: 0.5032, G Loss: 4.9943\n",
      "Epoch [21/95], Step [6/132], D Loss: 0.5032, G Loss: 5.5160\n",
      "Epoch [21/95], Step [11/132], D Loss: 0.5032, G Loss: 4.6715\n",
      "Epoch [21/95], Step [16/132], D Loss: 0.5032, G Loss: 4.3113\n",
      "Epoch [21/95], Step [21/132], D Loss: 0.5032, G Loss: 4.1673\n",
      "Epoch [21/95], Step [26/132], D Loss: 0.5032, G Loss: 3.8786\n",
      "Epoch [21/95], Step [31/132], D Loss: 0.5033, G Loss: 4.3858\n",
      "Epoch [21/95], Step [36/132], D Loss: 0.5032, G Loss: 3.8990\n",
      "Epoch [21/95], Step [41/132], D Loss: 0.5032, G Loss: 4.0443\n",
      "Epoch [21/95], Step [46/132], D Loss: 0.5032, G Loss: 4.1379\n",
      "Epoch [21/95], Step [51/132], D Loss: 0.5034, G Loss: 4.5761\n",
      "Epoch [21/95], Step [56/132], D Loss: 0.5033, G Loss: 4.1204\n",
      "Epoch [21/95], Step [61/132], D Loss: 0.5032, G Loss: 4.4197\n",
      "Epoch [21/95], Step [66/132], D Loss: 0.5032, G Loss: 5.5028\n",
      "Epoch [21/95], Step [71/132], D Loss: 0.5084, G Loss: 4.1093\n",
      "Epoch [21/95], Step [76/132], D Loss: 0.5032, G Loss: 4.1801\n",
      "Epoch [21/95], Step [81/132], D Loss: 0.5032, G Loss: 4.5145\n",
      "Epoch [21/95], Step [86/132], D Loss: 0.5032, G Loss: 3.7777\n",
      "Epoch [21/95], Step [91/132], D Loss: 0.5032, G Loss: 4.0807\n",
      "Epoch [21/95], Step [96/132], D Loss: 0.5032, G Loss: 3.9622\n",
      "Epoch [21/95], Step [101/132], D Loss: 0.5032, G Loss: 4.3781\n",
      "Epoch [21/95], Step [106/132], D Loss: 0.5032, G Loss: 4.0205\n",
      "Epoch [21/95], Step [111/132], D Loss: 0.5033, G Loss: 4.3059\n",
      "Epoch [21/95], Step [116/132], D Loss: 0.5032, G Loss: 4.5970\n",
      "Epoch [21/95], Step [121/132], D Loss: 0.5032, G Loss: 4.1235\n",
      "Epoch [21/95], Step [126/132], D Loss: 0.5032, G Loss: 3.9390\n",
      "Epoch [21/95], Step [131/132], D Loss: 0.5032, G Loss: 4.2641\n",
      "Epoch [21/95], Training Loss: 4.2587\n",
      "1 epoch time: 7.045792651176453\n",
      "Epoch [22/95], Step [1/132], D Loss: 0.5032, G Loss: 4.4254\n",
      "Epoch [22/95], Step [6/132], D Loss: 0.5033, G Loss: 3.6412\n",
      "Epoch [22/95], Step [11/132], D Loss: 0.5032, G Loss: 4.1154\n",
      "Epoch [22/95], Step [16/132], D Loss: 0.5032, G Loss: 4.7632\n",
      "Epoch [22/95], Step [21/132], D Loss: 0.5032, G Loss: 3.9292\n",
      "Epoch [22/95], Step [26/132], D Loss: 0.5032, G Loss: 4.3257\n",
      "Epoch [22/95], Step [31/132], D Loss: 0.5032, G Loss: 4.5140\n",
      "Epoch [22/95], Step [36/132], D Loss: 0.5032, G Loss: 4.7671\n",
      "Epoch [22/95], Step [41/132], D Loss: 0.5032, G Loss: 4.6094\n",
      "Epoch [22/95], Step [46/132], D Loss: 0.5032, G Loss: 4.1429\n",
      "Epoch [22/95], Step [51/132], D Loss: 0.5032, G Loss: 4.4377\n",
      "Epoch [22/95], Step [56/132], D Loss: 0.5032, G Loss: 4.9677\n",
      "Epoch [22/95], Step [61/132], D Loss: 0.5032, G Loss: 5.5651\n",
      "Epoch [22/95], Step [66/132], D Loss: 0.5032, G Loss: 4.3316\n",
      "Epoch [22/95], Step [71/132], D Loss: 0.5032, G Loss: 4.2799\n",
      "Epoch [22/95], Step [76/132], D Loss: 0.5032, G Loss: 4.1530\n",
      "Epoch [22/95], Step [81/132], D Loss: 0.5032, G Loss: 4.0428\n",
      "Epoch [22/95], Step [86/132], D Loss: 0.5032, G Loss: 4.2408\n",
      "Epoch [22/95], Step [91/132], D Loss: 0.5032, G Loss: 4.7519\n",
      "Epoch [22/95], Step [96/132], D Loss: 0.5032, G Loss: 3.7437\n",
      "Epoch [22/95], Step [101/132], D Loss: 0.5032, G Loss: 4.8845\n",
      "Epoch [22/95], Step [106/132], D Loss: 0.5032, G Loss: 4.0955\n",
      "Epoch [22/95], Step [111/132], D Loss: 0.5032, G Loss: 4.1917\n",
      "Epoch [22/95], Step [116/132], D Loss: 0.5032, G Loss: 4.2471\n",
      "Epoch [22/95], Step [121/132], D Loss: 0.5032, G Loss: 4.1275\n",
      "Epoch [22/95], Step [126/132], D Loss: 0.5032, G Loss: 3.7384\n",
      "Epoch [22/95], Step [131/132], D Loss: 0.5032, G Loss: 4.4307\n",
      "Epoch [22/95], Training Loss: 4.1819\n",
      "1 epoch time: 7.002699661254883\n",
      "Epoch [23/95], Step [1/132], D Loss: 0.5032, G Loss: 3.6494\n",
      "Epoch [23/95], Step [6/132], D Loss: 0.5032, G Loss: 4.3778\n",
      "Epoch [23/95], Step [11/132], D Loss: 0.5032, G Loss: 4.4138\n",
      "Epoch [23/95], Step [16/132], D Loss: 0.5032, G Loss: 3.6650\n",
      "Epoch [23/95], Step [21/132], D Loss: 0.5032, G Loss: 4.3811\n",
      "Epoch [23/95], Step [26/132], D Loss: 0.5032, G Loss: 4.2449\n",
      "Epoch [23/95], Step [31/132], D Loss: 0.5036, G Loss: 3.7489\n",
      "Epoch [23/95], Step [36/132], D Loss: 0.5032, G Loss: 4.7184\n",
      "Epoch [23/95], Step [41/132], D Loss: 0.5032, G Loss: 4.7761\n",
      "Epoch [23/95], Step [46/132], D Loss: 0.5032, G Loss: 3.6736\n",
      "Epoch [23/95], Step [51/132], D Loss: 0.5032, G Loss: 4.7379\n",
      "Epoch [23/95], Step [56/132], D Loss: 0.5032, G Loss: 4.1873\n",
      "Epoch [23/95], Step [61/132], D Loss: 0.5032, G Loss: 3.7501\n",
      "Epoch [23/95], Step [66/132], D Loss: 0.5032, G Loss: 5.0877\n",
      "Epoch [23/95], Step [71/132], D Loss: 0.5032, G Loss: 4.1310\n",
      "Epoch [23/95], Step [76/132], D Loss: 0.5032, G Loss: 4.8243\n",
      "Epoch [23/95], Step [81/132], D Loss: 0.5035, G Loss: 4.8611\n",
      "Epoch [23/95], Step [86/132], D Loss: 0.5032, G Loss: 4.3590\n",
      "Epoch [23/95], Step [91/132], D Loss: 0.5032, G Loss: 4.1500\n",
      "Epoch [23/95], Step [96/132], D Loss: 0.5032, G Loss: 4.3294\n",
      "Epoch [23/95], Step [101/132], D Loss: 0.5032, G Loss: 4.3772\n",
      "Epoch [23/95], Step [106/132], D Loss: 0.5032, G Loss: 3.8535\n",
      "Epoch [23/95], Step [111/132], D Loss: 0.5032, G Loss: 3.6319\n",
      "Epoch [23/95], Step [116/132], D Loss: 0.5032, G Loss: 4.4541\n",
      "Epoch [23/95], Step [121/132], D Loss: 0.5032, G Loss: 4.4877\n",
      "Epoch [23/95], Step [126/132], D Loss: 0.5032, G Loss: 4.1879\n",
      "Epoch [23/95], Step [131/132], D Loss: 0.5032, G Loss: 4.2898\n",
      "Epoch [23/95], Training Loss: 4.2276\n",
      "1 epoch time: 6.992004474004109\n",
      "Epoch [24/95], Step [1/132], D Loss: 0.5032, G Loss: 3.7566\n",
      "Epoch [24/95], Step [6/132], D Loss: 0.5032, G Loss: 4.0948\n",
      "Epoch [24/95], Step [11/132], D Loss: 0.5032, G Loss: 3.9040\n",
      "Epoch [24/95], Step [16/132], D Loss: 0.5032, G Loss: 3.6903\n",
      "Epoch [24/95], Step [21/132], D Loss: 0.5034, G Loss: 3.5085\n",
      "Epoch [24/95], Step [26/132], D Loss: 0.5032, G Loss: 3.8436\n",
      "Epoch [24/95], Step [31/132], D Loss: 0.5032, G Loss: 4.3082\n",
      "Epoch [24/95], Step [36/132], D Loss: 0.5032, G Loss: 3.9421\n",
      "Epoch [24/95], Step [41/132], D Loss: 0.5032, G Loss: 4.2179\n",
      "Epoch [24/95], Step [46/132], D Loss: 0.5032, G Loss: 3.7667\n",
      "Epoch [24/95], Step [51/132], D Loss: 0.5032, G Loss: 4.6445\n",
      "Epoch [24/95], Step [56/132], D Loss: 0.5032, G Loss: 3.5730\n",
      "Epoch [24/95], Step [61/132], D Loss: 0.5032, G Loss: 3.8764\n",
      "Epoch [24/95], Step [66/132], D Loss: 0.5032, G Loss: 4.1757\n",
      "Epoch [24/95], Step [71/132], D Loss: 0.5032, G Loss: 4.3611\n",
      "Epoch [24/95], Step [76/132], D Loss: 0.5032, G Loss: 4.7423\n",
      "Epoch [24/95], Step [81/132], D Loss: 0.5032, G Loss: 3.5008\n",
      "Epoch [24/95], Step [86/132], D Loss: 0.5032, G Loss: 4.0430\n",
      "Epoch [24/95], Step [91/132], D Loss: 0.5032, G Loss: 3.7147\n",
      "Epoch [24/95], Step [96/132], D Loss: 0.5032, G Loss: 4.4889\n",
      "Epoch [24/95], Step [101/132], D Loss: 0.5032, G Loss: 4.9847\n",
      "Epoch [24/95], Step [106/132], D Loss: 0.5032, G Loss: 3.6593\n",
      "Epoch [24/95], Step [111/132], D Loss: 0.5032, G Loss: 4.6654\n",
      "Epoch [24/95], Step [116/132], D Loss: 0.5032, G Loss: 5.1688\n",
      "Epoch [24/95], Step [121/132], D Loss: 0.5032, G Loss: 3.7926\n",
      "Epoch [24/95], Step [126/132], D Loss: 0.5032, G Loss: 3.9002\n",
      "Epoch [24/95], Step [131/132], D Loss: 0.5032, G Loss: 4.0496\n",
      "Epoch [24/95], Training Loss: 4.1660\n",
      "1 epoch time: 6.985315863291422\n",
      "Epoch [25/95], Step [1/132], D Loss: 0.5032, G Loss: 4.0714\n",
      "Epoch [25/95], Step [6/132], D Loss: 0.5032, G Loss: 4.7063\n",
      "Epoch [25/95], Step [11/132], D Loss: 0.5032, G Loss: 4.5066\n",
      "Epoch [25/95], Step [16/132], D Loss: 0.5032, G Loss: 4.6280\n",
      "Epoch [25/95], Step [21/132], D Loss: 0.5032, G Loss: 3.9978\n",
      "Epoch [25/95], Step [26/132], D Loss: 0.5032, G Loss: 4.7509\n",
      "Epoch [25/95], Step [31/132], D Loss: 0.5032, G Loss: 3.7099\n",
      "Epoch [25/95], Step [36/132], D Loss: 0.5032, G Loss: 4.2575\n",
      "Epoch [25/95], Step [41/132], D Loss: 0.5032, G Loss: 4.0749\n",
      "Epoch [25/95], Step [46/132], D Loss: 0.5032, G Loss: 4.1810\n",
      "Epoch [25/95], Step [51/132], D Loss: 0.5032, G Loss: 3.8251\n",
      "Epoch [25/95], Step [56/132], D Loss: 0.5032, G Loss: 5.0559\n",
      "Epoch [25/95], Step [61/132], D Loss: 0.5032, G Loss: 4.7568\n",
      "Epoch [25/95], Step [66/132], D Loss: 0.5032, G Loss: 3.9964\n",
      "Epoch [25/95], Step [71/132], D Loss: 0.5032, G Loss: 3.6433\n",
      "Epoch [25/95], Step [76/132], D Loss: 0.5032, G Loss: 3.9569\n",
      "Epoch [25/95], Step [81/132], D Loss: 0.5032, G Loss: 3.9806\n",
      "Epoch [25/95], Step [86/132], D Loss: 0.5032, G Loss: 4.4368\n",
      "Epoch [25/95], Step [91/132], D Loss: 0.5032, G Loss: 4.6963\n",
      "Epoch [25/95], Step [96/132], D Loss: 0.5032, G Loss: 4.2735\n",
      "Epoch [25/95], Step [101/132], D Loss: 0.7660, G Loss: 4.1653\n",
      "Epoch [25/95], Step [106/132], D Loss: 0.6931, G Loss: 4.3113\n",
      "Epoch [25/95], Step [111/132], D Loss: 0.6931, G Loss: 4.1035\n",
      "Epoch [25/95], Step [116/132], D Loss: 0.6931, G Loss: 4.0874\n",
      "Epoch [25/95], Step [121/132], D Loss: 0.6931, G Loss: 4.2153\n",
      "Epoch [25/95], Step [126/132], D Loss: 0.6931, G Loss: 3.9241\n",
      "Epoch [25/95], Step [131/132], D Loss: 0.6931, G Loss: 4.5110\n",
      "Epoch [25/95], Training Loss: 4.1719\n",
      "1 epoch time: 7.03613938887914\n",
      "Epoch [26/95], Step [1/132], D Loss: 0.6931, G Loss: 3.7651\n",
      "Epoch [26/95], Step [6/132], D Loss: 0.6931, G Loss: 4.0680\n",
      "Epoch [26/95], Step [11/132], D Loss: 0.6931, G Loss: 3.7997\n",
      "Epoch [26/95], Step [16/132], D Loss: 0.6931, G Loss: 4.6704\n",
      "Epoch [26/95], Step [21/132], D Loss: 0.6931, G Loss: 3.7657\n",
      "Epoch [26/95], Step [26/132], D Loss: 0.6931, G Loss: 3.7283\n",
      "Epoch [26/95], Step [31/132], D Loss: 0.6931, G Loss: 4.1350\n",
      "Epoch [26/95], Step [36/132], D Loss: 0.6931, G Loss: 4.4946\n",
      "Epoch [26/95], Step [41/132], D Loss: 0.6931, G Loss: 4.6648\n",
      "Epoch [26/95], Step [46/132], D Loss: 0.6931, G Loss: 4.4003\n",
      "Epoch [26/95], Step [51/132], D Loss: 0.6931, G Loss: 4.0153\n",
      "Epoch [26/95], Step [56/132], D Loss: 0.6931, G Loss: 3.2453\n",
      "Epoch [26/95], Step [61/132], D Loss: 0.6931, G Loss: 3.8218\n",
      "Epoch [26/95], Step [66/132], D Loss: 0.6931, G Loss: 3.9877\n",
      "Epoch [26/95], Step [71/132], D Loss: 0.6931, G Loss: 4.8954\n",
      "Epoch [26/95], Step [76/132], D Loss: 0.6931, G Loss: 4.2601\n",
      "Epoch [26/95], Step [81/132], D Loss: 0.6931, G Loss: 4.8969\n",
      "Epoch [26/95], Step [86/132], D Loss: 0.6931, G Loss: 3.5646\n",
      "Epoch [26/95], Step [91/132], D Loss: 0.6931, G Loss: 3.8253\n",
      "Epoch [26/95], Step [96/132], D Loss: 0.6931, G Loss: 3.7260\n",
      "Epoch [26/95], Step [101/132], D Loss: 0.6931, G Loss: 3.8224\n",
      "Epoch [26/95], Step [106/132], D Loss: 0.6931, G Loss: 4.3127\n",
      "Epoch [26/95], Step [111/132], D Loss: 0.6931, G Loss: 4.0345\n",
      "Epoch [26/95], Step [116/132], D Loss: 0.6931, G Loss: 3.6485\n",
      "Epoch [26/95], Step [121/132], D Loss: 0.6931, G Loss: 4.3752\n",
      "Epoch [26/95], Step [126/132], D Loss: 0.6931, G Loss: 4.2231\n",
      "Epoch [26/95], Step [131/132], D Loss: 0.6931, G Loss: 3.9975\n",
      "Epoch [26/95], Training Loss: 4.1345\n",
      "1 epoch time: 7.030595914522807\n",
      "Epoch [27/95], Step [1/132], D Loss: 0.6931, G Loss: 3.8835\n",
      "Epoch [27/95], Step [6/132], D Loss: 0.6931, G Loss: 3.8156\n",
      "Epoch [27/95], Step [11/132], D Loss: 0.6931, G Loss: 4.3113\n",
      "Epoch [27/95], Step [16/132], D Loss: 0.6931, G Loss: 3.9718\n",
      "Epoch [27/95], Step [21/132], D Loss: 0.6931, G Loss: 4.6867\n",
      "Epoch [27/95], Step [26/132], D Loss: 0.6931, G Loss: 4.0172\n",
      "Epoch [27/95], Step [31/132], D Loss: 0.6931, G Loss: 4.1410\n",
      "Epoch [27/95], Step [36/132], D Loss: 0.6931, G Loss: 3.5424\n",
      "Epoch [27/95], Step [41/132], D Loss: 0.6931, G Loss: 4.1184\n",
      "Epoch [27/95], Step [46/132], D Loss: 0.6931, G Loss: 3.7325\n",
      "Epoch [27/95], Step [51/132], D Loss: 0.6931, G Loss: 3.5806\n",
      "Epoch [27/95], Step [56/132], D Loss: 0.6931, G Loss: 3.6561\n",
      "Epoch [27/95], Step [61/132], D Loss: 0.6931, G Loss: 4.3687\n",
      "Epoch [27/95], Step [66/132], D Loss: 0.6931, G Loss: 4.8902\n",
      "Epoch [27/95], Step [71/132], D Loss: 0.6931, G Loss: 3.3366\n",
      "Epoch [27/95], Step [76/132], D Loss: 0.6931, G Loss: 4.4433\n",
      "Epoch [27/95], Step [81/132], D Loss: 0.6931, G Loss: 3.4295\n",
      "Epoch [27/95], Step [86/132], D Loss: 0.6931, G Loss: 4.2998\n",
      "Epoch [27/95], Step [91/132], D Loss: 0.6931, G Loss: 3.6685\n",
      "Epoch [27/95], Step [96/132], D Loss: 0.6930, G Loss: 4.2324\n",
      "Epoch [27/95], Step [101/132], D Loss: 0.6929, G Loss: 4.0232\n",
      "Epoch [27/95], Step [106/132], D Loss: 0.6927, G Loss: 3.7413\n",
      "Epoch [27/95], Step [111/132], D Loss: 0.6838, G Loss: 3.8124\n",
      "Epoch [27/95], Step [116/132], D Loss: 0.6765, G Loss: 4.7727\n",
      "Epoch [27/95], Step [121/132], D Loss: 0.6638, G Loss: 3.9007\n",
      "Epoch [27/95], Step [126/132], D Loss: 0.6961, G Loss: 4.4445\n",
      "Epoch [27/95], Step [131/132], D Loss: 0.7861, G Loss: 3.8322\n",
      "Epoch [27/95], Training Loss: 4.1045\n",
      "1 epoch time: 7.006070335706075\n",
      "Epoch [28/95], Step [1/132], D Loss: 0.7248, G Loss: 4.5217\n",
      "Epoch [28/95], Step [6/132], D Loss: 0.5761, G Loss: 3.8352\n",
      "Epoch [28/95], Step [11/132], D Loss: 0.6873, G Loss: 4.0012\n",
      "Epoch [28/95], Step [16/132], D Loss: 0.7899, G Loss: 3.2777\n",
      "Epoch [28/95], Step [21/132], D Loss: 0.6447, G Loss: 3.6469\n",
      "Epoch [28/95], Step [26/132], D Loss: 0.6929, G Loss: 3.7854\n",
      "Epoch [28/95], Step [31/132], D Loss: 0.6931, G Loss: 3.5795\n",
      "Epoch [28/95], Step [36/132], D Loss: 0.6931, G Loss: 4.2739\n",
      "Epoch [28/95], Step [41/132], D Loss: 0.6931, G Loss: 4.5948\n",
      "Epoch [28/95], Step [46/132], D Loss: 0.6929, G Loss: 3.4354\n",
      "Epoch [28/95], Step [51/132], D Loss: 0.6920, G Loss: 4.2655\n",
      "Epoch [28/95], Step [56/132], D Loss: 0.6831, G Loss: 4.3812\n",
      "Epoch [28/95], Step [61/132], D Loss: 0.6707, G Loss: 4.5320\n",
      "Epoch [28/95], Step [66/132], D Loss: 0.6795, G Loss: 4.0782\n",
      "Epoch [28/95], Step [71/132], D Loss: 0.5473, G Loss: 3.6448\n",
      "Epoch [28/95], Step [76/132], D Loss: 0.5935, G Loss: 3.2457\n",
      "Epoch [28/95], Step [81/132], D Loss: 0.6050, G Loss: 3.8276\n",
      "Epoch [28/95], Step [86/132], D Loss: 0.7333, G Loss: 3.6364\n",
      "Epoch [28/95], Step [91/132], D Loss: 0.5376, G Loss: 4.3328\n",
      "Epoch [28/95], Step [96/132], D Loss: 0.5701, G Loss: 3.7663\n",
      "Epoch [28/95], Step [101/132], D Loss: 0.7291, G Loss: 3.6908\n",
      "Epoch [28/95], Step [106/132], D Loss: 0.7729, G Loss: 3.8063\n",
      "Epoch [28/95], Step [111/132], D Loss: 0.5913, G Loss: 4.2509\n",
      "Epoch [28/95], Step [116/132], D Loss: 0.7745, G Loss: 4.8992\n",
      "Epoch [28/95], Step [121/132], D Loss: 0.5880, G Loss: 4.9328\n",
      "Epoch [28/95], Step [126/132], D Loss: 0.8110, G Loss: 4.0259\n",
      "Epoch [28/95], Step [131/132], D Loss: 0.6901, G Loss: 3.4198\n",
      "Epoch [28/95], Training Loss: 4.0226\n",
      "1 epoch time: 6.978736825784048\n",
      "Epoch [29/95], Step [1/132], D Loss: 0.6931, G Loss: 3.7075\n",
      "Epoch [29/95], Step [6/132], D Loss: 0.6899, G Loss: 3.9548\n",
      "Epoch [29/95], Step [11/132], D Loss: 0.6633, G Loss: 3.7991\n",
      "Epoch [29/95], Step [16/132], D Loss: 0.5097, G Loss: 4.7795\n",
      "Epoch [29/95], Step [21/132], D Loss: 0.5079, G Loss: 4.9158\n",
      "Epoch [29/95], Step [26/132], D Loss: 0.8130, G Loss: 3.4903\n",
      "Epoch [29/95], Step [31/132], D Loss: 0.8132, G Loss: 3.5084\n",
      "Epoch [29/95], Step [36/132], D Loss: 0.7837, G Loss: 3.9068\n",
      "Epoch [29/95], Step [41/132], D Loss: 0.7212, G Loss: 4.3380\n",
      "Epoch [29/95], Step [46/132], D Loss: 0.6742, G Loss: 3.6910\n",
      "Epoch [29/95], Step [51/132], D Loss: 0.6437, G Loss: 3.4689\n",
      "Epoch [29/95], Step [56/132], D Loss: 0.6286, G Loss: 4.6172\n",
      "Epoch [29/95], Step [61/132], D Loss: 0.6926, G Loss: 3.4228\n",
      "Epoch [29/95], Step [66/132], D Loss: 0.6923, G Loss: 3.9762\n",
      "Epoch [29/95], Step [71/132], D Loss: 0.6504, G Loss: 4.3290\n",
      "Epoch [29/95], Step [76/132], D Loss: 0.6927, G Loss: 4.3825\n",
      "Epoch [29/95], Step [81/132], D Loss: 0.6499, G Loss: 4.1136\n",
      "Epoch [29/95], Step [86/132], D Loss: 0.5271, G Loss: 4.1782\n",
      "Epoch [29/95], Step [91/132], D Loss: 0.6897, G Loss: 4.0721\n",
      "Epoch [29/95], Step [96/132], D Loss: 0.6274, G Loss: 3.7592\n",
      "Epoch [29/95], Step [101/132], D Loss: 0.8056, G Loss: 3.9160\n",
      "Epoch [29/95], Step [106/132], D Loss: 0.7193, G Loss: 3.4951\n",
      "Epoch [29/95], Step [111/132], D Loss: 0.5963, G Loss: 4.4395\n",
      "Epoch [29/95], Step [116/132], D Loss: 0.6589, G Loss: 4.3755\n",
      "Epoch [29/95], Step [121/132], D Loss: 0.8132, G Loss: 3.9954\n",
      "Epoch [29/95], Step [126/132], D Loss: 0.8067, G Loss: 3.5014\n",
      "Epoch [29/95], Step [131/132], D Loss: 0.6892, G Loss: 3.7846\n",
      "Epoch [29/95], Training Loss: 4.0350\n",
      "1 epoch time: 6.996207729975382\n",
      "Epoch [30/95], Step [1/132], D Loss: 0.6550, G Loss: 4.3051\n",
      "Epoch [30/95], Step [6/132], D Loss: 0.5130, G Loss: 3.9033\n",
      "Epoch [30/95], Step [11/132], D Loss: 0.6931, G Loss: 4.0880\n",
      "Epoch [30/95], Step [16/132], D Loss: 0.6931, G Loss: 3.9446\n",
      "Epoch [30/95], Step [21/132], D Loss: 0.6931, G Loss: 3.5002\n",
      "Epoch [30/95], Step [26/132], D Loss: 0.6931, G Loss: 4.4885\n",
      "Epoch [30/95], Step [31/132], D Loss: 0.6931, G Loss: 4.6069\n",
      "Epoch [30/95], Step [36/132], D Loss: 0.6931, G Loss: 3.5143\n",
      "Epoch [30/95], Step [41/132], D Loss: 0.6931, G Loss: 4.1023\n",
      "Epoch [30/95], Step [46/132], D Loss: 0.6931, G Loss: 3.4774\n",
      "Epoch [30/95], Step [51/132], D Loss: 0.6931, G Loss: 4.6226\n",
      "Epoch [30/95], Step [56/132], D Loss: 0.6931, G Loss: 4.0342\n",
      "Epoch [30/95], Step [61/132], D Loss: 0.6931, G Loss: 4.1420\n",
      "Epoch [30/95], Step [66/132], D Loss: 0.6931, G Loss: 3.9140\n",
      "Epoch [30/95], Step [71/132], D Loss: 0.6931, G Loss: 3.4441\n",
      "Epoch [30/95], Step [76/132], D Loss: 0.6931, G Loss: 4.5302\n",
      "Epoch [30/95], Step [81/132], D Loss: 0.6931, G Loss: 4.1798\n",
      "Epoch [30/95], Step [86/132], D Loss: 0.6931, G Loss: 4.3867\n",
      "Epoch [30/95], Step [91/132], D Loss: 0.6931, G Loss: 3.9270\n",
      "Epoch [30/95], Step [96/132], D Loss: 0.6931, G Loss: 4.3712\n",
      "Epoch [30/95], Step [101/132], D Loss: 0.6931, G Loss: 4.0598\n",
      "Epoch [30/95], Step [106/132], D Loss: 0.6931, G Loss: 3.4513\n",
      "Epoch [30/95], Step [111/132], D Loss: 0.6931, G Loss: 4.9022\n",
      "Epoch [30/95], Step [116/132], D Loss: 0.6931, G Loss: 4.0363\n",
      "Epoch [30/95], Step [121/132], D Loss: 0.6931, G Loss: 3.8998\n",
      "Epoch [30/95], Step [126/132], D Loss: 0.6931, G Loss: 3.8795\n",
      "Epoch [30/95], Step [131/132], D Loss: 0.6930, G Loss: 3.8887\n",
      "Epoch [30/95], Training Loss: 4.0708\n",
      "1 epoch time: 7.009117897351583\n",
      "Epoch [31/95], Step [1/132], D Loss: 0.6931, G Loss: 4.7373\n",
      "Epoch [31/95], Step [6/132], D Loss: 0.6930, G Loss: 4.0644\n",
      "Epoch [31/95], Step [11/132], D Loss: 0.6928, G Loss: 3.3420\n",
      "Epoch [31/95], Step [16/132], D Loss: 0.6924, G Loss: 3.9747\n",
      "Epoch [31/95], Step [21/132], D Loss: 0.6871, G Loss: 3.8687\n",
      "Epoch [31/95], Step [26/132], D Loss: 0.6865, G Loss: 5.5840\n",
      "Epoch [31/95], Step [31/132], D Loss: 0.6611, G Loss: 10.0011\n",
      "Epoch [31/95], Step [36/132], D Loss: 0.6198, G Loss: 5.6940\n",
      "Epoch [31/95], Step [41/132], D Loss: 0.5650, G Loss: 4.5256\n",
      "Epoch [31/95], Step [46/132], D Loss: 0.5066, G Loss: 3.9984\n",
      "Epoch [31/95], Step [51/132], D Loss: 0.5033, G Loss: 4.2607\n",
      "Epoch [31/95], Step [56/132], D Loss: 0.5052, G Loss: 4.6113\n",
      "Epoch [31/95], Step [61/132], D Loss: 0.5034, G Loss: 4.3259\n",
      "Epoch [31/95], Step [66/132], D Loss: 0.5037, G Loss: 3.8885\n",
      "Epoch [31/95], Step [71/132], D Loss: 0.5037, G Loss: 3.6053\n",
      "Epoch [31/95], Step [76/132], D Loss: 0.5033, G Loss: 4.3051\n",
      "Epoch [31/95], Step [81/132], D Loss: 0.5033, G Loss: 4.0806\n",
      "Epoch [31/95], Step [86/132], D Loss: 0.5034, G Loss: 3.7215\n",
      "Epoch [31/95], Step [91/132], D Loss: 0.5034, G Loss: 4.3114\n",
      "Epoch [31/95], Step [96/132], D Loss: 0.5033, G Loss: 3.4742\n",
      "Epoch [31/95], Step [101/132], D Loss: 0.5085, G Loss: 3.9504\n",
      "Epoch [31/95], Step [106/132], D Loss: 0.5033, G Loss: 4.1138\n",
      "Epoch [31/95], Step [111/132], D Loss: 0.5032, G Loss: 4.3017\n",
      "Epoch [31/95], Step [116/132], D Loss: 0.5033, G Loss: 4.2833\n",
      "Epoch [31/95], Step [121/132], D Loss: 0.5032, G Loss: 4.3656\n",
      "Epoch [31/95], Step [126/132], D Loss: 0.5033, G Loss: 4.8743\n",
      "Epoch [31/95], Step [131/132], D Loss: 0.5035, G Loss: 4.0085\n",
      "Epoch [31/95], Training Loss: 4.4081\n",
      "1 epoch time: 7.017625574270884\n",
      "Epoch [32/95], Step [1/132], D Loss: 0.5033, G Loss: 3.8190\n",
      "Epoch [32/95], Step [6/132], D Loss: 0.5032, G Loss: 3.3188\n",
      "Epoch [32/95], Step [11/132], D Loss: 0.5032, G Loss: 4.1112\n",
      "Epoch [32/95], Step [16/132], D Loss: 0.5032, G Loss: 4.1012\n",
      "Epoch [32/95], Step [21/132], D Loss: 0.5032, G Loss: 4.7268\n",
      "Epoch [32/95], Step [26/132], D Loss: 0.5032, G Loss: 4.4889\n",
      "Epoch [32/95], Step [31/132], D Loss: 0.5032, G Loss: 4.1365\n",
      "Epoch [32/95], Step [36/132], D Loss: 0.5215, G Loss: 3.6461\n",
      "Epoch [32/95], Step [41/132], D Loss: 0.8132, G Loss: 3.5627\n",
      "Epoch [32/95], Step [46/132], D Loss: 0.8133, G Loss: 3.8535\n",
      "Epoch [32/95], Step [51/132], D Loss: 0.8133, G Loss: 3.3989\n",
      "Epoch [32/95], Step [56/132], D Loss: 0.8133, G Loss: 3.5515\n",
      "Epoch [32/95], Step [61/132], D Loss: 0.8133, G Loss: 4.2158\n",
      "Epoch [32/95], Step [66/132], D Loss: 0.8133, G Loss: 4.4841\n",
      "Epoch [32/95], Step [71/132], D Loss: 0.8133, G Loss: 3.8201\n",
      "Epoch [32/95], Step [76/132], D Loss: 0.8133, G Loss: 3.1058\n",
      "Epoch [32/95], Step [81/132], D Loss: 0.8132, G Loss: 4.6682\n",
      "Epoch [32/95], Step [86/132], D Loss: 0.8133, G Loss: 3.8476\n",
      "Epoch [32/95], Step [91/132], D Loss: 0.8133, G Loss: 3.5469\n",
      "Epoch [32/95], Step [96/132], D Loss: 0.8133, G Loss: 3.8785\n",
      "Epoch [32/95], Step [101/132], D Loss: 0.8133, G Loss: 3.2763\n",
      "Epoch [32/95], Step [106/132], D Loss: 0.8133, G Loss: 4.6713\n",
      "Epoch [32/95], Step [111/132], D Loss: 0.8133, G Loss: 3.7169\n",
      "Epoch [32/95], Step [116/132], D Loss: 0.8133, G Loss: 3.4273\n",
      "Epoch [32/95], Step [121/132], D Loss: 0.8132, G Loss: 3.3502\n",
      "Epoch [32/95], Step [126/132], D Loss: 0.8106, G Loss: 3.5542\n",
      "Epoch [32/95], Step [131/132], D Loss: 0.7740, G Loss: 4.2873\n",
      "Epoch [32/95], Training Loss: 3.9025\n",
      "1 epoch time: 6.97867203950882\n",
      "Epoch [33/95], Step [1/132], D Loss: 0.7132, G Loss: 4.3146\n",
      "Epoch [33/95], Step [6/132], D Loss: 0.5043, G Loss: 3.7536\n",
      "Epoch [33/95], Step [11/132], D Loss: 0.5315, G Loss: 3.7025\n",
      "Epoch [33/95], Step [16/132], D Loss: 0.5047, G Loss: 3.7850\n",
      "Epoch [33/95], Step [21/132], D Loss: 0.5032, G Loss: 3.8949\n",
      "Epoch [33/95], Step [26/132], D Loss: 0.5032, G Loss: 3.7143\n",
      "Epoch [33/95], Step [31/132], D Loss: 0.5033, G Loss: 3.8729\n",
      "Epoch [33/95], Step [36/132], D Loss: 0.5033, G Loss: 3.8330\n",
      "Epoch [33/95], Step [41/132], D Loss: 0.5033, G Loss: 4.3845\n",
      "Epoch [33/95], Step [46/132], D Loss: 0.8132, G Loss: 4.3999\n",
      "Epoch [33/95], Step [51/132], D Loss: 0.6188, G Loss: 3.4310\n",
      "Epoch [33/95], Step [56/132], D Loss: 0.6931, G Loss: 3.8483\n",
      "Epoch [33/95], Step [61/132], D Loss: 0.6931, G Loss: 4.1078\n",
      "Epoch [33/95], Step [66/132], D Loss: 0.6931, G Loss: 3.8995\n",
      "Epoch [33/95], Step [71/132], D Loss: 0.6931, G Loss: 3.4058\n",
      "Epoch [33/95], Step [76/132], D Loss: 0.6931, G Loss: 4.7062\n",
      "Epoch [33/95], Step [81/132], D Loss: 0.6931, G Loss: 3.7105\n",
      "Epoch [33/95], Step [86/132], D Loss: 0.6931, G Loss: 3.3749\n",
      "Epoch [33/95], Step [91/132], D Loss: 0.6931, G Loss: 4.6414\n",
      "Epoch [33/95], Step [96/132], D Loss: 0.6931, G Loss: 3.9707\n",
      "Epoch [33/95], Step [101/132], D Loss: 0.6931, G Loss: 4.2159\n",
      "Epoch [33/95], Step [106/132], D Loss: 0.6931, G Loss: 3.6692\n",
      "Epoch [33/95], Step [111/132], D Loss: 0.6931, G Loss: 4.0080\n",
      "Epoch [33/95], Step [116/132], D Loss: 0.6931, G Loss: 4.5016\n",
      "Epoch [33/95], Step [121/132], D Loss: 0.6931, G Loss: 4.2911\n",
      "Epoch [33/95], Step [126/132], D Loss: 0.6931, G Loss: 4.1382\n",
      "Epoch [33/95], Step [131/132], D Loss: 0.6931, G Loss: 4.2575\n",
      "Epoch [33/95], Training Loss: 4.0245\n",
      "1 epoch time: 7.016478848457337\n",
      "Epoch [34/95], Step [1/132], D Loss: 0.6931, G Loss: 4.8182\n",
      "Epoch [34/95], Step [6/132], D Loss: 0.6931, G Loss: 4.2229\n",
      "Epoch [34/95], Step [11/132], D Loss: 0.6931, G Loss: 3.3079\n",
      "Epoch [34/95], Step [16/132], D Loss: 0.6931, G Loss: 3.4900\n",
      "Epoch [34/95], Step [21/132], D Loss: 0.6931, G Loss: 4.2787\n",
      "Epoch [34/95], Step [26/132], D Loss: 0.6931, G Loss: 3.3291\n",
      "Epoch [34/95], Step [31/132], D Loss: 0.6931, G Loss: 4.4276\n",
      "Epoch [34/95], Step [36/132], D Loss: 0.6931, G Loss: 3.4409\n",
      "Epoch [34/95], Step [41/132], D Loss: 0.6931, G Loss: 3.9265\n",
      "Epoch [34/95], Step [46/132], D Loss: 0.6931, G Loss: 3.9131\n",
      "Epoch [34/95], Step [51/132], D Loss: 0.6931, G Loss: 3.3370\n",
      "Epoch [34/95], Step [56/132], D Loss: 0.6931, G Loss: 3.9735\n",
      "Epoch [34/95], Step [61/132], D Loss: 0.6931, G Loss: 4.4500\n",
      "Epoch [34/95], Step [66/132], D Loss: 0.6931, G Loss: 4.0917\n",
      "Epoch [34/95], Step [71/132], D Loss: 0.6931, G Loss: 4.4160\n",
      "Epoch [34/95], Step [76/132], D Loss: 0.6931, G Loss: 4.0493\n",
      "Epoch [34/95], Step [81/132], D Loss: 0.6931, G Loss: 3.9491\n",
      "Epoch [34/95], Step [86/132], D Loss: 0.6931, G Loss: 3.3232\n",
      "Epoch [34/95], Step [91/132], D Loss: 0.6931, G Loss: 3.8623\n",
      "Epoch [34/95], Step [96/132], D Loss: 0.6931, G Loss: 4.1633\n",
      "Epoch [34/95], Step [101/132], D Loss: 0.6931, G Loss: 4.0427\n",
      "Epoch [34/95], Step [106/132], D Loss: 0.6931, G Loss: 3.5947\n",
      "Epoch [34/95], Step [111/132], D Loss: 0.6931, G Loss: 4.6381\n",
      "Epoch [34/95], Step [116/132], D Loss: 0.6931, G Loss: 3.4761\n",
      "Epoch [34/95], Step [121/132], D Loss: 0.6931, G Loss: 3.9595\n",
      "Epoch [34/95], Step [126/132], D Loss: 0.6931, G Loss: 4.2847\n",
      "Epoch [34/95], Step [131/132], D Loss: 0.6931, G Loss: 4.0882\n",
      "Epoch [34/95], Training Loss: 4.0041\n",
      "1 epoch time: 7.03086613814036\n",
      "Epoch [35/95], Step [1/132], D Loss: 0.6931, G Loss: 4.3801\n",
      "Epoch [35/95], Step [6/132], D Loss: 0.6931, G Loss: 4.4942\n",
      "Epoch [35/95], Step [11/132], D Loss: 0.6931, G Loss: 4.4262\n",
      "Epoch [35/95], Step [16/132], D Loss: 0.6931, G Loss: 3.7318\n",
      "Epoch [35/95], Step [21/132], D Loss: 0.6931, G Loss: 3.8634\n",
      "Epoch [35/95], Step [26/132], D Loss: 0.6931, G Loss: 5.0008\n",
      "Epoch [35/95], Step [31/132], D Loss: 0.6931, G Loss: 3.8620\n",
      "Epoch [35/95], Step [36/132], D Loss: 0.6931, G Loss: 3.3514\n",
      "Epoch [35/95], Step [41/132], D Loss: 0.6931, G Loss: 3.8380\n",
      "Epoch [35/95], Step [46/132], D Loss: 0.6931, G Loss: 3.6885\n",
      "Epoch [35/95], Step [51/132], D Loss: 0.6931, G Loss: 3.9239\n",
      "Epoch [35/95], Step [56/132], D Loss: 0.6931, G Loss: 3.5007\n",
      "Epoch [35/95], Step [61/132], D Loss: 0.6931, G Loss: 4.1114\n",
      "Epoch [35/95], Step [66/132], D Loss: 0.6931, G Loss: 3.6536\n",
      "Epoch [35/95], Step [71/132], D Loss: 0.6931, G Loss: 3.3035\n",
      "Epoch [35/95], Step [76/132], D Loss: 0.6931, G Loss: 3.8215\n",
      "Epoch [35/95], Step [81/132], D Loss: 0.6931, G Loss: 4.2788\n",
      "Epoch [35/95], Step [86/132], D Loss: 0.6931, G Loss: 4.2808\n",
      "Epoch [35/95], Step [91/132], D Loss: 0.6931, G Loss: 4.0087\n",
      "Epoch [35/95], Step [96/132], D Loss: 0.6931, G Loss: 4.1198\n",
      "Epoch [35/95], Step [101/132], D Loss: 0.6931, G Loss: 3.8281\n",
      "Epoch [35/95], Step [106/132], D Loss: 0.6931, G Loss: 3.6907\n",
      "Epoch [35/95], Step [111/132], D Loss: 0.6931, G Loss: 4.8998\n",
      "Epoch [35/95], Step [116/132], D Loss: 0.6931, G Loss: 3.4127\n",
      "Epoch [35/95], Step [121/132], D Loss: 0.6931, G Loss: 3.4197\n",
      "Epoch [35/95], Step [126/132], D Loss: 0.6931, G Loss: 3.5362\n",
      "Epoch [35/95], Step [131/132], D Loss: 0.6931, G Loss: 3.8912\n",
      "Epoch [35/95], Training Loss: 4.0071\n",
      "1 epoch time: 7.029431736469268\n",
      "Epoch [36/95], Step [1/132], D Loss: 0.6931, G Loss: 4.7215\n",
      "Epoch [36/95], Step [6/132], D Loss: 0.6931, G Loss: 3.8062\n",
      "Epoch [36/95], Step [11/132], D Loss: 0.6931, G Loss: 3.7672\n",
      "Epoch [36/95], Step [16/132], D Loss: 0.6931, G Loss: 4.1957\n",
      "Epoch [36/95], Step [21/132], D Loss: 0.6931, G Loss: 4.4697\n",
      "Epoch [36/95], Step [26/132], D Loss: 0.6931, G Loss: 4.3128\n",
      "Epoch [36/95], Step [31/132], D Loss: 0.6931, G Loss: 4.0509\n",
      "Epoch [36/95], Step [36/132], D Loss: 0.6931, G Loss: 3.9827\n",
      "Epoch [36/95], Step [41/132], D Loss: 0.6931, G Loss: 3.5523\n",
      "Epoch [36/95], Step [46/132], D Loss: 0.6931, G Loss: 3.5869\n",
      "Epoch [36/95], Step [51/132], D Loss: 0.6931, G Loss: 3.8170\n",
      "Epoch [36/95], Step [56/132], D Loss: 0.6931, G Loss: 4.0684\n",
      "Epoch [36/95], Step [61/132], D Loss: 0.6931, G Loss: 3.2239\n",
      "Epoch [36/95], Step [66/132], D Loss: 0.6931, G Loss: 4.0399\n",
      "Epoch [36/95], Step [71/132], D Loss: 0.6931, G Loss: 4.0375\n",
      "Epoch [36/95], Step [76/132], D Loss: 0.6931, G Loss: 3.8916\n",
      "Epoch [36/95], Step [81/132], D Loss: 0.6931, G Loss: 3.4508\n",
      "Epoch [36/95], Step [86/132], D Loss: 0.6931, G Loss: 4.0208\n",
      "Epoch [36/95], Step [91/132], D Loss: 0.6931, G Loss: 3.8843\n",
      "Epoch [36/95], Step [96/132], D Loss: 0.6931, G Loss: 3.8765\n",
      "Epoch [36/95], Step [101/132], D Loss: 0.6931, G Loss: 3.9352\n",
      "Epoch [36/95], Step [106/132], D Loss: 0.6931, G Loss: 4.1466\n",
      "Epoch [36/95], Step [111/132], D Loss: 0.6931, G Loss: 3.7309\n",
      "Epoch [36/95], Step [116/132], D Loss: 0.6931, G Loss: 3.9088\n",
      "Epoch [36/95], Step [121/132], D Loss: 0.6931, G Loss: 4.3140\n",
      "Epoch [36/95], Step [126/132], D Loss: 0.6931, G Loss: 3.7656\n",
      "Epoch [36/95], Step [131/132], D Loss: 0.6931, G Loss: 3.8313\n",
      "Epoch [36/95], Training Loss: 3.9669\n",
      "1 epoch time: 7.037881008783976\n",
      "Epoch [37/95], Step [1/132], D Loss: 0.6931, G Loss: 4.4201\n",
      "Epoch [37/95], Step [6/132], D Loss: 0.6931, G Loss: 3.7956\n",
      "Epoch [37/95], Step [11/132], D Loss: 0.6931, G Loss: 3.6477\n",
      "Epoch [37/95], Step [16/132], D Loss: 0.6931, G Loss: 4.7135\n",
      "Epoch [37/95], Step [21/132], D Loss: 0.6931, G Loss: 4.2173\n",
      "Epoch [37/95], Step [26/132], D Loss: 0.6931, G Loss: 4.0684\n",
      "Epoch [37/95], Step [31/132], D Loss: 0.6931, G Loss: 4.8486\n",
      "Epoch [37/95], Step [36/132], D Loss: 0.6931, G Loss: 4.4963\n",
      "Epoch [37/95], Step [41/132], D Loss: 0.6931, G Loss: 3.8414\n",
      "Epoch [37/95], Step [46/132], D Loss: 0.6931, G Loss: 3.4523\n",
      "Epoch [37/95], Step [51/132], D Loss: 0.6931, G Loss: 3.4156\n",
      "Epoch [37/95], Step [56/132], D Loss: 0.6931, G Loss: 4.7924\n",
      "Epoch [37/95], Step [61/132], D Loss: 0.6931, G Loss: 3.8166\n",
      "Epoch [37/95], Step [66/132], D Loss: 0.6931, G Loss: 3.4453\n",
      "Epoch [37/95], Step [71/132], D Loss: 0.6931, G Loss: 3.4780\n",
      "Epoch [37/95], Step [76/132], D Loss: 0.6931, G Loss: 3.6428\n",
      "Epoch [37/95], Step [81/132], D Loss: 0.6931, G Loss: 3.5271\n",
      "Epoch [37/95], Step [86/132], D Loss: 0.6931, G Loss: 4.0609\n",
      "Epoch [37/95], Step [91/132], D Loss: 0.6931, G Loss: 3.2993\n",
      "Epoch [37/95], Step [96/132], D Loss: 0.6931, G Loss: 3.9542\n",
      "Epoch [37/95], Step [101/132], D Loss: 0.6931, G Loss: 4.0817\n",
      "Epoch [37/95], Step [106/132], D Loss: 0.6931, G Loss: 4.3376\n",
      "Epoch [37/95], Step [111/132], D Loss: 0.6931, G Loss: 4.4131\n",
      "Epoch [37/95], Step [116/132], D Loss: 0.6931, G Loss: 3.3910\n",
      "Epoch [37/95], Step [121/132], D Loss: 0.6931, G Loss: 3.7539\n",
      "Epoch [37/95], Step [126/132], D Loss: 0.6931, G Loss: 3.7499\n",
      "Epoch [37/95], Step [131/132], D Loss: 0.6931, G Loss: 4.3535\n",
      "Epoch [37/95], Training Loss: 3.9662\n",
      "1 epoch time: 7.033326109250386\n",
      "Epoch [38/95], Step [1/132], D Loss: 0.6931, G Loss: 4.6562\n",
      "Epoch [38/95], Step [6/132], D Loss: 0.6931, G Loss: 4.1273\n",
      "Epoch [38/95], Step [11/132], D Loss: 0.6931, G Loss: 3.9587\n",
      "Epoch [38/95], Step [16/132], D Loss: 0.6931, G Loss: 3.6710\n",
      "Epoch [38/95], Step [21/132], D Loss: 0.6931, G Loss: 4.0754\n",
      "Epoch [38/95], Step [26/132], D Loss: 0.6931, G Loss: 3.8662\n",
      "Epoch [38/95], Step [31/132], D Loss: 0.6931, G Loss: 3.8951\n",
      "Epoch [38/95], Step [36/132], D Loss: 0.6931, G Loss: 4.3686\n",
      "Epoch [38/95], Step [41/132], D Loss: 0.6931, G Loss: 4.0278\n",
      "Epoch [38/95], Step [46/132], D Loss: 0.6931, G Loss: 3.9372\n",
      "Epoch [38/95], Step [51/132], D Loss: 0.6931, G Loss: 4.0040\n",
      "Epoch [38/95], Step [56/132], D Loss: 0.6931, G Loss: 4.6506\n",
      "Epoch [38/95], Step [61/132], D Loss: 0.6931, G Loss: 4.3361\n",
      "Epoch [38/95], Step [66/132], D Loss: 0.6931, G Loss: 4.2838\n",
      "Epoch [38/95], Step [71/132], D Loss: 0.6931, G Loss: 3.9305\n",
      "Epoch [38/95], Step [76/132], D Loss: 0.6931, G Loss: 3.4988\n",
      "Epoch [38/95], Step [81/132], D Loss: 0.6931, G Loss: 3.8236\n",
      "Epoch [38/95], Step [86/132], D Loss: 0.6931, G Loss: 3.9582\n",
      "Epoch [38/95], Step [91/132], D Loss: 0.6931, G Loss: 3.9725\n",
      "Epoch [38/95], Step [96/132], D Loss: 0.6931, G Loss: 3.6596\n",
      "Epoch [38/95], Step [101/132], D Loss: 0.6931, G Loss: 3.9847\n",
      "Epoch [38/95], Step [106/132], D Loss: 0.6931, G Loss: 3.8617\n",
      "Epoch [38/95], Step [111/132], D Loss: 0.6931, G Loss: 4.2528\n",
      "Epoch [38/95], Step [116/132], D Loss: 0.6931, G Loss: 3.9628\n",
      "Epoch [38/95], Step [121/132], D Loss: 0.6931, G Loss: 4.1948\n",
      "Epoch [38/95], Step [126/132], D Loss: 0.6931, G Loss: 4.0879\n",
      "Epoch [38/95], Step [131/132], D Loss: 0.6931, G Loss: 3.4950\n",
      "Epoch [38/95], Training Loss: 3.9469\n",
      "1 epoch time: 7.074620473384857\n",
      "Epoch [39/95], Step [1/132], D Loss: 0.6931, G Loss: 4.0473\n",
      "Epoch [39/95], Step [6/132], D Loss: 0.6931, G Loss: 3.9867\n",
      "Epoch [39/95], Step [11/132], D Loss: 0.6931, G Loss: 3.7231\n",
      "Epoch [39/95], Step [16/132], D Loss: 0.6931, G Loss: 3.6657\n",
      "Epoch [39/95], Step [21/132], D Loss: 0.6931, G Loss: 3.8389\n",
      "Epoch [39/95], Step [26/132], D Loss: 0.6931, G Loss: 3.5858\n",
      "Epoch [39/95], Step [31/132], D Loss: 0.6931, G Loss: 4.5582\n",
      "Epoch [39/95], Step [36/132], D Loss: 0.6931, G Loss: 3.9725\n",
      "Epoch [39/95], Step [41/132], D Loss: 0.6931, G Loss: 3.0233\n",
      "Epoch [39/95], Step [46/132], D Loss: 0.6931, G Loss: 4.6817\n",
      "Epoch [39/95], Step [51/132], D Loss: 0.6931, G Loss: 3.9909\n",
      "Epoch [39/95], Step [56/132], D Loss: 0.6931, G Loss: 3.7421\n",
      "Epoch [39/95], Step [61/132], D Loss: 0.6931, G Loss: 3.4597\n",
      "Epoch [39/95], Step [66/132], D Loss: 0.6931, G Loss: 3.7603\n",
      "Epoch [39/95], Step [71/132], D Loss: 0.6931, G Loss: 4.2250\n",
      "Epoch [39/95], Step [76/132], D Loss: 0.6931, G Loss: 3.6166\n",
      "Epoch [39/95], Step [81/132], D Loss: 0.6931, G Loss: 4.2888\n",
      "Epoch [39/95], Step [86/132], D Loss: 0.6931, G Loss: 4.0896\n",
      "Epoch [39/95], Step [91/132], D Loss: 0.6931, G Loss: 3.9098\n",
      "Epoch [39/95], Step [96/132], D Loss: 0.6931, G Loss: 3.7081\n",
      "Epoch [39/95], Step [101/132], D Loss: 0.6931, G Loss: 3.8884\n",
      "Epoch [39/95], Step [106/132], D Loss: 0.6931, G Loss: 4.3368\n",
      "Epoch [39/95], Step [111/132], D Loss: 0.6931, G Loss: 3.7161\n",
      "Epoch [39/95], Step [116/132], D Loss: 0.6931, G Loss: 3.8312\n",
      "Epoch [39/95], Step [121/132], D Loss: 0.6931, G Loss: 4.2592\n",
      "Epoch [39/95], Step [126/132], D Loss: 0.6931, G Loss: 3.8958\n",
      "Epoch [39/95], Step [131/132], D Loss: 0.6931, G Loss: 3.3135\n",
      "Epoch [39/95], Training Loss: 3.9358\n",
      "1 epoch time: 7.0482628583908085\n",
      "Epoch [40/95], Step [1/132], D Loss: 0.6931, G Loss: 3.8685\n",
      "Epoch [40/95], Step [6/132], D Loss: 0.6931, G Loss: 4.0450\n",
      "Epoch [40/95], Step [11/132], D Loss: 0.6931, G Loss: 3.7355\n",
      "Epoch [40/95], Step [16/132], D Loss: 0.6931, G Loss: 4.0841\n",
      "Epoch [40/95], Step [21/132], D Loss: 0.6931, G Loss: 3.9347\n",
      "Epoch [40/95], Step [26/132], D Loss: 0.6931, G Loss: 3.7379\n",
      "Epoch [40/95], Step [31/132], D Loss: 0.6931, G Loss: 4.0966\n",
      "Epoch [40/95], Step [36/132], D Loss: 0.6931, G Loss: 3.3872\n",
      "Epoch [40/95], Step [41/132], D Loss: 0.6931, G Loss: 4.1711\n",
      "Epoch [40/95], Step [46/132], D Loss: 0.6931, G Loss: 4.3793\n",
      "Epoch [40/95], Step [51/132], D Loss: 0.6931, G Loss: 4.0364\n",
      "Epoch [40/95], Step [56/132], D Loss: 0.6931, G Loss: 4.1223\n",
      "Epoch [40/95], Step [61/132], D Loss: 0.6931, G Loss: 4.1355\n",
      "Epoch [40/95], Step [66/132], D Loss: 0.6931, G Loss: 4.2701\n",
      "Epoch [40/95], Step [71/132], D Loss: 0.6931, G Loss: 3.4235\n",
      "Epoch [40/95], Step [76/132], D Loss: 0.6931, G Loss: 3.5905\n",
      "Epoch [40/95], Step [81/132], D Loss: 0.6931, G Loss: 3.5412\n",
      "Epoch [40/95], Step [86/132], D Loss: 0.6931, G Loss: 3.9116\n",
      "Epoch [40/95], Step [91/132], D Loss: 0.6931, G Loss: 3.3269\n",
      "Epoch [40/95], Step [96/132], D Loss: 0.6931, G Loss: 3.6580\n",
      "Epoch [40/95], Step [101/132], D Loss: 0.6931, G Loss: 3.3968\n",
      "Epoch [40/95], Step [106/132], D Loss: 0.6931, G Loss: 3.7574\n",
      "Epoch [40/95], Step [111/132], D Loss: 0.6931, G Loss: 4.0873\n",
      "Epoch [40/95], Step [116/132], D Loss: 0.6931, G Loss: 4.8090\n",
      "Epoch [40/95], Step [121/132], D Loss: 0.6931, G Loss: 4.8880\n",
      "Epoch [40/95], Step [126/132], D Loss: 0.6931, G Loss: 3.5154\n",
      "Epoch [40/95], Step [131/132], D Loss: 0.6931, G Loss: 4.0032\n",
      "Epoch [40/95], Training Loss: 3.9189\n",
      "1 epoch time: 7.038679198424021\n",
      "Epoch [41/95], Step [1/132], D Loss: 0.6931, G Loss: 3.8066\n",
      "Epoch [41/95], Step [6/132], D Loss: 0.6931, G Loss: 3.9188\n",
      "Epoch [41/95], Step [11/132], D Loss: 0.6931, G Loss: 3.7063\n",
      "Epoch [41/95], Step [16/132], D Loss: 0.6931, G Loss: 3.8664\n",
      "Epoch [41/95], Step [21/132], D Loss: 0.6931, G Loss: 3.6791\n",
      "Epoch [41/95], Step [26/132], D Loss: 0.6931, G Loss: 3.9817\n",
      "Epoch [41/95], Step [31/132], D Loss: 0.6931, G Loss: 3.8601\n",
      "Epoch [41/95], Step [36/132], D Loss: 0.6931, G Loss: 3.5592\n",
      "Epoch [41/95], Step [41/132], D Loss: 0.6931, G Loss: 3.5858\n",
      "Epoch [41/95], Step [46/132], D Loss: 0.6931, G Loss: 3.8654\n",
      "Epoch [41/95], Step [51/132], D Loss: 0.6931, G Loss: 4.1572\n",
      "Epoch [41/95], Step [56/132], D Loss: 0.6931, G Loss: 3.4734\n",
      "Epoch [41/95], Step [61/132], D Loss: 0.6931, G Loss: 3.7831\n",
      "Epoch [41/95], Step [66/132], D Loss: 0.6931, G Loss: 3.5449\n",
      "Epoch [41/95], Step [71/132], D Loss: 0.6931, G Loss: 3.7105\n",
      "Epoch [41/95], Step [76/132], D Loss: 0.6931, G Loss: 4.3089\n",
      "Epoch [41/95], Step [81/132], D Loss: 0.6931, G Loss: 3.8800\n",
      "Epoch [41/95], Step [86/132], D Loss: 0.6931, G Loss: 3.6547\n",
      "Epoch [41/95], Step [91/132], D Loss: 0.6931, G Loss: 4.1641\n",
      "Epoch [41/95], Step [96/132], D Loss: 0.6931, G Loss: 3.7974\n",
      "Epoch [41/95], Step [101/132], D Loss: 0.6931, G Loss: 4.1014\n",
      "Epoch [41/95], Step [106/132], D Loss: 0.6931, G Loss: 4.1706\n",
      "Epoch [41/95], Step [111/132], D Loss: 0.6931, G Loss: 3.7029\n",
      "Epoch [41/95], Step [116/132], D Loss: 0.6931, G Loss: 3.6673\n",
      "Epoch [41/95], Step [121/132], D Loss: 0.6931, G Loss: 3.8086\n",
      "Epoch [41/95], Step [126/132], D Loss: 0.6931, G Loss: 3.2957\n",
      "Epoch [41/95], Step [131/132], D Loss: 0.6931, G Loss: 4.0293\n",
      "Epoch [41/95], Training Loss: 3.8973\n",
      "1 epoch time: 7.045944702625275\n",
      "Epoch [42/95], Step [1/132], D Loss: 0.6931, G Loss: 4.1691\n",
      "Epoch [42/95], Step [6/132], D Loss: 0.6931, G Loss: 3.6431\n",
      "Epoch [42/95], Step [11/132], D Loss: 0.6931, G Loss: 3.6492\n",
      "Epoch [42/95], Step [16/132], D Loss: 0.6931, G Loss: 4.5790\n",
      "Epoch [42/95], Step [21/132], D Loss: 0.6931, G Loss: 3.9042\n",
      "Epoch [42/95], Step [26/132], D Loss: 0.6931, G Loss: 3.2322\n",
      "Epoch [42/95], Step [31/132], D Loss: 0.6931, G Loss: 3.4193\n",
      "Epoch [42/95], Step [36/132], D Loss: 0.6931, G Loss: 4.1384\n",
      "Epoch [42/95], Step [41/132], D Loss: 0.6931, G Loss: 4.1375\n",
      "Epoch [42/95], Step [46/132], D Loss: 0.6931, G Loss: 3.9508\n",
      "Epoch [42/95], Step [51/132], D Loss: 0.6931, G Loss: 4.0430\n",
      "Epoch [42/95], Step [56/132], D Loss: 0.6931, G Loss: 3.8862\n",
      "Epoch [42/95], Step [61/132], D Loss: 0.6931, G Loss: 3.8774\n",
      "Epoch [42/95], Step [66/132], D Loss: 0.6931, G Loss: 3.5852\n",
      "Epoch [42/95], Step [71/132], D Loss: 0.6931, G Loss: 3.6791\n",
      "Epoch [42/95], Step [76/132], D Loss: 0.6931, G Loss: 3.5559\n",
      "Epoch [42/95], Step [81/132], D Loss: 0.6931, G Loss: 4.2481\n",
      "Epoch [42/95], Step [86/132], D Loss: 0.6931, G Loss: 3.8981\n",
      "Epoch [42/95], Step [91/132], D Loss: 0.6931, G Loss: 3.9464\n",
      "Epoch [42/95], Step [96/132], D Loss: 0.6931, G Loss: 3.6497\n",
      "Epoch [42/95], Step [101/132], D Loss: 0.6931, G Loss: 3.7143\n",
      "Epoch [42/95], Step [106/132], D Loss: 0.6931, G Loss: 3.9307\n",
      "Epoch [42/95], Step [111/132], D Loss: 0.6931, G Loss: 3.9611\n",
      "Epoch [42/95], Step [116/132], D Loss: 0.6931, G Loss: 3.9212\n",
      "Epoch [42/95], Step [121/132], D Loss: 0.6931, G Loss: 4.1463\n",
      "Epoch [42/95], Step [126/132], D Loss: 0.6931, G Loss: 4.0483\n",
      "Epoch [42/95], Step [131/132], D Loss: 0.6931, G Loss: 3.6661\n",
      "Epoch [42/95], Training Loss: 3.8886\n",
      "1 epoch time: 7.0327163179715475\n",
      "Epoch [43/95], Step [1/132], D Loss: 0.6931, G Loss: 3.7826\n",
      "Epoch [43/95], Step [6/132], D Loss: 0.6931, G Loss: 3.7829\n",
      "Epoch [43/95], Step [11/132], D Loss: 0.6931, G Loss: 3.7633\n",
      "Epoch [43/95], Step [16/132], D Loss: 0.6931, G Loss: 3.1368\n",
      "Epoch [43/95], Step [21/132], D Loss: 0.6931, G Loss: 4.2094\n",
      "Epoch [43/95], Step [26/132], D Loss: 0.6931, G Loss: 4.6464\n",
      "Epoch [43/95], Step [31/132], D Loss: 0.6931, G Loss: 3.1274\n",
      "Epoch [43/95], Step [36/132], D Loss: 0.6931, G Loss: 3.6503\n",
      "Epoch [43/95], Step [41/132], D Loss: 0.6931, G Loss: 3.5401\n",
      "Epoch [43/95], Step [46/132], D Loss: 0.6931, G Loss: 3.8729\n",
      "Epoch [43/95], Step [51/132], D Loss: 0.6931, G Loss: 4.2435\n",
      "Epoch [43/95], Step [56/132], D Loss: 0.6931, G Loss: 3.2301\n",
      "Epoch [43/95], Step [61/132], D Loss: 0.6931, G Loss: 4.2922\n",
      "Epoch [43/95], Step [66/132], D Loss: 0.6931, G Loss: 3.6253\n",
      "Epoch [43/95], Step [71/132], D Loss: 0.6931, G Loss: 4.0129\n",
      "Epoch [43/95], Step [76/132], D Loss: 0.6931, G Loss: 3.6676\n",
      "Epoch [43/95], Step [81/132], D Loss: 0.6931, G Loss: 2.9958\n",
      "Epoch [43/95], Step [86/132], D Loss: 0.6931, G Loss: 3.7455\n",
      "Epoch [43/95], Step [91/132], D Loss: 0.6931, G Loss: 3.2972\n",
      "Epoch [43/95], Step [96/132], D Loss: 0.6931, G Loss: 3.7162\n",
      "Epoch [43/95], Step [101/132], D Loss: 0.6931, G Loss: 5.1139\n",
      "Epoch [43/95], Step [106/132], D Loss: 0.6931, G Loss: 4.4713\n",
      "Epoch [43/95], Step [111/132], D Loss: 0.6931, G Loss: 4.7114\n",
      "Epoch [43/95], Step [116/132], D Loss: 0.6931, G Loss: 3.6545\n",
      "Epoch [43/95], Step [121/132], D Loss: 0.6931, G Loss: 3.7906\n",
      "Epoch [43/95], Step [126/132], D Loss: 0.6931, G Loss: 4.2481\n",
      "Epoch [43/95], Step [131/132], D Loss: 0.6931, G Loss: 4.3073\n",
      "Epoch [43/95], Training Loss: 3.8891\n",
      "1 epoch time: 7.0079840580622355\n",
      "Epoch [44/95], Step [1/132], D Loss: 0.6931, G Loss: 3.8432\n",
      "Epoch [44/95], Step [6/132], D Loss: 0.6931, G Loss: 3.5549\n",
      "Epoch [44/95], Step [11/132], D Loss: 0.6931, G Loss: 4.2685\n",
      "Epoch [44/95], Step [16/132], D Loss: 0.6931, G Loss: 3.8534\n",
      "Epoch [44/95], Step [21/132], D Loss: 0.6931, G Loss: 3.4780\n",
      "Epoch [44/95], Step [26/132], D Loss: 0.6931, G Loss: 3.8611\n",
      "Epoch [44/95], Step [31/132], D Loss: 0.6931, G Loss: 4.0540\n",
      "Epoch [44/95], Step [36/132], D Loss: 0.6931, G Loss: 3.9788\n",
      "Epoch [44/95], Step [41/132], D Loss: 0.6931, G Loss: 4.1850\n",
      "Epoch [44/95], Step [46/132], D Loss: 0.6931, G Loss: 3.6416\n",
      "Epoch [44/95], Step [51/132], D Loss: 0.6931, G Loss: 3.7240\n",
      "Epoch [44/95], Step [56/132], D Loss: 0.6931, G Loss: 3.7648\n",
      "Epoch [44/95], Step [61/132], D Loss: 0.6931, G Loss: 3.8529\n",
      "Epoch [44/95], Step [66/132], D Loss: 0.6931, G Loss: 4.2791\n",
      "Epoch [44/95], Step [71/132], D Loss: 0.6931, G Loss: 4.2989\n",
      "Epoch [44/95], Step [76/132], D Loss: 0.6931, G Loss: 3.6907\n",
      "Epoch [44/95], Step [81/132], D Loss: 0.6931, G Loss: 4.5661\n",
      "Epoch [44/95], Step [86/132], D Loss: 0.6931, G Loss: 3.8738\n",
      "Epoch [44/95], Step [91/132], D Loss: 0.6931, G Loss: 3.5868\n",
      "Epoch [44/95], Step [96/132], D Loss: 0.6931, G Loss: 4.2423\n",
      "Epoch [44/95], Step [101/132], D Loss: 0.6931, G Loss: 4.2748\n",
      "Epoch [44/95], Step [106/132], D Loss: 0.6931, G Loss: 3.3414\n",
      "Epoch [44/95], Step [111/132], D Loss: 0.6931, G Loss: 3.6766\n",
      "Epoch [44/95], Step [116/132], D Loss: 0.6931, G Loss: 3.5215\n",
      "Epoch [44/95], Step [121/132], D Loss: 0.6931, G Loss: 4.0470\n",
      "Epoch [44/95], Step [126/132], D Loss: 0.6931, G Loss: 3.9771\n",
      "Epoch [44/95], Step [131/132], D Loss: 0.6931, G Loss: 3.4641\n",
      "Epoch [44/95], Training Loss: 3.8735\n",
      "1 epoch time: 7.056172180175781\n",
      "Epoch [45/95], Step [1/132], D Loss: 0.6931, G Loss: 3.1097\n",
      "Epoch [45/95], Step [6/132], D Loss: 0.6931, G Loss: 3.6230\n",
      "Epoch [45/95], Step [11/132], D Loss: 0.6931, G Loss: 3.7295\n",
      "Epoch [45/95], Step [16/132], D Loss: 0.6931, G Loss: 4.1232\n",
      "Epoch [45/95], Step [21/132], D Loss: 0.6931, G Loss: 3.3818\n",
      "Epoch [45/95], Step [26/132], D Loss: 0.6931, G Loss: 3.6888\n",
      "Epoch [45/95], Step [31/132], D Loss: 0.6931, G Loss: 4.0941\n",
      "Epoch [45/95], Step [36/132], D Loss: 0.6931, G Loss: 4.6237\n",
      "Epoch [45/95], Step [41/132], D Loss: 0.6931, G Loss: 4.0074\n",
      "Epoch [45/95], Step [46/132], D Loss: 0.6931, G Loss: 5.0247\n",
      "Epoch [45/95], Step [51/132], D Loss: 0.6931, G Loss: 3.6825\n",
      "Epoch [45/95], Step [56/132], D Loss: 0.6931, G Loss: 3.5265\n",
      "Epoch [45/95], Step [61/132], D Loss: 0.6931, G Loss: 4.3957\n",
      "Epoch [45/95], Step [66/132], D Loss: 0.6931, G Loss: 3.9004\n",
      "Epoch [45/95], Step [71/132], D Loss: 0.6931, G Loss: 3.9714\n",
      "Epoch [45/95], Step [76/132], D Loss: 0.6931, G Loss: 3.1788\n",
      "Epoch [45/95], Step [81/132], D Loss: 0.6931, G Loss: 3.6705\n",
      "Epoch [45/95], Step [86/132], D Loss: 0.6931, G Loss: 3.9933\n",
      "Epoch [45/95], Step [91/132], D Loss: 0.6931, G Loss: 3.9512\n",
      "Epoch [45/95], Step [96/132], D Loss: 0.6931, G Loss: 3.9233\n",
      "Epoch [45/95], Step [101/132], D Loss: 0.6931, G Loss: 4.3308\n",
      "Epoch [45/95], Step [106/132], D Loss: 0.6931, G Loss: 4.0011\n",
      "Epoch [45/95], Step [111/132], D Loss: 0.6931, G Loss: 3.6845\n",
      "Epoch [45/95], Step [116/132], D Loss: 0.6931, G Loss: 4.4601\n",
      "Epoch [45/95], Step [121/132], D Loss: 0.6931, G Loss: 4.8879\n",
      "Epoch [45/95], Step [126/132], D Loss: 0.6931, G Loss: 3.4973\n",
      "Epoch [45/95], Step [131/132], D Loss: 0.6931, G Loss: 3.6774\n",
      "Epoch [45/95], Training Loss: 3.8561\n",
      "1 epoch time: 7.05968089501063\n",
      "Epoch [46/95], Step [1/132], D Loss: 0.6931, G Loss: 4.2056\n",
      "Epoch [46/95], Step [6/132], D Loss: 0.6931, G Loss: 4.0812\n",
      "Epoch [46/95], Step [11/132], D Loss: 0.6931, G Loss: 3.6489\n",
      "Epoch [46/95], Step [16/132], D Loss: 0.6931, G Loss: 3.7103\n",
      "Epoch [46/95], Step [21/132], D Loss: 0.6931, G Loss: 3.6366\n",
      "Epoch [46/95], Step [26/132], D Loss: 0.6931, G Loss: 3.8107\n",
      "Epoch [46/95], Step [31/132], D Loss: 0.6930, G Loss: 3.9008\n",
      "Epoch [46/95], Step [36/132], D Loss: 0.6883, G Loss: 3.4386\n",
      "Epoch [46/95], Step [41/132], D Loss: 0.6766, G Loss: 3.4190\n",
      "Epoch [46/95], Step [46/132], D Loss: 0.6676, G Loss: 3.9337\n",
      "Epoch [46/95], Step [51/132], D Loss: 0.6991, G Loss: 4.0584\n",
      "Epoch [46/95], Step [56/132], D Loss: 0.6930, G Loss: 4.6103\n",
      "Epoch [46/95], Step [61/132], D Loss: 0.6931, G Loss: 3.6890\n",
      "Epoch [46/95], Step [66/132], D Loss: 0.6931, G Loss: 4.2687\n",
      "Epoch [46/95], Step [71/132], D Loss: 0.6931, G Loss: 4.4228\n",
      "Epoch [46/95], Step [76/132], D Loss: 0.6931, G Loss: 3.5624\n",
      "Epoch [46/95], Step [81/132], D Loss: 0.6931, G Loss: 4.2835\n",
      "Epoch [46/95], Step [86/132], D Loss: 0.6931, G Loss: 2.9487\n",
      "Epoch [46/95], Step [91/132], D Loss: 0.6931, G Loss: 3.4737\n",
      "Epoch [46/95], Step [96/132], D Loss: 0.6927, G Loss: 3.7096\n",
      "Epoch [46/95], Step [101/132], D Loss: 0.6868, G Loss: 3.5939\n",
      "Epoch [46/95], Step [106/132], D Loss: 0.6585, G Loss: 4.4490\n",
      "Epoch [46/95], Step [111/132], D Loss: 0.6001, G Loss: 3.6205\n",
      "Epoch [46/95], Step [116/132], D Loss: 0.6041, G Loss: 4.1283\n",
      "Epoch [46/95], Step [121/132], D Loss: 0.7443, G Loss: 4.0361\n",
      "Epoch [46/95], Step [126/132], D Loss: 0.6353, G Loss: 3.2296\n",
      "Epoch [46/95], Step [131/132], D Loss: 0.5765, G Loss: 3.7174\n",
      "Epoch [46/95], Training Loss: 3.8821\n",
      "1 epoch time: 7.04561520020167\n",
      "Epoch [47/95], Step [1/132], D Loss: 0.7820, G Loss: 4.2022\n",
      "Epoch [47/95], Step [6/132], D Loss: 0.6365, G Loss: 4.0281\n",
      "Epoch [47/95], Step [11/132], D Loss: 0.6884, G Loss: 4.1576\n",
      "Epoch [47/95], Step [16/132], D Loss: 0.6637, G Loss: 4.1084\n",
      "Epoch [47/95], Step [21/132], D Loss: 0.6439, G Loss: 3.9971\n",
      "Epoch [47/95], Step [26/132], D Loss: 0.6177, G Loss: 3.5965\n",
      "Epoch [47/95], Step [31/132], D Loss: 0.6980, G Loss: 4.0945\n",
      "Epoch [47/95], Step [36/132], D Loss: 0.6441, G Loss: 4.0403\n",
      "Epoch [47/95], Step [41/132], D Loss: 0.6877, G Loss: 3.6448\n",
      "Epoch [47/95], Step [46/132], D Loss: 0.6542, G Loss: 4.2968\n",
      "Epoch [47/95], Step [51/132], D Loss: 0.5635, G Loss: 3.7912\n",
      "Epoch [47/95], Step [56/132], D Loss: 0.5850, G Loss: 4.3148\n",
      "Epoch [47/95], Step [61/132], D Loss: 0.7028, G Loss: 3.6468\n",
      "Epoch [47/95], Step [66/132], D Loss: 0.6931, G Loss: 3.8011\n",
      "Epoch [47/95], Step [71/132], D Loss: 0.6931, G Loss: 3.4126\n",
      "Epoch [47/95], Step [76/132], D Loss: 0.6918, G Loss: 4.0176\n",
      "Epoch [47/95], Step [81/132], D Loss: 0.5737, G Loss: 3.8889\n",
      "Epoch [47/95], Step [86/132], D Loss: 0.6177, G Loss: 3.9013\n",
      "Epoch [47/95], Step [91/132], D Loss: 0.8081, G Loss: 4.0544\n",
      "Epoch [47/95], Step [96/132], D Loss: 0.6929, G Loss: 3.9585\n",
      "Epoch [47/95], Step [101/132], D Loss: 0.6083, G Loss: 3.7488\n",
      "Epoch [47/95], Step [106/132], D Loss: 0.6929, G Loss: 3.7396\n",
      "Epoch [47/95], Step [111/132], D Loss: 0.7038, G Loss: 3.8701\n",
      "Epoch [47/95], Step [116/132], D Loss: 0.6931, G Loss: 3.8487\n",
      "Epoch [47/95], Step [121/132], D Loss: 0.6931, G Loss: 3.7233\n",
      "Epoch [47/95], Step [126/132], D Loss: 0.6929, G Loss: 3.3441\n",
      "Epoch [47/95], Step [131/132], D Loss: 0.6930, G Loss: 3.4430\n",
      "Epoch [47/95], Training Loss: 3.8300\n",
      "1 epoch time: 7.064795788129171\n",
      "Epoch [48/95], Step [1/132], D Loss: 0.6931, G Loss: 3.4300\n",
      "Epoch [48/95], Step [6/132], D Loss: 0.6896, G Loss: 3.5677\n",
      "Epoch [48/95], Step [11/132], D Loss: 0.6832, G Loss: 4.1028\n",
      "Epoch [48/95], Step [16/132], D Loss: 0.6626, G Loss: 4.3286\n",
      "Epoch [48/95], Step [21/132], D Loss: 0.6121, G Loss: 4.1769\n",
      "Epoch [48/95], Step [26/132], D Loss: 0.7892, G Loss: 3.5586\n",
      "Epoch [48/95], Step [31/132], D Loss: 0.6223, G Loss: 4.0273\n",
      "Epoch [48/95], Step [36/132], D Loss: 0.7820, G Loss: 4.2039\n",
      "Epoch [48/95], Step [41/132], D Loss: 0.6577, G Loss: 3.6420\n",
      "Epoch [48/95], Step [46/132], D Loss: 0.6288, G Loss: 4.2301\n",
      "Epoch [48/95], Step [51/132], D Loss: 0.6929, G Loss: 3.8597\n",
      "Epoch [48/95], Step [56/132], D Loss: 0.6882, G Loss: 3.2802\n",
      "Epoch [48/95], Step [61/132], D Loss: 0.5208, G Loss: 4.3912\n",
      "Epoch [48/95], Step [66/132], D Loss: 0.6871, G Loss: 3.4480\n",
      "Epoch [48/95], Step [71/132], D Loss: 0.6130, G Loss: 3.8074\n",
      "Epoch [48/95], Step [76/132], D Loss: 0.5155, G Loss: 3.3932\n",
      "Epoch [48/95], Step [81/132], D Loss: 0.8312, G Loss: 4.5922\n",
      "Epoch [48/95], Step [86/132], D Loss: 0.6626, G Loss: 3.4852\n",
      "Epoch [48/95], Step [91/132], D Loss: 0.6910, G Loss: 3.8227\n",
      "Epoch [48/95], Step [96/132], D Loss: 0.8094, G Loss: 3.5507\n",
      "Epoch [48/95], Step [101/132], D Loss: 0.6468, G Loss: 3.8093\n",
      "Epoch [48/95], Step [106/132], D Loss: 0.7010, G Loss: 3.6233\n",
      "Epoch [48/95], Step [111/132], D Loss: 0.7075, G Loss: 3.6992\n",
      "Epoch [48/95], Step [116/132], D Loss: 0.6579, G Loss: 4.0236\n",
      "Epoch [48/95], Step [121/132], D Loss: 0.5505, G Loss: 3.4018\n",
      "Epoch [48/95], Step [126/132], D Loss: 0.6125, G Loss: 3.5095\n",
      "Epoch [48/95], Step [131/132], D Loss: 0.8074, G Loss: 3.8701\n",
      "Epoch [48/95], Training Loss: 3.8087\n",
      "1 epoch time: 7.04070334037145\n",
      "Epoch [49/95], Step [1/132], D Loss: 0.6699, G Loss: 4.2724\n",
      "Epoch [49/95], Step [6/132], D Loss: 0.5424, G Loss: 4.2284\n",
      "Epoch [49/95], Step [11/132], D Loss: 0.7386, G Loss: 4.4529\n",
      "Epoch [49/95], Step [16/132], D Loss: 0.6920, G Loss: 3.9446\n",
      "Epoch [49/95], Step [21/132], D Loss: 0.7072, G Loss: 3.9612\n",
      "Epoch [49/95], Step [26/132], D Loss: 0.6930, G Loss: 3.8565\n",
      "Epoch [49/95], Step [31/132], D Loss: 0.6929, G Loss: 4.1820\n",
      "Epoch [49/95], Step [36/132], D Loss: 0.6930, G Loss: 3.8590\n",
      "Epoch [49/95], Step [41/132], D Loss: 0.6892, G Loss: 4.6247\n",
      "Epoch [49/95], Step [46/132], D Loss: 0.5325, G Loss: 4.1156\n",
      "Epoch [49/95], Step [51/132], D Loss: 0.5129, G Loss: 4.5993\n",
      "Epoch [49/95], Step [56/132], D Loss: 0.5085, G Loss: 5.1376\n",
      "Epoch [49/95], Step [61/132], D Loss: 0.6650, G Loss: 4.5847\n",
      "Epoch [49/95], Step [66/132], D Loss: 0.5107, G Loss: 3.8242\n",
      "Epoch [49/95], Step [71/132], D Loss: 0.5049, G Loss: 3.8648\n",
      "Epoch [49/95], Step [76/132], D Loss: 0.5039, G Loss: 3.2926\n",
      "Epoch [49/95], Step [81/132], D Loss: 0.5042, G Loss: 3.5904\n",
      "Epoch [49/95], Step [86/132], D Loss: 0.5033, G Loss: 4.0925\n",
      "Epoch [49/95], Step [91/132], D Loss: 0.5033, G Loss: 3.6318\n",
      "Epoch [49/95], Step [96/132], D Loss: 0.5080, G Loss: 3.7378\n",
      "Epoch [49/95], Step [101/132], D Loss: 0.5053, G Loss: 3.6834\n",
      "Epoch [49/95], Step [106/132], D Loss: 0.5259, G Loss: 3.6857\n",
      "Epoch [49/95], Step [111/132], D Loss: 0.5036, G Loss: 3.6307\n",
      "Epoch [49/95], Step [116/132], D Loss: 0.7843, G Loss: 3.1401\n",
      "Epoch [49/95], Step [121/132], D Loss: 0.6292, G Loss: 4.1098\n",
      "Epoch [49/95], Step [126/132], D Loss: 0.5116, G Loss: 4.1401\n",
      "Epoch [49/95], Step [131/132], D Loss: 0.5108, G Loss: 3.7899\n",
      "Epoch [49/95], Training Loss: 3.8426\n",
      "1 epoch time: 7.022776246070862\n",
      "Epoch [50/95], Step [1/132], D Loss: 0.5035, G Loss: 3.2684\n",
      "Epoch [50/95], Step [6/132], D Loss: 0.6287, G Loss: 3.2218\n",
      "Epoch [50/95], Step [11/132], D Loss: 0.5035, G Loss: 3.7777\n",
      "Epoch [50/95], Step [16/132], D Loss: 0.5033, G Loss: 3.8959\n",
      "Epoch [50/95], Step [21/132], D Loss: 0.5032, G Loss: 4.5019\n",
      "Epoch [50/95], Step [26/132], D Loss: 0.5494, G Loss: 3.9613\n",
      "Epoch [50/95], Step [31/132], D Loss: 0.5337, G Loss: 3.5942\n",
      "Epoch [50/95], Step [36/132], D Loss: 0.5982, G Loss: 4.4303\n",
      "Epoch [50/95], Step [41/132], D Loss: 0.5032, G Loss: 3.6761\n",
      "Epoch [50/95], Step [46/132], D Loss: 0.5032, G Loss: 4.4134\n",
      "Epoch [50/95], Step [51/132], D Loss: 0.5033, G Loss: 3.9316\n",
      "Epoch [50/95], Step [56/132], D Loss: 0.5531, G Loss: 3.5798\n",
      "Epoch [50/95], Step [61/132], D Loss: 0.5982, G Loss: 3.9699\n",
      "Epoch [50/95], Step [66/132], D Loss: 0.5214, G Loss: 3.1409\n",
      "Epoch [50/95], Step [71/132], D Loss: 0.8133, G Loss: 3.8683\n",
      "Epoch [50/95], Step [76/132], D Loss: 0.8132, G Loss: 3.7905\n",
      "Epoch [50/95], Step [81/132], D Loss: 0.7005, G Loss: 3.9406\n",
      "Epoch [50/95], Step [86/132], D Loss: 0.6940, G Loss: 3.9662\n",
      "Epoch [50/95], Step [91/132], D Loss: 0.7944, G Loss: 3.7249\n",
      "Epoch [50/95], Step [96/132], D Loss: 0.6801, G Loss: 3.3648\n",
      "Epoch [50/95], Step [101/132], D Loss: 0.5399, G Loss: 3.7850\n",
      "Epoch [50/95], Step [106/132], D Loss: 0.6931, G Loss: 3.8344\n",
      "Epoch [50/95], Step [111/132], D Loss: 0.6931, G Loss: 3.6459\n",
      "Epoch [50/95], Step [116/132], D Loss: 0.6931, G Loss: 3.7515\n",
      "Epoch [50/95], Step [121/132], D Loss: 0.6931, G Loss: 3.6692\n",
      "Epoch [50/95], Step [126/132], D Loss: 0.6931, G Loss: 3.5822\n",
      "Epoch [50/95], Step [131/132], D Loss: 0.6931, G Loss: 3.7495\n",
      "Epoch [50/95], Training Loss: 3.8178\n",
      "1 epoch time: 7.052492435773214\n",
      "Epoch [51/95], Step [1/132], D Loss: 0.6931, G Loss: 4.2448\n",
      "Epoch [51/95], Step [6/132], D Loss: 0.6931, G Loss: 4.0589\n",
      "Epoch [51/95], Step [11/132], D Loss: 0.6931, G Loss: 3.5766\n",
      "Epoch [51/95], Step [16/132], D Loss: 0.6931, G Loss: 3.4472\n",
      "Epoch [51/95], Step [21/132], D Loss: 0.6931, G Loss: 3.4939\n",
      "Epoch [51/95], Step [26/132], D Loss: 0.6931, G Loss: 4.1212\n",
      "Epoch [51/95], Step [31/132], D Loss: 0.6931, G Loss: 3.6282\n",
      "Epoch [51/95], Step [36/132], D Loss: 0.6930, G Loss: 3.8655\n",
      "Epoch [51/95], Step [41/132], D Loss: 0.6931, G Loss: 3.2502\n",
      "Epoch [51/95], Step [46/132], D Loss: 0.6931, G Loss: 3.5991\n",
      "Epoch [51/95], Step [51/132], D Loss: 0.6931, G Loss: 3.7176\n",
      "Epoch [51/95], Step [56/132], D Loss: 0.6931, G Loss: 3.5364\n",
      "Epoch [51/95], Step [61/132], D Loss: 0.6931, G Loss: 3.4038\n",
      "Epoch [51/95], Step [66/132], D Loss: 0.6931, G Loss: 3.4555\n",
      "Epoch [51/95], Step [71/132], D Loss: 0.6931, G Loss: 3.7751\n",
      "Epoch [51/95], Step [76/132], D Loss: 0.6931, G Loss: 3.4463\n",
      "Epoch [51/95], Step [81/132], D Loss: 0.6930, G Loss: 3.1457\n",
      "Epoch [51/95], Step [86/132], D Loss: 0.6931, G Loss: 4.0115\n",
      "Epoch [51/95], Step [91/132], D Loss: 0.6931, G Loss: 4.1768\n",
      "Epoch [51/95], Step [96/132], D Loss: 0.6931, G Loss: 3.5956\n",
      "Epoch [51/95], Step [101/132], D Loss: 0.6929, G Loss: 4.2042\n",
      "Epoch [51/95], Step [106/132], D Loss: 0.6684, G Loss: 3.5250\n",
      "Epoch [51/95], Step [111/132], D Loss: 0.6203, G Loss: 3.2105\n",
      "Epoch [51/95], Step [116/132], D Loss: 0.5037, G Loss: 3.4937\n",
      "Epoch [51/95], Step [121/132], D Loss: 0.5922, G Loss: 3.9417\n",
      "Epoch [51/95], Step [126/132], D Loss: 0.5032, G Loss: 3.4587\n",
      "Epoch [51/95], Step [131/132], D Loss: 0.5033, G Loss: 3.7594\n",
      "Epoch [51/95], Training Loss: 3.7630\n",
      "1 epoch time: 7.023270750045777\n",
      "Epoch [52/95], Step [1/132], D Loss: 0.5033, G Loss: 3.6960\n",
      "Epoch [52/95], Step [6/132], D Loss: 0.5032, G Loss: 4.3088\n",
      "Epoch [52/95], Step [11/132], D Loss: 0.5051, G Loss: 3.4870\n",
      "Epoch [52/95], Step [16/132], D Loss: 0.5032, G Loss: 3.7054\n",
      "Epoch [52/95], Step [21/132], D Loss: 0.5032, G Loss: 4.4342\n",
      "Epoch [52/95], Step [26/132], D Loss: 0.5033, G Loss: 4.0898\n",
      "Epoch [52/95], Step [31/132], D Loss: 0.5033, G Loss: 3.7024\n",
      "Epoch [52/95], Step [36/132], D Loss: 0.5032, G Loss: 3.8258\n",
      "Epoch [52/95], Step [41/132], D Loss: 0.5032, G Loss: 3.3577\n",
      "Epoch [52/95], Step [46/132], D Loss: 0.5037, G Loss: 3.6935\n",
      "Epoch [52/95], Step [51/132], D Loss: 0.5036, G Loss: 4.2820\n",
      "Epoch [52/95], Step [56/132], D Loss: 0.5037, G Loss: 3.7236\n",
      "Epoch [52/95], Step [61/132], D Loss: 0.5032, G Loss: 3.3352\n",
      "Epoch [52/95], Step [66/132], D Loss: 0.8133, G Loss: 3.2660\n",
      "Epoch [52/95], Step [71/132], D Loss: 0.8133, G Loss: 3.0904\n",
      "Epoch [52/95], Step [76/132], D Loss: 0.6813, G Loss: 4.4995\n",
      "Epoch [52/95], Step [81/132], D Loss: 0.5032, G Loss: 3.4132\n",
      "Epoch [52/95], Step [86/132], D Loss: 0.5049, G Loss: 4.2486\n",
      "Epoch [52/95], Step [91/132], D Loss: 0.6804, G Loss: 4.1843\n",
      "Epoch [52/95], Step [96/132], D Loss: 0.8088, G Loss: 3.1109\n",
      "Epoch [52/95], Step [101/132], D Loss: 0.7210, G Loss: 3.0714\n",
      "Epoch [52/95], Step [106/132], D Loss: 0.6903, G Loss: 3.8389\n",
      "Epoch [52/95], Step [111/132], D Loss: 0.5722, G Loss: 3.7574\n",
      "Epoch [52/95], Step [116/132], D Loss: 0.7332, G Loss: 4.3534\n",
      "Epoch [52/95], Step [121/132], D Loss: 0.6931, G Loss: 3.5381\n",
      "Epoch [52/95], Step [126/132], D Loss: 0.6931, G Loss: 3.4876\n",
      "Epoch [52/95], Step [131/132], D Loss: 0.6931, G Loss: 3.3899\n",
      "Epoch [52/95], Training Loss: 3.7561\n",
      "1 epoch time: 6.999192035198211\n",
      "Epoch [53/95], Step [1/132], D Loss: 0.6931, G Loss: 3.5819\n",
      "Epoch [53/95], Step [6/132], D Loss: 0.6925, G Loss: 3.4029\n",
      "Epoch [53/95], Step [11/132], D Loss: 0.6506, G Loss: 4.0240\n",
      "Epoch [53/95], Step [16/132], D Loss: 0.5794, G Loss: 3.3115\n",
      "Epoch [53/95], Step [21/132], D Loss: 0.7664, G Loss: 3.8267\n",
      "Epoch [53/95], Step [26/132], D Loss: 0.5904, G Loss: 3.7631\n",
      "Epoch [53/95], Step [31/132], D Loss: 0.6078, G Loss: 4.1788\n",
      "Epoch [53/95], Step [36/132], D Loss: 0.6885, G Loss: 3.9627\n",
      "Epoch [53/95], Step [41/132], D Loss: 0.5079, G Loss: 4.0218\n",
      "Epoch [53/95], Step [46/132], D Loss: 0.5211, G Loss: 3.4295\n",
      "Epoch [53/95], Step [51/132], D Loss: 0.7598, G Loss: 3.9903\n",
      "Epoch [53/95], Step [56/132], D Loss: 0.7256, G Loss: 3.4787\n",
      "Epoch [53/95], Step [61/132], D Loss: 0.7649, G Loss: 3.8238\n",
      "Epoch [53/95], Step [66/132], D Loss: 0.7553, G Loss: 3.8039\n",
      "Epoch [53/95], Step [71/132], D Loss: 0.6502, G Loss: 4.0938\n",
      "Epoch [53/95], Step [76/132], D Loss: 0.6247, G Loss: 3.7721\n",
      "Epoch [53/95], Step [81/132], D Loss: 0.6513, G Loss: 3.9559\n",
      "Epoch [53/95], Step [86/132], D Loss: 0.5857, G Loss: 3.9122\n",
      "Epoch [53/95], Step [91/132], D Loss: 0.6801, G Loss: 3.4584\n",
      "Epoch [53/95], Step [96/132], D Loss: 0.6852, G Loss: 3.5087\n",
      "Epoch [53/95], Step [101/132], D Loss: 0.6027, G Loss: 4.2363\n",
      "Epoch [53/95], Step [106/132], D Loss: 0.6404, G Loss: 4.1044\n",
      "Epoch [53/95], Step [111/132], D Loss: 0.5570, G Loss: 3.6629\n",
      "Epoch [53/95], Step [116/132], D Loss: 0.5048, G Loss: 4.0663\n",
      "Epoch [53/95], Step [121/132], D Loss: 0.5164, G Loss: 3.7704\n",
      "Epoch [53/95], Step [126/132], D Loss: 0.5073, G Loss: 3.5897\n",
      "Epoch [53/95], Step [131/132], D Loss: 0.5065, G Loss: 3.6913\n",
      "Epoch [53/95], Training Loss: 3.7797\n",
      "1 epoch time: 7.047080322106679\n",
      "Epoch [54/95], Step [1/132], D Loss: 0.5479, G Loss: 3.7494\n",
      "Epoch [54/95], Step [6/132], D Loss: 0.5391, G Loss: 4.0988\n",
      "Epoch [54/95], Step [11/132], D Loss: 0.5032, G Loss: 3.6560\n",
      "Epoch [54/95], Step [16/132], D Loss: 0.5035, G Loss: 4.2809\n",
      "Epoch [54/95], Step [21/132], D Loss: 0.5033, G Loss: 3.6976\n",
      "Epoch [54/95], Step [26/132], D Loss: 0.5724, G Loss: 3.1623\n",
      "Epoch [54/95], Step [31/132], D Loss: 0.5137, G Loss: 3.4413\n",
      "Epoch [54/95], Step [36/132], D Loss: 0.7771, G Loss: 4.2386\n",
      "Epoch [54/95], Step [41/132], D Loss: 0.5623, G Loss: 3.5147\n",
      "Epoch [54/95], Step [46/132], D Loss: 0.5448, G Loss: 3.6240\n",
      "Epoch [54/95], Step [51/132], D Loss: 0.5032, G Loss: 4.4019\n",
      "Epoch [54/95], Step [56/132], D Loss: 0.5032, G Loss: 3.4454\n",
      "Epoch [54/95], Step [61/132], D Loss: 0.5033, G Loss: 4.2500\n",
      "Epoch [54/95], Step [66/132], D Loss: 0.5033, G Loss: 3.5004\n",
      "Epoch [54/95], Step [71/132], D Loss: 0.5032, G Loss: 4.1055\n",
      "Epoch [54/95], Step [76/132], D Loss: 0.5038, G Loss: 4.4647\n",
      "Epoch [54/95], Step [81/132], D Loss: 0.5032, G Loss: 3.5963\n",
      "Epoch [54/95], Step [86/132], D Loss: 0.5032, G Loss: 3.4834\n",
      "Epoch [54/95], Step [91/132], D Loss: 0.5032, G Loss: 3.6666\n",
      "Epoch [54/95], Step [96/132], D Loss: 0.5032, G Loss: 4.0346\n",
      "Epoch [54/95], Step [101/132], D Loss: 0.5032, G Loss: 3.7505\n",
      "Epoch [54/95], Step [106/132], D Loss: 0.5032, G Loss: 3.3967\n",
      "Epoch [54/95], Step [111/132], D Loss: 0.5032, G Loss: 4.2661\n",
      "Epoch [54/95], Step [116/132], D Loss: 0.5033, G Loss: 3.3819\n",
      "Epoch [54/95], Step [121/132], D Loss: 0.5032, G Loss: 3.6275\n",
      "Epoch [54/95], Step [126/132], D Loss: 0.5032, G Loss: 3.6579\n",
      "Epoch [54/95], Step [131/132], D Loss: 0.5034, G Loss: 3.7843\n",
      "Epoch [54/95], Training Loss: 3.7912\n",
      "1 epoch time: 6.985655184586843\n",
      "Epoch [55/95], Step [1/132], D Loss: 0.5032, G Loss: 4.1803\n",
      "Epoch [55/95], Step [6/132], D Loss: 0.5032, G Loss: 3.8059\n",
      "Epoch [55/95], Step [11/132], D Loss: 0.5032, G Loss: 4.1241\n",
      "Epoch [55/95], Step [16/132], D Loss: 0.5034, G Loss: 3.8684\n",
      "Epoch [55/95], Step [21/132], D Loss: 0.5238, G Loss: 3.7439\n",
      "Epoch [55/95], Step [26/132], D Loss: 0.5032, G Loss: 3.7914\n",
      "Epoch [55/95], Step [31/132], D Loss: 0.5032, G Loss: 3.8322\n",
      "Epoch [55/95], Step [36/132], D Loss: 0.5032, G Loss: 4.2329\n",
      "Epoch [55/95], Step [41/132], D Loss: 0.5032, G Loss: 3.8223\n",
      "Epoch [55/95], Step [46/132], D Loss: 0.5032, G Loss: 3.9202\n",
      "Epoch [55/95], Step [51/132], D Loss: 0.5032, G Loss: 3.2952\n",
      "Epoch [55/95], Step [56/132], D Loss: 0.5032, G Loss: 3.9559\n",
      "Epoch [55/95], Step [61/132], D Loss: 0.5032, G Loss: 4.4744\n",
      "Epoch [55/95], Step [66/132], D Loss: 0.5032, G Loss: 4.1980\n",
      "Epoch [55/95], Step [71/132], D Loss: 0.5032, G Loss: 4.2474\n",
      "Epoch [55/95], Step [76/132], D Loss: 0.5035, G Loss: 3.5650\n",
      "Epoch [55/95], Step [81/132], D Loss: 0.5116, G Loss: 3.5397\n",
      "Epoch [55/95], Step [86/132], D Loss: 0.6303, G Loss: 3.7475\n",
      "Epoch [55/95], Step [91/132], D Loss: 0.5768, G Loss: 3.4791\n",
      "Epoch [55/95], Step [96/132], D Loss: 0.5032, G Loss: 4.0900\n",
      "Epoch [55/95], Step [101/132], D Loss: 0.5032, G Loss: 3.7006\n",
      "Epoch [55/95], Step [106/132], D Loss: 0.5032, G Loss: 4.0618\n",
      "Epoch [55/95], Step [111/132], D Loss: 0.5032, G Loss: 4.0231\n",
      "Epoch [55/95], Step [116/132], D Loss: 0.5035, G Loss: 4.2900\n",
      "Epoch [55/95], Step [121/132], D Loss: 0.5032, G Loss: 3.6100\n",
      "Epoch [55/95], Step [126/132], D Loss: 0.5032, G Loss: 3.9857\n",
      "Epoch [55/95], Step [131/132], D Loss: 0.5032, G Loss: 3.5528\n",
      "Epoch [55/95], Training Loss: 3.7458\n",
      "1 epoch time: 7.052826861540477\n",
      "Epoch [56/95], Step [1/132], D Loss: 0.5033, G Loss: 3.8885\n",
      "Epoch [56/95], Step [6/132], D Loss: 0.5032, G Loss: 3.5674\n",
      "Epoch [56/95], Step [11/132], D Loss: 0.5035, G Loss: 3.4046\n",
      "Epoch [56/95], Step [16/132], D Loss: 0.5741, G Loss: 4.0779\n",
      "Epoch [56/95], Step [21/132], D Loss: 0.5036, G Loss: 3.5659\n",
      "Epoch [56/95], Step [26/132], D Loss: 0.5062, G Loss: 4.2055\n",
      "Epoch [56/95], Step [31/132], D Loss: 0.5033, G Loss: 3.3365\n",
      "Epoch [56/95], Step [36/132], D Loss: 0.5032, G Loss: 3.6428\n",
      "Epoch [56/95], Step [41/132], D Loss: 0.5032, G Loss: 3.9793\n",
      "Epoch [56/95], Step [46/132], D Loss: 0.5032, G Loss: 3.0592\n",
      "Epoch [56/95], Step [51/132], D Loss: 0.5051, G Loss: 4.3443\n",
      "Epoch [56/95], Step [56/132], D Loss: 0.5032, G Loss: 3.1198\n",
      "Epoch [56/95], Step [61/132], D Loss: 0.5032, G Loss: 3.9843\n",
      "Epoch [56/95], Step [66/132], D Loss: 0.5032, G Loss: 3.9211\n",
      "Epoch [56/95], Step [71/132], D Loss: 0.5032, G Loss: 3.7067\n",
      "Epoch [56/95], Step [76/132], D Loss: 0.5032, G Loss: 3.3004\n",
      "Epoch [56/95], Step [81/132], D Loss: 0.5032, G Loss: 3.6146\n",
      "Epoch [56/95], Step [86/132], D Loss: 0.5034, G Loss: 3.9120\n",
      "Epoch [56/95], Step [91/132], D Loss: 0.5032, G Loss: 2.9928\n",
      "Epoch [56/95], Step [96/132], D Loss: 0.5032, G Loss: 3.0642\n",
      "Epoch [56/95], Step [101/132], D Loss: 0.5032, G Loss: 4.1184\n",
      "Epoch [56/95], Step [106/132], D Loss: 0.5032, G Loss: 4.0646\n",
      "Epoch [56/95], Step [111/132], D Loss: 0.5039, G Loss: 4.2992\n",
      "Epoch [56/95], Step [116/132], D Loss: 0.5150, G Loss: 3.1491\n",
      "Epoch [56/95], Step [121/132], D Loss: 0.5034, G Loss: 4.1677\n",
      "Epoch [56/95], Step [126/132], D Loss: 0.5033, G Loss: 3.4155\n",
      "Epoch [56/95], Step [131/132], D Loss: 0.5037, G Loss: 3.0959\n",
      "Epoch [56/95], Training Loss: 3.7555\n",
      "1 epoch time: 7.053223296006521\n",
      "Epoch [57/95], Step [1/132], D Loss: 0.5032, G Loss: 4.0633\n",
      "Epoch [57/95], Step [6/132], D Loss: 0.5032, G Loss: 4.0697\n",
      "Epoch [57/95], Step [11/132], D Loss: 0.6620, G Loss: 3.4987\n",
      "Epoch [57/95], Step [16/132], D Loss: 0.5033, G Loss: 4.1661\n",
      "Epoch [57/95], Step [21/132], D Loss: 0.5136, G Loss: 3.5987\n",
      "Epoch [57/95], Step [26/132], D Loss: 0.5032, G Loss: 3.3736\n",
      "Epoch [57/95], Step [31/132], D Loss: 0.5032, G Loss: 3.7451\n",
      "Epoch [57/95], Step [36/132], D Loss: 0.5032, G Loss: 3.3645\n",
      "Epoch [57/95], Step [41/132], D Loss: 0.5032, G Loss: 3.7533\n",
      "Epoch [57/95], Step [46/132], D Loss: 0.5032, G Loss: 4.1410\n",
      "Epoch [57/95], Step [51/132], D Loss: 0.5032, G Loss: 3.4736\n",
      "Epoch [57/95], Step [56/132], D Loss: 0.5032, G Loss: 3.8806\n",
      "Epoch [57/95], Step [61/132], D Loss: 0.5066, G Loss: 3.1634\n",
      "Epoch [57/95], Step [66/132], D Loss: 0.6354, G Loss: 4.5486\n",
      "Epoch [57/95], Step [71/132], D Loss: 0.5033, G Loss: 3.9096\n",
      "Epoch [57/95], Step [76/132], D Loss: 0.5033, G Loss: 3.8843\n",
      "Epoch [57/95], Step [81/132], D Loss: 0.5033, G Loss: 3.4070\n",
      "Epoch [57/95], Step [86/132], D Loss: 0.5032, G Loss: 3.6109\n",
      "Epoch [57/95], Step [91/132], D Loss: 0.5032, G Loss: 3.6533\n",
      "Epoch [57/95], Step [96/132], D Loss: 0.5032, G Loss: 3.8740\n",
      "Epoch [57/95], Step [101/132], D Loss: 0.5032, G Loss: 3.8972\n",
      "Epoch [57/95], Step [106/132], D Loss: 0.5032, G Loss: 3.7088\n",
      "Epoch [57/95], Step [111/132], D Loss: 0.5032, G Loss: 3.1632\n",
      "Epoch [57/95], Step [116/132], D Loss: 0.5032, G Loss: 3.5384\n",
      "Epoch [57/95], Step [121/132], D Loss: 0.5032, G Loss: 3.5935\n",
      "Epoch [57/95], Step [126/132], D Loss: 0.5032, G Loss: 3.6757\n",
      "Epoch [57/95], Step [131/132], D Loss: 0.5032, G Loss: 3.4716\n",
      "Epoch [57/95], Training Loss: 3.7515\n",
      "1 epoch time: 7.05282641251882\n",
      "Epoch [58/95], Step [1/132], D Loss: 0.5032, G Loss: 3.8461\n",
      "Epoch [58/95], Step [6/132], D Loss: 0.5032, G Loss: 3.6957\n",
      "Epoch [58/95], Step [11/132], D Loss: 0.5032, G Loss: 3.9089\n",
      "Epoch [58/95], Step [16/132], D Loss: 0.5032, G Loss: 3.7730\n",
      "Epoch [58/95], Step [21/132], D Loss: 0.5032, G Loss: 4.2834\n",
      "Epoch [58/95], Step [26/132], D Loss: 0.5032, G Loss: 3.5894\n",
      "Epoch [58/95], Step [31/132], D Loss: 0.5032, G Loss: 3.2486\n",
      "Epoch [58/95], Step [36/132], D Loss: 0.5032, G Loss: 3.9461\n",
      "Epoch [58/95], Step [41/132], D Loss: 0.5032, G Loss: 4.1296\n",
      "Epoch [58/95], Step [46/132], D Loss: 0.5032, G Loss: 3.9641\n",
      "Epoch [58/95], Step [51/132], D Loss: 0.5032, G Loss: 3.8235\n",
      "Epoch [58/95], Step [56/132], D Loss: 0.5032, G Loss: 4.4299\n",
      "Epoch [58/95], Step [61/132], D Loss: 0.5032, G Loss: 3.2469\n",
      "Epoch [58/95], Step [66/132], D Loss: 0.5032, G Loss: 3.6446\n",
      "Epoch [58/95], Step [71/132], D Loss: 0.8133, G Loss: 4.2419\n",
      "Epoch [58/95], Step [76/132], D Loss: 0.8120, G Loss: 3.3876\n",
      "Epoch [58/95], Step [81/132], D Loss: 0.8093, G Loss: 3.1264\n",
      "Epoch [58/95], Step [86/132], D Loss: 0.7192, G Loss: 3.3378\n",
      "Epoch [58/95], Step [91/132], D Loss: 0.5433, G Loss: 2.6526\n",
      "Epoch [58/95], Step [96/132], D Loss: 0.5817, G Loss: 3.3862\n",
      "Epoch [58/95], Step [101/132], D Loss: 0.6925, G Loss: 3.0055\n",
      "Epoch [58/95], Step [106/132], D Loss: 0.6336, G Loss: 3.3376\n",
      "Epoch [58/95], Step [111/132], D Loss: 0.5090, G Loss: 4.2225\n",
      "Epoch [58/95], Step [116/132], D Loss: 0.5036, G Loss: 4.1228\n",
      "Epoch [58/95], Step [121/132], D Loss: 0.5032, G Loss: 4.0129\n",
      "Epoch [58/95], Step [126/132], D Loss: 0.5033, G Loss: 4.7128\n",
      "Epoch [58/95], Step [131/132], D Loss: 0.5032, G Loss: 3.4106\n",
      "Epoch [58/95], Training Loss: 3.7270\n",
      "1 epoch time: 7.06308749516805\n",
      "Epoch [59/95], Step [1/132], D Loss: 0.5032, G Loss: 3.9231\n",
      "Epoch [59/95], Step [6/132], D Loss: 0.5047, G Loss: 3.6414\n",
      "Epoch [59/95], Step [11/132], D Loss: 0.5035, G Loss: 3.1972\n",
      "Epoch [59/95], Step [16/132], D Loss: 0.5032, G Loss: 3.9478\n",
      "Epoch [59/95], Step [21/132], D Loss: 0.5032, G Loss: 4.1176\n",
      "Epoch [59/95], Step [26/132], D Loss: 0.5032, G Loss: 4.4873\n",
      "Epoch [59/95], Step [31/132], D Loss: 0.5032, G Loss: 3.1432\n",
      "Epoch [59/95], Step [36/132], D Loss: 0.5032, G Loss: 3.7465\n",
      "Epoch [59/95], Step [41/132], D Loss: 0.5033, G Loss: 4.0753\n",
      "Epoch [59/95], Step [46/132], D Loss: 0.5032, G Loss: 3.9124\n",
      "Epoch [59/95], Step [51/132], D Loss: 0.5032, G Loss: 3.7196\n",
      "Epoch [59/95], Step [56/132], D Loss: 0.5032, G Loss: 3.7154\n",
      "Epoch [59/95], Step [61/132], D Loss: 0.5032, G Loss: 3.3704\n",
      "Epoch [59/95], Step [66/132], D Loss: 0.5032, G Loss: 3.9747\n",
      "Epoch [59/95], Step [71/132], D Loss: 0.5032, G Loss: 3.6129\n",
      "Epoch [59/95], Step [76/132], D Loss: 0.5032, G Loss: 3.9256\n",
      "Epoch [59/95], Step [81/132], D Loss: 0.5032, G Loss: 4.0008\n",
      "Epoch [59/95], Step [86/132], D Loss: 0.5032, G Loss: 3.1992\n",
      "Epoch [59/95], Step [91/132], D Loss: 0.5566, G Loss: 4.0361\n",
      "Epoch [59/95], Step [96/132], D Loss: 0.5034, G Loss: 3.8157\n",
      "Epoch [59/95], Step [101/132], D Loss: 0.8138, G Loss: 3.5391\n",
      "Epoch [59/95], Step [106/132], D Loss: 0.6794, G Loss: 3.8219\n",
      "Epoch [59/95], Step [111/132], D Loss: 0.5109, G Loss: 3.6082\n",
      "Epoch [59/95], Step [116/132], D Loss: 0.5469, G Loss: 3.3611\n",
      "Epoch [59/95], Step [121/132], D Loss: 0.5594, G Loss: 4.0387\n",
      "Epoch [59/95], Step [126/132], D Loss: 0.5032, G Loss: 3.6116\n",
      "Epoch [59/95], Step [131/132], D Loss: 0.5033, G Loss: 3.9704\n",
      "Epoch [59/95], Training Loss: 3.7366\n",
      "1 epoch time: 7.037327222029368\n",
      "Epoch [60/95], Step [1/132], D Loss: 0.5032, G Loss: 3.1930\n",
      "Epoch [60/95], Step [6/132], D Loss: 0.5632, G Loss: 3.4444\n",
      "Epoch [60/95], Step [11/132], D Loss: 0.5032, G Loss: 3.6132\n",
      "Epoch [60/95], Step [16/132], D Loss: 0.5032, G Loss: 3.6092\n",
      "Epoch [60/95], Step [21/132], D Loss: 0.5032, G Loss: 3.5890\n",
      "Epoch [60/95], Step [26/132], D Loss: 0.5032, G Loss: 3.4181\n",
      "Epoch [60/95], Step [31/132], D Loss: 0.5033, G Loss: 4.2640\n",
      "Epoch [60/95], Step [36/132], D Loss: 0.5440, G Loss: 4.1638\n",
      "Epoch [60/95], Step [41/132], D Loss: 0.5033, G Loss: 4.4131\n",
      "Epoch [60/95], Step [46/132], D Loss: 0.5566, G Loss: 3.9144\n",
      "Epoch [60/95], Step [51/132], D Loss: 0.5039, G Loss: 3.8002\n",
      "Epoch [60/95], Step [56/132], D Loss: 0.5032, G Loss: 3.5984\n",
      "Epoch [60/95], Step [61/132], D Loss: 0.5032, G Loss: 3.7434\n",
      "Epoch [60/95], Step [66/132], D Loss: 0.5032, G Loss: 3.1113\n",
      "Epoch [60/95], Step [71/132], D Loss: 0.5033, G Loss: 3.0545\n",
      "Epoch [60/95], Step [76/132], D Loss: 0.5032, G Loss: 4.3220\n",
      "Epoch [60/95], Step [81/132], D Loss: 0.5032, G Loss: 4.9853\n",
      "Epoch [60/95], Step [86/132], D Loss: 0.5032, G Loss: 3.5061\n",
      "Epoch [60/95], Step [91/132], D Loss: 0.5032, G Loss: 3.7180\n",
      "Epoch [60/95], Step [96/132], D Loss: 0.5032, G Loss: 3.7916\n",
      "Epoch [60/95], Step [101/132], D Loss: 0.5032, G Loss: 3.2829\n",
      "Epoch [60/95], Step [106/132], D Loss: 0.5032, G Loss: 3.6714\n",
      "Epoch [60/95], Step [111/132], D Loss: 0.5032, G Loss: 3.1767\n",
      "Epoch [60/95], Step [116/132], D Loss: 0.5032, G Loss: 3.7042\n",
      "Epoch [60/95], Step [121/132], D Loss: 0.5032, G Loss: 4.1583\n",
      "Epoch [60/95], Step [126/132], D Loss: 0.5032, G Loss: 4.1214\n",
      "Epoch [60/95], Step [131/132], D Loss: 0.5033, G Loss: 3.2188\n",
      "Epoch [60/95], Training Loss: 3.7173\n",
      "1 epoch time: 7.067102098464966\n",
      "Epoch [61/95], Step [1/132], D Loss: 0.5032, G Loss: 3.6781\n",
      "Epoch [61/95], Step [6/132], D Loss: 0.5032, G Loss: 3.7393\n",
      "Epoch [61/95], Step [11/132], D Loss: 0.5032, G Loss: 3.9415\n",
      "Epoch [61/95], Step [16/132], D Loss: 0.5032, G Loss: 4.1309\n",
      "Epoch [61/95], Step [21/132], D Loss: 0.5032, G Loss: 3.4127\n",
      "Epoch [61/95], Step [26/132], D Loss: 0.5033, G Loss: 3.1289\n",
      "Epoch [61/95], Step [31/132], D Loss: 0.5032, G Loss: 3.7593\n",
      "Epoch [61/95], Step [36/132], D Loss: 0.5032, G Loss: 3.3076\n",
      "Epoch [61/95], Step [41/132], D Loss: 0.5032, G Loss: 3.8982\n",
      "Epoch [61/95], Step [46/132], D Loss: 0.5032, G Loss: 3.3586\n",
      "Epoch [61/95], Step [51/132], D Loss: 0.5032, G Loss: 3.6326\n",
      "Epoch [61/95], Step [56/132], D Loss: 0.5032, G Loss: 3.6941\n",
      "Epoch [61/95], Step [61/132], D Loss: 0.5032, G Loss: 3.8217\n",
      "Epoch [61/95], Step [66/132], D Loss: 0.5032, G Loss: 3.6683\n",
      "Epoch [61/95], Step [71/132], D Loss: 0.5032, G Loss: 3.8131\n",
      "Epoch [61/95], Step [76/132], D Loss: 0.5032, G Loss: 3.1510\n",
      "Epoch [61/95], Step [81/132], D Loss: 0.5032, G Loss: 4.1141\n",
      "Epoch [61/95], Step [86/132], D Loss: 0.5032, G Loss: 3.9879\n",
      "Epoch [61/95], Step [91/132], D Loss: 0.5032, G Loss: 3.9636\n",
      "Epoch [61/95], Step [96/132], D Loss: 0.5032, G Loss: 3.7418\n",
      "Epoch [61/95], Step [101/132], D Loss: 0.5032, G Loss: 3.4127\n",
      "Epoch [61/95], Step [106/132], D Loss: 0.5032, G Loss: 3.3007\n",
      "Epoch [61/95], Step [111/132], D Loss: 0.5041, G Loss: 3.3796\n",
      "Epoch [61/95], Step [116/132], D Loss: 0.5032, G Loss: 3.7771\n",
      "Epoch [61/95], Step [121/132], D Loss: 0.5032, G Loss: 3.7262\n",
      "Epoch [61/95], Step [126/132], D Loss: 0.5032, G Loss: 3.8221\n",
      "Epoch [61/95], Step [131/132], D Loss: 0.5032, G Loss: 4.4298\n",
      "Epoch [61/95], Training Loss: 3.6978\n",
      "1 epoch time: 7.051209870974223\n",
      "Epoch [62/95], Step [1/132], D Loss: 0.5032, G Loss: 3.9664\n",
      "Epoch [62/95], Step [6/132], D Loss: 0.5032, G Loss: 3.9442\n",
      "Epoch [62/95], Step [11/132], D Loss: 0.5032, G Loss: 4.0225\n",
      "Epoch [62/95], Step [16/132], D Loss: 0.5032, G Loss: 3.1830\n",
      "Epoch [62/95], Step [21/132], D Loss: 0.5032, G Loss: 3.6086\n",
      "Epoch [62/95], Step [26/132], D Loss: 0.5032, G Loss: 4.1169\n",
      "Epoch [62/95], Step [31/132], D Loss: 0.5032, G Loss: 3.5942\n",
      "Epoch [62/95], Step [36/132], D Loss: 0.5032, G Loss: 3.7584\n",
      "Epoch [62/95], Step [41/132], D Loss: 0.5032, G Loss: 4.4988\n",
      "Epoch [62/95], Step [46/132], D Loss: 0.5032, G Loss: 3.2817\n",
      "Epoch [62/95], Step [51/132], D Loss: 0.5032, G Loss: 3.5355\n",
      "Epoch [62/95], Step [56/132], D Loss: 0.5032, G Loss: 4.0604\n",
      "Epoch [62/95], Step [61/132], D Loss: 0.5032, G Loss: 4.0485\n",
      "Epoch [62/95], Step [66/132], D Loss: 0.5035, G Loss: 3.7553\n",
      "Epoch [62/95], Step [71/132], D Loss: 0.5078, G Loss: 3.4237\n",
      "Epoch [62/95], Step [76/132], D Loss: 0.5032, G Loss: 3.8641\n",
      "Epoch [62/95], Step [81/132], D Loss: 0.5032, G Loss: 3.2129\n",
      "Epoch [62/95], Step [86/132], D Loss: 0.5032, G Loss: 3.0072\n",
      "Epoch [62/95], Step [91/132], D Loss: 0.5032, G Loss: 3.4626\n",
      "Epoch [62/95], Step [96/132], D Loss: 0.5032, G Loss: 4.0293\n",
      "Epoch [62/95], Step [101/132], D Loss: 0.5032, G Loss: 3.9192\n",
      "Epoch [62/95], Step [106/132], D Loss: 0.5032, G Loss: 3.4237\n",
      "Epoch [62/95], Step [111/132], D Loss: 0.5032, G Loss: 3.4361\n",
      "Epoch [62/95], Step [116/132], D Loss: 0.5032, G Loss: 4.1111\n",
      "Epoch [62/95], Step [121/132], D Loss: 0.5069, G Loss: 3.4915\n",
      "Epoch [62/95], Step [126/132], D Loss: 0.5032, G Loss: 3.7124\n",
      "Epoch [62/95], Step [131/132], D Loss: 0.5032, G Loss: 4.8659\n",
      "Epoch [62/95], Training Loss: 3.6854\n",
      "1 epoch time: 7.031285909811656\n",
      "Epoch [63/95], Step [1/132], D Loss: 0.5032, G Loss: 3.7293\n",
      "Epoch [63/95], Step [6/132], D Loss: 0.5032, G Loss: 3.3927\n",
      "Epoch [63/95], Step [11/132], D Loss: 0.5032, G Loss: 3.1605\n",
      "Epoch [63/95], Step [16/132], D Loss: 0.5033, G Loss: 3.4262\n",
      "Epoch [63/95], Step [21/132], D Loss: 0.5032, G Loss: 3.7833\n",
      "Epoch [63/95], Step [26/132], D Loss: 0.5036, G Loss: 3.2511\n",
      "Epoch [63/95], Step [31/132], D Loss: 0.5032, G Loss: 3.2503\n",
      "Epoch [63/95], Step [36/132], D Loss: 0.5032, G Loss: 3.4378\n",
      "Epoch [63/95], Step [41/132], D Loss: 0.5032, G Loss: 3.7869\n",
      "Epoch [63/95], Step [46/132], D Loss: 0.5032, G Loss: 3.9132\n",
      "Epoch [63/95], Step [51/132], D Loss: 0.5032, G Loss: 3.8167\n",
      "Epoch [63/95], Step [56/132], D Loss: 0.5032, G Loss: 3.9834\n",
      "Epoch [63/95], Step [61/132], D Loss: 0.5032, G Loss: 3.4499\n",
      "Epoch [63/95], Step [66/132], D Loss: 0.5032, G Loss: 3.7177\n",
      "Epoch [63/95], Step [71/132], D Loss: 0.5032, G Loss: 3.5818\n",
      "Epoch [63/95], Step [76/132], D Loss: 0.5032, G Loss: 3.0321\n",
      "Epoch [63/95], Step [81/132], D Loss: 0.5032, G Loss: 3.6881\n",
      "Epoch [63/95], Step [86/132], D Loss: 0.5032, G Loss: 3.1559\n",
      "Epoch [63/95], Step [91/132], D Loss: 0.5032, G Loss: 3.5663\n",
      "Epoch [63/95], Step [96/132], D Loss: 0.5032, G Loss: 4.3044\n",
      "Epoch [63/95], Step [101/132], D Loss: 0.5032, G Loss: 3.9837\n",
      "Epoch [63/95], Step [106/132], D Loss: 0.5032, G Loss: 3.2468\n",
      "Epoch [63/95], Step [111/132], D Loss: 0.5032, G Loss: 3.2445\n",
      "Epoch [63/95], Step [116/132], D Loss: 0.5036, G Loss: 3.3201\n",
      "Epoch [63/95], Step [121/132], D Loss: 0.5032, G Loss: 3.7797\n",
      "Epoch [63/95], Step [126/132], D Loss: 0.5032, G Loss: 4.1877\n",
      "Epoch [63/95], Step [131/132], D Loss: 0.5032, G Loss: 3.9263\n",
      "Epoch [63/95], Training Loss: 3.6784\n",
      "1 epoch time: 7.077391656239827\n",
      "Epoch [64/95], Step [1/132], D Loss: 0.5032, G Loss: 4.5045\n",
      "Epoch [64/95], Step [6/132], D Loss: 0.5032, G Loss: 3.9365\n",
      "Epoch [64/95], Step [11/132], D Loss: 0.5032, G Loss: 4.7658\n",
      "Epoch [64/95], Step [16/132], D Loss: 0.5032, G Loss: 3.4803\n",
      "Epoch [64/95], Step [21/132], D Loss: 0.5032, G Loss: 4.0926\n",
      "Epoch [64/95], Step [26/132], D Loss: 0.5032, G Loss: 4.3183\n",
      "Epoch [64/95], Step [31/132], D Loss: 0.5032, G Loss: 4.0983\n",
      "Epoch [64/95], Step [36/132], D Loss: 0.5032, G Loss: 3.5014\n",
      "Epoch [64/95], Step [41/132], D Loss: 0.5032, G Loss: 3.8360\n",
      "Epoch [64/95], Step [46/132], D Loss: 0.5032, G Loss: 3.3764\n",
      "Epoch [64/95], Step [51/132], D Loss: 0.5032, G Loss: 3.3427\n",
      "Epoch [64/95], Step [56/132], D Loss: 0.5032, G Loss: 4.0255\n",
      "Epoch [64/95], Step [61/132], D Loss: 0.5032, G Loss: 3.8617\n",
      "Epoch [64/95], Step [66/132], D Loss: 0.5032, G Loss: 3.2576\n",
      "Epoch [64/95], Step [71/132], D Loss: 0.5032, G Loss: 3.7646\n",
      "Epoch [64/95], Step [76/132], D Loss: 0.5032, G Loss: 3.1966\n",
      "Epoch [64/95], Step [81/132], D Loss: 0.5032, G Loss: 3.6424\n",
      "Epoch [64/95], Step [86/132], D Loss: 0.5032, G Loss: 3.1571\n",
      "Epoch [64/95], Step [91/132], D Loss: 0.5032, G Loss: 3.4403\n",
      "Epoch [64/95], Step [96/132], D Loss: 0.5032, G Loss: 4.0094\n",
      "Epoch [64/95], Step [101/132], D Loss: 0.8133, G Loss: 3.5293\n",
      "Epoch [64/95], Step [106/132], D Loss: 0.7604, G Loss: 3.6440\n",
      "Epoch [64/95], Step [111/132], D Loss: 0.7101, G Loss: 3.4361\n",
      "Epoch [64/95], Step [116/132], D Loss: 0.5306, G Loss: 3.5064\n",
      "Epoch [64/95], Step [121/132], D Loss: 0.5990, G Loss: 3.6008\n",
      "Epoch [64/95], Step [126/132], D Loss: 0.5039, G Loss: 3.9274\n",
      "Epoch [64/95], Step [131/132], D Loss: 0.5034, G Loss: 4.1811\n",
      "Epoch [64/95], Training Loss: 3.6911\n",
      "1 epoch time: 7.056662976741791\n",
      "Epoch [65/95], Step [1/132], D Loss: 0.5033, G Loss: 4.4697\n",
      "Epoch [65/95], Step [6/132], D Loss: 0.5034, G Loss: 3.8083\n",
      "Epoch [65/95], Step [11/132], D Loss: 0.5032, G Loss: 3.1720\n",
      "Epoch [65/95], Step [16/132], D Loss: 0.5032, G Loss: 3.5695\n",
      "Epoch [65/95], Step [21/132], D Loss: 0.5033, G Loss: 3.5000\n",
      "Epoch [65/95], Step [26/132], D Loss: 0.5032, G Loss: 3.8439\n",
      "Epoch [65/95], Step [31/132], D Loss: 0.5910, G Loss: 3.7271\n",
      "Epoch [65/95], Step [36/132], D Loss: 0.7209, G Loss: 3.4042\n",
      "Epoch [65/95], Step [41/132], D Loss: 0.5675, G Loss: 3.7927\n",
      "Epoch [65/95], Step [46/132], D Loss: 0.6414, G Loss: 3.7960\n",
      "Epoch [65/95], Step [51/132], D Loss: 0.5032, G Loss: 3.9058\n",
      "Epoch [65/95], Step [56/132], D Loss: 0.5953, G Loss: 3.9204\n",
      "Epoch [65/95], Step [61/132], D Loss: 0.5032, G Loss: 3.7176\n",
      "Epoch [65/95], Step [66/132], D Loss: 0.5032, G Loss: 3.4365\n",
      "Epoch [65/95], Step [71/132], D Loss: 0.5032, G Loss: 3.4651\n",
      "Epoch [65/95], Step [76/132], D Loss: 0.5032, G Loss: 3.9635\n",
      "Epoch [65/95], Step [81/132], D Loss: 0.5032, G Loss: 3.1915\n",
      "Epoch [65/95], Step [86/132], D Loss: 0.5037, G Loss: 3.4250\n",
      "Epoch [65/95], Step [91/132], D Loss: 0.5032, G Loss: 3.5348\n",
      "Epoch [65/95], Step [96/132], D Loss: 0.5032, G Loss: 3.7197\n",
      "Epoch [65/95], Step [101/132], D Loss: 0.5032, G Loss: 3.7307\n",
      "Epoch [65/95], Step [106/132], D Loss: 0.5080, G Loss: 3.6610\n",
      "Epoch [65/95], Step [111/132], D Loss: 0.5032, G Loss: 3.5895\n",
      "Epoch [65/95], Step [116/132], D Loss: 0.5032, G Loss: 4.2757\n",
      "Epoch [65/95], Step [121/132], D Loss: 0.5032, G Loss: 3.6935\n",
      "Epoch [65/95], Step [126/132], D Loss: 0.5032, G Loss: 3.9539\n",
      "Epoch [65/95], Step [131/132], D Loss: 0.5032, G Loss: 3.8321\n",
      "Epoch [65/95], Training Loss: 3.6954\n",
      "1 epoch time: 7.037947694460551\n",
      "Epoch [66/95], Step [1/132], D Loss: 0.5032, G Loss: 3.6957\n",
      "Epoch [66/95], Step [6/132], D Loss: 0.5032, G Loss: 3.1839\n",
      "Epoch [66/95], Step [11/132], D Loss: 0.5032, G Loss: 3.4451\n",
      "Epoch [66/95], Step [16/132], D Loss: 0.5032, G Loss: 4.2303\n",
      "Epoch [66/95], Step [21/132], D Loss: 0.5032, G Loss: 4.2834\n",
      "Epoch [66/95], Step [26/132], D Loss: 0.5518, G Loss: 3.2148\n",
      "Epoch [66/95], Step [31/132], D Loss: 0.8133, G Loss: 3.3824\n",
      "Epoch [66/95], Step [36/132], D Loss: 0.8133, G Loss: 3.1501\n",
      "Epoch [66/95], Step [41/132], D Loss: 0.8133, G Loss: 3.5924\n",
      "Epoch [66/95], Step [46/132], D Loss: 0.7833, G Loss: 3.6545\n",
      "Epoch [66/95], Step [51/132], D Loss: 0.7513, G Loss: 3.4221\n",
      "Epoch [66/95], Step [56/132], D Loss: 0.7309, G Loss: 3.3873\n",
      "Epoch [66/95], Step [61/132], D Loss: 0.7192, G Loss: 3.2091\n",
      "Epoch [66/95], Step [66/132], D Loss: 0.6739, G Loss: 3.7208\n",
      "Epoch [66/95], Step [71/132], D Loss: 0.6389, G Loss: 3.5269\n",
      "Epoch [66/95], Step [76/132], D Loss: 0.7141, G Loss: 3.9633\n",
      "Epoch [66/95], Step [81/132], D Loss: 0.6988, G Loss: 3.9535\n",
      "Epoch [66/95], Step [86/132], D Loss: 0.6701, G Loss: 3.4791\n",
      "Epoch [66/95], Step [91/132], D Loss: 0.5081, G Loss: 3.6383\n",
      "Epoch [66/95], Step [96/132], D Loss: 0.8133, G Loss: 3.6380\n",
      "Epoch [66/95], Step [101/132], D Loss: 0.6513, G Loss: 4.0995\n",
      "Epoch [66/95], Step [106/132], D Loss: 0.6765, G Loss: 3.6356\n",
      "Epoch [66/95], Step [111/132], D Loss: 0.6931, G Loss: 3.7843\n",
      "Epoch [66/95], Step [116/132], D Loss: 0.6911, G Loss: 3.8644\n",
      "Epoch [66/95], Step [121/132], D Loss: 0.6869, G Loss: 3.7076\n",
      "Epoch [66/95], Step [126/132], D Loss: 0.5815, G Loss: 3.8344\n",
      "Epoch [66/95], Step [131/132], D Loss: 0.5556, G Loss: 3.5031\n",
      "Epoch [66/95], Training Loss: 3.6201\n",
      "1 epoch time: 7.038007465998332\n",
      "Epoch [67/95], Step [1/132], D Loss: 0.5035, G Loss: 4.2727\n",
      "Epoch [67/95], Step [6/132], D Loss: 0.6206, G Loss: 3.8264\n",
      "Epoch [67/95], Step [11/132], D Loss: 0.5032, G Loss: 4.0277\n",
      "Epoch [67/95], Step [16/132], D Loss: 0.5032, G Loss: 3.5937\n",
      "Epoch [67/95], Step [21/132], D Loss: 0.5045, G Loss: 3.0969\n",
      "Epoch [67/95], Step [26/132], D Loss: 0.5038, G Loss: 3.9237\n",
      "Epoch [67/95], Step [31/132], D Loss: 0.5032, G Loss: 3.8271\n",
      "Epoch [67/95], Step [36/132], D Loss: 0.5032, G Loss: 3.5511\n",
      "Epoch [67/95], Step [41/132], D Loss: 0.8129, G Loss: 3.3825\n",
      "Epoch [67/95], Step [46/132], D Loss: 0.6241, G Loss: 3.4584\n",
      "Epoch [67/95], Step [51/132], D Loss: 0.5244, G Loss: 3.2655\n",
      "Epoch [67/95], Step [56/132], D Loss: 0.5043, G Loss: 3.5655\n",
      "Epoch [67/95], Step [61/132], D Loss: 0.5335, G Loss: 3.3141\n",
      "Epoch [67/95], Step [66/132], D Loss: 0.8133, G Loss: 3.4540\n",
      "Epoch [67/95], Step [71/132], D Loss: 0.7423, G Loss: 3.9052\n",
      "Epoch [67/95], Step [76/132], D Loss: 0.6529, G Loss: 3.8477\n",
      "Epoch [67/95], Step [81/132], D Loss: 0.6054, G Loss: 3.7141\n",
      "Epoch [67/95], Step [86/132], D Loss: 0.7215, G Loss: 4.0137\n",
      "Epoch [67/95], Step [91/132], D Loss: 0.6448, G Loss: 3.7993\n",
      "Epoch [67/95], Step [96/132], D Loss: 0.5298, G Loss: 3.6610\n",
      "Epoch [67/95], Step [101/132], D Loss: 0.6073, G Loss: 3.8911\n",
      "Epoch [67/95], Step [106/132], D Loss: 0.5477, G Loss: 3.6651\n",
      "Epoch [67/95], Step [111/132], D Loss: 0.5036, G Loss: 4.7184\n",
      "Epoch [67/95], Step [116/132], D Loss: 0.5038, G Loss: 3.2175\n",
      "Epoch [67/95], Step [121/132], D Loss: 0.5032, G Loss: 4.7486\n",
      "Epoch [67/95], Step [126/132], D Loss: 0.5083, G Loss: 3.9097\n",
      "Epoch [67/95], Step [131/132], D Loss: 0.5038, G Loss: 3.8489\n",
      "Epoch [67/95], Training Loss: 3.6810\n",
      "1 epoch time: 7.049058834711711\n",
      "Epoch [68/95], Step [1/132], D Loss: 0.8052, G Loss: 3.1990\n",
      "Epoch [68/95], Step [6/132], D Loss: 0.6547, G Loss: 4.6162\n",
      "Epoch [68/95], Step [11/132], D Loss: 0.5473, G Loss: 3.4375\n",
      "Epoch [68/95], Step [16/132], D Loss: 0.5081, G Loss: 3.7131\n",
      "Epoch [68/95], Step [21/132], D Loss: 0.7451, G Loss: 3.3747\n",
      "Epoch [68/95], Step [26/132], D Loss: 0.5039, G Loss: 3.3642\n",
      "Epoch [68/95], Step [31/132], D Loss: 0.5464, G Loss: 3.3948\n",
      "Epoch [68/95], Step [36/132], D Loss: 0.6764, G Loss: 3.4837\n",
      "Epoch [68/95], Step [41/132], D Loss: 0.5884, G Loss: 3.1890\n",
      "Epoch [68/95], Step [46/132], D Loss: 0.5038, G Loss: 4.0710\n",
      "Epoch [68/95], Step [51/132], D Loss: 0.5032, G Loss: 3.6500\n",
      "Epoch [68/95], Step [56/132], D Loss: 0.5032, G Loss: 3.6188\n",
      "Epoch [68/95], Step [61/132], D Loss: 0.5035, G Loss: 3.9463\n",
      "Epoch [68/95], Step [66/132], D Loss: 0.5032, G Loss: 3.7473\n",
      "Epoch [68/95], Step [71/132], D Loss: 0.5034, G Loss: 3.5069\n",
      "Epoch [68/95], Step [76/132], D Loss: 0.5032, G Loss: 3.9564\n",
      "Epoch [68/95], Step [81/132], D Loss: 0.5032, G Loss: 3.6994\n",
      "Epoch [68/95], Step [86/132], D Loss: 0.5649, G Loss: 3.8039\n",
      "Epoch [68/95], Step [91/132], D Loss: 0.5032, G Loss: 3.8075\n",
      "Epoch [68/95], Step [96/132], D Loss: 0.5033, G Loss: 4.1565\n",
      "Epoch [68/95], Step [101/132], D Loss: 0.5034, G Loss: 4.6462\n",
      "Epoch [68/95], Step [106/132], D Loss: 0.6545, G Loss: 3.0830\n",
      "Epoch [68/95], Step [111/132], D Loss: 0.5238, G Loss: 3.8888\n",
      "Epoch [68/95], Step [116/132], D Loss: 0.5172, G Loss: 3.5827\n",
      "Epoch [68/95], Step [121/132], D Loss: 0.6556, G Loss: 3.9714\n",
      "Epoch [68/95], Step [126/132], D Loss: 0.6170, G Loss: 4.2219\n",
      "Epoch [68/95], Step [131/132], D Loss: 0.5090, G Loss: 3.3558\n",
      "Epoch [68/95], Training Loss: 3.6899\n",
      "1 epoch time: 7.030429299672445\n",
      "Epoch [69/95], Step [1/132], D Loss: 0.5904, G Loss: 4.1335\n",
      "Epoch [69/95], Step [6/132], D Loss: 0.5049, G Loss: 3.8507\n",
      "Epoch [69/95], Step [11/132], D Loss: 0.5033, G Loss: 3.5723\n",
      "Epoch [69/95], Step [16/132], D Loss: 0.5040, G Loss: 3.2747\n",
      "Epoch [69/95], Step [21/132], D Loss: 0.5185, G Loss: 3.5639\n",
      "Epoch [69/95], Step [26/132], D Loss: 0.5032, G Loss: 3.6544\n",
      "Epoch [69/95], Step [31/132], D Loss: 0.5032, G Loss: 2.8768\n",
      "Epoch [69/95], Step [36/132], D Loss: 0.5032, G Loss: 3.4994\n",
      "Epoch [69/95], Step [41/132], D Loss: 0.5036, G Loss: 3.7169\n",
      "Epoch [69/95], Step [46/132], D Loss: 0.5032, G Loss: 3.8604\n",
      "Epoch [69/95], Step [51/132], D Loss: 0.5032, G Loss: 3.9296\n",
      "Epoch [69/95], Step [56/132], D Loss: 0.5032, G Loss: 3.7921\n",
      "Epoch [69/95], Step [61/132], D Loss: 0.5032, G Loss: 3.8933\n",
      "Epoch [69/95], Step [66/132], D Loss: 0.5035, G Loss: 3.1120\n",
      "Epoch [69/95], Step [71/132], D Loss: 0.5032, G Loss: 3.7987\n",
      "Epoch [69/95], Step [76/132], D Loss: 0.5032, G Loss: 3.1651\n",
      "Epoch [69/95], Step [81/132], D Loss: 0.5101, G Loss: 3.4944\n",
      "Epoch [69/95], Step [86/132], D Loss: 0.5043, G Loss: 3.8852\n",
      "Epoch [69/95], Step [91/132], D Loss: 0.5241, G Loss: 3.0542\n",
      "Epoch [69/95], Step [96/132], D Loss: 0.5032, G Loss: 3.2736\n",
      "Epoch [69/95], Step [101/132], D Loss: 0.5831, G Loss: 3.4053\n",
      "Epoch [69/95], Step [106/132], D Loss: 0.5048, G Loss: 3.4354\n",
      "Epoch [69/95], Step [111/132], D Loss: 0.5032, G Loss: 3.6735\n",
      "Epoch [69/95], Step [116/132], D Loss: 0.5032, G Loss: 3.4646\n",
      "Epoch [69/95], Step [121/132], D Loss: 0.5964, G Loss: 2.9080\n",
      "Epoch [69/95], Step [126/132], D Loss: 0.5033, G Loss: 4.1651\n",
      "Epoch [69/95], Step [131/132], D Loss: 0.5048, G Loss: 3.4380\n",
      "Epoch [69/95], Training Loss: 3.6620\n",
      "1 epoch time: 7.028935968875885\n",
      "Epoch [70/95], Step [1/132], D Loss: 0.5032, G Loss: 3.5690\n",
      "Epoch [70/95], Step [6/132], D Loss: 0.5060, G Loss: 2.8788\n",
      "Epoch [70/95], Step [11/132], D Loss: 0.5037, G Loss: 3.3370\n",
      "Epoch [70/95], Step [16/132], D Loss: 0.5032, G Loss: 3.1456\n",
      "Epoch [70/95], Step [21/132], D Loss: 0.5032, G Loss: 3.8392\n",
      "Epoch [70/95], Step [26/132], D Loss: 0.5032, G Loss: 3.6113\n",
      "Epoch [70/95], Step [31/132], D Loss: 0.5032, G Loss: 3.2798\n",
      "Epoch [70/95], Step [36/132], D Loss: 0.5032, G Loss: 3.6424\n",
      "Epoch [70/95], Step [41/132], D Loss: 0.5032, G Loss: 3.7562\n",
      "Epoch [70/95], Step [46/132], D Loss: 0.5032, G Loss: 3.1365\n",
      "Epoch [70/95], Step [51/132], D Loss: 0.5032, G Loss: 3.6781\n",
      "Epoch [70/95], Step [56/132], D Loss: 0.5155, G Loss: 3.3285\n",
      "Epoch [70/95], Step [61/132], D Loss: 0.6348, G Loss: 3.2672\n",
      "Epoch [70/95], Step [66/132], D Loss: 0.5032, G Loss: 4.0561\n",
      "Epoch [70/95], Step [71/132], D Loss: 0.5042, G Loss: 3.3033\n",
      "Epoch [70/95], Step [76/132], D Loss: 0.5033, G Loss: 3.5166\n",
      "Epoch [70/95], Step [81/132], D Loss: 0.5033, G Loss: 3.4413\n",
      "Epoch [70/95], Step [86/132], D Loss: 0.5032, G Loss: 3.3347\n",
      "Epoch [70/95], Step [91/132], D Loss: 0.5032, G Loss: 4.0494\n",
      "Epoch [70/95], Step [96/132], D Loss: 0.5033, G Loss: 3.8991\n",
      "Epoch [70/95], Step [101/132], D Loss: 0.6642, G Loss: 3.5239\n",
      "Epoch [70/95], Step [106/132], D Loss: 0.8149, G Loss: 3.4494\n",
      "Epoch [70/95], Step [111/132], D Loss: 0.5033, G Loss: 3.4338\n",
      "Epoch [70/95], Step [116/132], D Loss: 0.5032, G Loss: 3.9587\n",
      "Epoch [70/95], Step [121/132], D Loss: 0.5310, G Loss: 3.8536\n",
      "Epoch [70/95], Step [126/132], D Loss: 0.5432, G Loss: 4.0363\n",
      "Epoch [70/95], Step [131/132], D Loss: 0.6537, G Loss: 3.4411\n",
      "Epoch [70/95], Training Loss: 3.6192\n",
      "1 epoch time: 7.068779329458873\n",
      "Epoch [71/95], Step [1/132], D Loss: 0.5035, G Loss: 3.4469\n",
      "Epoch [71/95], Step [6/132], D Loss: 0.5036, G Loss: 3.8713\n",
      "Epoch [71/95], Step [11/132], D Loss: 0.5032, G Loss: 4.1771\n",
      "Epoch [71/95], Step [16/132], D Loss: 0.5032, G Loss: 3.7098\n",
      "Epoch [71/95], Step [21/132], D Loss: 0.5868, G Loss: 3.5591\n",
      "Epoch [71/95], Step [26/132], D Loss: 0.5032, G Loss: 3.6461\n",
      "Epoch [71/95], Step [31/132], D Loss: 0.5032, G Loss: 3.1385\n",
      "Epoch [71/95], Step [36/132], D Loss: 0.5032, G Loss: 3.4903\n",
      "Epoch [71/95], Step [41/132], D Loss: 0.5032, G Loss: 4.4086\n",
      "Epoch [71/95], Step [46/132], D Loss: 0.5032, G Loss: 3.0683\n",
      "Epoch [71/95], Step [51/132], D Loss: 0.5032, G Loss: 4.4990\n",
      "Epoch [71/95], Step [56/132], D Loss: 0.5038, G Loss: 3.3227\n",
      "Epoch [71/95], Step [61/132], D Loss: 0.5032, G Loss: 3.6666\n",
      "Epoch [71/95], Step [66/132], D Loss: 0.5032, G Loss: 3.6401\n",
      "Epoch [71/95], Step [71/132], D Loss: 0.5032, G Loss: 3.6189\n",
      "Epoch [71/95], Step [76/132], D Loss: 0.5032, G Loss: 4.1090\n",
      "Epoch [71/95], Step [81/132], D Loss: 0.5032, G Loss: 3.8594\n",
      "Epoch [71/95], Step [86/132], D Loss: 0.5032, G Loss: 3.6024\n",
      "Epoch [71/95], Step [91/132], D Loss: 0.5033, G Loss: 3.8151\n",
      "Epoch [71/95], Step [96/132], D Loss: 0.5032, G Loss: 3.2066\n",
      "Epoch [71/95], Step [101/132], D Loss: 0.5032, G Loss: 3.2823\n",
      "Epoch [71/95], Step [106/132], D Loss: 0.5705, G Loss: 4.1017\n",
      "Epoch [71/95], Step [111/132], D Loss: 0.5054, G Loss: 3.6608\n",
      "Epoch [71/95], Step [116/132], D Loss: 0.5032, G Loss: 3.9678\n",
      "Epoch [71/95], Step [121/132], D Loss: 0.5032, G Loss: 3.8954\n",
      "Epoch [71/95], Step [126/132], D Loss: 0.5032, G Loss: 3.9544\n",
      "Epoch [71/95], Step [131/132], D Loss: 0.5033, G Loss: 3.9547\n",
      "Epoch [71/95], Training Loss: 3.6381\n",
      "1 epoch time: 7.07125453154246\n",
      "Epoch [72/95], Step [1/132], D Loss: 0.5039, G Loss: 3.7629\n",
      "Epoch [72/95], Step [6/132], D Loss: 0.7111, G Loss: 3.9154\n",
      "Epoch [72/95], Step [11/132], D Loss: 0.5550, G Loss: 3.1161\n",
      "Epoch [72/95], Step [16/132], D Loss: 0.5034, G Loss: 4.0310\n",
      "Epoch [72/95], Step [21/132], D Loss: 0.5032, G Loss: 4.7524\n",
      "Epoch [72/95], Step [26/132], D Loss: 0.5032, G Loss: 3.7975\n",
      "Epoch [72/95], Step [31/132], D Loss: 0.5189, G Loss: 3.6918\n",
      "Epoch [72/95], Step [36/132], D Loss: 0.5033, G Loss: 3.5297\n",
      "Epoch [72/95], Step [41/132], D Loss: 0.5032, G Loss: 3.4158\n",
      "Epoch [72/95], Step [46/132], D Loss: 0.5032, G Loss: 3.7770\n",
      "Epoch [72/95], Step [51/132], D Loss: 0.5032, G Loss: 3.3616\n",
      "Epoch [72/95], Step [56/132], D Loss: 0.5033, G Loss: 3.6515\n",
      "Epoch [72/95], Step [61/132], D Loss: 0.5032, G Loss: 3.4432\n",
      "Epoch [72/95], Step [66/132], D Loss: 0.5032, G Loss: 4.0903\n",
      "Epoch [72/95], Step [71/132], D Loss: 0.5032, G Loss: 3.6589\n",
      "Epoch [72/95], Step [76/132], D Loss: 0.5032, G Loss: 3.4522\n",
      "Epoch [72/95], Step [81/132], D Loss: 0.5032, G Loss: 3.3877\n",
      "Epoch [72/95], Step [86/132], D Loss: 0.5032, G Loss: 3.5067\n",
      "Epoch [72/95], Step [91/132], D Loss: 0.5032, G Loss: 4.2119\n",
      "Epoch [72/95], Step [96/132], D Loss: 0.5032, G Loss: 3.4610\n",
      "Epoch [72/95], Step [101/132], D Loss: 0.5033, G Loss: 3.0142\n",
      "Epoch [72/95], Step [106/132], D Loss: 0.5032, G Loss: 3.7962\n",
      "Epoch [72/95], Step [111/132], D Loss: 0.5032, G Loss: 3.7140\n",
      "Epoch [72/95], Step [116/132], D Loss: 0.5032, G Loss: 3.9565\n",
      "Epoch [72/95], Step [121/132], D Loss: 0.5032, G Loss: 3.3407\n",
      "Epoch [72/95], Step [126/132], D Loss: 0.5032, G Loss: 3.4379\n",
      "Epoch [72/95], Step [131/132], D Loss: 0.5049, G Loss: 3.5664\n",
      "Epoch [72/95], Training Loss: 3.6555\n",
      "1 epoch time: 7.101241862773895\n",
      "Epoch [73/95], Step [1/132], D Loss: 0.8133, G Loss: 4.7301\n",
      "Epoch [73/95], Step [6/132], D Loss: 0.5487, G Loss: 3.8361\n",
      "Epoch [73/95], Step [11/132], D Loss: 0.5032, G Loss: 3.8436\n",
      "Epoch [73/95], Step [16/132], D Loss: 0.5033, G Loss: 3.0466\n",
      "Epoch [73/95], Step [21/132], D Loss: 0.5032, G Loss: 3.6033\n",
      "Epoch [73/95], Step [26/132], D Loss: 0.5032, G Loss: 3.7092\n",
      "Epoch [73/95], Step [31/132], D Loss: 0.5045, G Loss: 3.5124\n",
      "Epoch [73/95], Step [36/132], D Loss: 0.6929, G Loss: 4.0455\n",
      "Epoch [73/95], Step [41/132], D Loss: 0.5033, G Loss: 3.2082\n",
      "Epoch [73/95], Step [46/132], D Loss: 0.5159, G Loss: 3.2794\n",
      "Epoch [73/95], Step [51/132], D Loss: 0.7867, G Loss: 3.1740\n",
      "Epoch [73/95], Step [56/132], D Loss: 0.6960, G Loss: 3.7585\n",
      "Epoch [73/95], Step [61/132], D Loss: 0.6931, G Loss: 3.4049\n",
      "Epoch [73/95], Step [66/132], D Loss: 0.6931, G Loss: 3.5403\n",
      "Epoch [73/95], Step [71/132], D Loss: 0.6931, G Loss: 3.5910\n",
      "Epoch [73/95], Step [76/132], D Loss: 0.6931, G Loss: 3.7849\n",
      "Epoch [73/95], Step [81/132], D Loss: 0.6931, G Loss: 3.3277\n",
      "Epoch [73/95], Step [86/132], D Loss: 0.6931, G Loss: 3.7637\n",
      "Epoch [73/95], Step [91/132], D Loss: 0.6931, G Loss: 3.5494\n",
      "Epoch [73/95], Step [96/132], D Loss: 0.6931, G Loss: 3.8980\n",
      "Epoch [73/95], Step [101/132], D Loss: 0.6931, G Loss: 3.8351\n",
      "Epoch [73/95], Step [106/132], D Loss: 0.6931, G Loss: 3.7593\n",
      "Epoch [73/95], Step [111/132], D Loss: 0.6931, G Loss: 3.9203\n",
      "Epoch [73/95], Step [116/132], D Loss: 0.6931, G Loss: 3.6324\n",
      "Epoch [73/95], Step [121/132], D Loss: 0.6931, G Loss: 3.4754\n",
      "Epoch [73/95], Step [126/132], D Loss: 0.6931, G Loss: 2.8227\n",
      "Epoch [73/95], Step [131/132], D Loss: 0.6931, G Loss: 3.5731\n",
      "Epoch [73/95], Training Loss: 3.6297\n",
      "1 epoch time: 7.055114189783732\n",
      "Epoch [74/95], Step [1/132], D Loss: 0.6931, G Loss: 3.6813\n",
      "Epoch [74/95], Step [6/132], D Loss: 0.6931, G Loss: 3.6142\n",
      "Epoch [74/95], Step [11/132], D Loss: 0.6931, G Loss: 3.7663\n",
      "Epoch [74/95], Step [16/132], D Loss: 0.6931, G Loss: 4.0407\n",
      "Epoch [74/95], Step [21/132], D Loss: 0.6931, G Loss: 3.7512\n",
      "Epoch [74/95], Step [26/132], D Loss: 0.6931, G Loss: 3.9887\n",
      "Epoch [74/95], Step [31/132], D Loss: 0.6931, G Loss: 3.6285\n",
      "Epoch [74/95], Step [36/132], D Loss: 0.6931, G Loss: 3.8836\n",
      "Epoch [74/95], Step [41/132], D Loss: 0.6931, G Loss: 3.5257\n",
      "Epoch [74/95], Step [46/132], D Loss: 0.6931, G Loss: 3.2270\n",
      "Epoch [74/95], Step [51/132], D Loss: 0.6931, G Loss: 2.9138\n",
      "Epoch [74/95], Step [56/132], D Loss: 0.6931, G Loss: 3.6275\n",
      "Epoch [74/95], Step [61/132], D Loss: 0.6931, G Loss: 3.2515\n",
      "Epoch [74/95], Step [66/132], D Loss: 0.6931, G Loss: 4.0669\n",
      "Epoch [74/95], Step [71/132], D Loss: 0.6931, G Loss: 3.6711\n",
      "Epoch [74/95], Step [76/132], D Loss: 0.6931, G Loss: 3.3704\n",
      "Epoch [74/95], Step [81/132], D Loss: 0.6931, G Loss: 3.6072\n",
      "Epoch [74/95], Step [86/132], D Loss: 0.6931, G Loss: 3.6080\n",
      "Epoch [74/95], Step [91/132], D Loss: 0.6931, G Loss: 3.6454\n",
      "Epoch [74/95], Step [96/132], D Loss: 0.6931, G Loss: 3.8855\n",
      "Epoch [74/95], Step [101/132], D Loss: 0.6931, G Loss: 4.3598\n",
      "Epoch [74/95], Step [106/132], D Loss: 0.6931, G Loss: 3.2196\n",
      "Epoch [74/95], Step [111/132], D Loss: 0.6931, G Loss: 3.6577\n",
      "Epoch [74/95], Step [116/132], D Loss: 0.6931, G Loss: 3.3168\n",
      "Epoch [74/95], Step [121/132], D Loss: 0.6931, G Loss: 3.5859\n",
      "Epoch [74/95], Step [126/132], D Loss: 0.6931, G Loss: 3.6621\n",
      "Epoch [74/95], Step [131/132], D Loss: 0.6931, G Loss: 4.4290\n",
      "Epoch [74/95], Training Loss: 3.6032\n",
      "1 epoch time: 7.0350257674853\n",
      "Epoch [75/95], Step [1/132], D Loss: 0.6931, G Loss: 3.4279\n",
      "Epoch [75/95], Step [6/132], D Loss: 0.6931, G Loss: 3.0768\n",
      "Epoch [75/95], Step [11/132], D Loss: 0.6931, G Loss: 3.0639\n",
      "Epoch [75/95], Step [16/132], D Loss: 0.6931, G Loss: 3.6927\n",
      "Epoch [75/95], Step [21/132], D Loss: 0.6931, G Loss: 3.3513\n",
      "Epoch [75/95], Step [26/132], D Loss: 0.6931, G Loss: 4.0853\n",
      "Epoch [75/95], Step [31/132], D Loss: 0.6931, G Loss: 3.4842\n",
      "Epoch [75/95], Step [36/132], D Loss: 0.6931, G Loss: 3.6663\n",
      "Epoch [75/95], Step [41/132], D Loss: 0.6931, G Loss: 4.0348\n",
      "Epoch [75/95], Step [46/132], D Loss: 0.6931, G Loss: 3.3286\n",
      "Epoch [75/95], Step [51/132], D Loss: 0.6931, G Loss: 3.1610\n",
      "Epoch [75/95], Step [56/132], D Loss: 0.6931, G Loss: 3.9836\n",
      "Epoch [75/95], Step [61/132], D Loss: 0.6931, G Loss: 3.3742\n",
      "Epoch [75/95], Step [66/132], D Loss: 0.6931, G Loss: 3.7481\n",
      "Epoch [75/95], Step [71/132], D Loss: 0.6931, G Loss: 4.2079\n",
      "Epoch [75/95], Step [76/132], D Loss: 0.6931, G Loss: 3.5763\n",
      "Epoch [75/95], Step [81/132], D Loss: 0.6931, G Loss: 3.5423\n",
      "Epoch [75/95], Step [86/132], D Loss: 0.6931, G Loss: 4.4147\n",
      "Epoch [75/95], Step [91/132], D Loss: 0.6931, G Loss: 3.9335\n",
      "Epoch [75/95], Step [96/132], D Loss: 0.6931, G Loss: 3.3243\n",
      "Epoch [75/95], Step [101/132], D Loss: 0.6931, G Loss: 3.4027\n",
      "Epoch [75/95], Step [106/132], D Loss: 0.6931, G Loss: 3.3733\n",
      "Epoch [75/95], Step [111/132], D Loss: 0.6931, G Loss: 3.5456\n",
      "Epoch [75/95], Step [116/132], D Loss: 0.6931, G Loss: 3.8441\n",
      "Epoch [75/95], Step [121/132], D Loss: 0.6931, G Loss: 3.9270\n",
      "Epoch [75/95], Step [126/132], D Loss: 0.6931, G Loss: 3.6626\n",
      "Epoch [75/95], Step [131/132], D Loss: 0.6931, G Loss: 4.3015\n",
      "Epoch [75/95], Training Loss: 3.6459\n",
      "1 epoch time: 7.016095197200775\n",
      "Epoch [76/95], Step [1/132], D Loss: 0.6931, G Loss: 3.7324\n",
      "Epoch [76/95], Step [6/132], D Loss: 0.6931, G Loss: 3.7533\n",
      "Epoch [76/95], Step [11/132], D Loss: 0.6931, G Loss: 3.2144\n",
      "Epoch [76/95], Step [16/132], D Loss: 0.6931, G Loss: 3.7462\n",
      "Epoch [76/95], Step [21/132], D Loss: 0.6931, G Loss: 3.3671\n",
      "Epoch [76/95], Step [26/132], D Loss: 0.6931, G Loss: 3.6439\n",
      "Epoch [76/95], Step [31/132], D Loss: 0.6931, G Loss: 4.3175\n",
      "Epoch [76/95], Step [36/132], D Loss: 0.6931, G Loss: 3.3582\n",
      "Epoch [76/95], Step [41/132], D Loss: 0.6931, G Loss: 3.9673\n",
      "Epoch [76/95], Step [46/132], D Loss: 0.6931, G Loss: 4.0019\n",
      "Epoch [76/95], Step [51/132], D Loss: 0.6931, G Loss: 3.4810\n",
      "Epoch [76/95], Step [56/132], D Loss: 0.6931, G Loss: 4.2901\n",
      "Epoch [76/95], Step [61/132], D Loss: 0.6931, G Loss: 3.6540\n",
      "Epoch [76/95], Step [66/132], D Loss: 0.6931, G Loss: 3.5026\n",
      "Epoch [76/95], Step [71/132], D Loss: 0.6931, G Loss: 3.5907\n",
      "Epoch [76/95], Step [76/132], D Loss: 0.6931, G Loss: 4.0244\n",
      "Epoch [76/95], Step [81/132], D Loss: 0.6931, G Loss: 3.1617\n",
      "Epoch [76/95], Step [86/132], D Loss: 0.6931, G Loss: 3.5354\n",
      "Epoch [76/95], Step [91/132], D Loss: 0.6931, G Loss: 3.5790\n",
      "Epoch [76/95], Step [96/132], D Loss: 0.6931, G Loss: 4.4889\n",
      "Epoch [76/95], Step [101/132], D Loss: 0.6931, G Loss: 3.5445\n",
      "Epoch [76/95], Step [106/132], D Loss: 0.6931, G Loss: 3.1691\n",
      "Epoch [76/95], Step [111/132], D Loss: 0.6931, G Loss: 3.6848\n",
      "Epoch [76/95], Step [116/132], D Loss: 0.6931, G Loss: 3.5449\n",
      "Epoch [76/95], Step [121/132], D Loss: 0.6931, G Loss: 3.3279\n",
      "Epoch [76/95], Step [126/132], D Loss: 0.6931, G Loss: 3.3282\n",
      "Epoch [76/95], Step [131/132], D Loss: 0.6931, G Loss: 3.1532\n",
      "Epoch [76/95], Training Loss: 3.5984\n",
      "1 epoch time: 7.023872073491415\n",
      "Epoch [77/95], Step [1/132], D Loss: 0.6931, G Loss: 3.7886\n",
      "Epoch [77/95], Step [6/132], D Loss: 0.6931, G Loss: 3.4589\n",
      "Epoch [77/95], Step [11/132], D Loss: 0.6931, G Loss: 3.0127\n",
      "Epoch [77/95], Step [16/132], D Loss: 0.6931, G Loss: 3.4064\n",
      "Epoch [77/95], Step [21/132], D Loss: 0.6931, G Loss: 3.7212\n",
      "Epoch [77/95], Step [26/132], D Loss: 0.6931, G Loss: 3.6532\n",
      "Epoch [77/95], Step [31/132], D Loss: 0.6931, G Loss: 3.2803\n",
      "Epoch [77/95], Step [36/132], D Loss: 0.6931, G Loss: 3.5661\n",
      "Epoch [77/95], Step [41/132], D Loss: 0.6931, G Loss: 3.2767\n",
      "Epoch [77/95], Step [46/132], D Loss: 0.6931, G Loss: 4.3478\n",
      "Epoch [77/95], Step [51/132], D Loss: 0.6931, G Loss: 3.5988\n",
      "Epoch [77/95], Step [56/132], D Loss: 0.6931, G Loss: 4.3127\n",
      "Epoch [77/95], Step [61/132], D Loss: 0.6931, G Loss: 3.9547\n",
      "Epoch [77/95], Step [66/132], D Loss: 0.6931, G Loss: 3.5400\n",
      "Epoch [77/95], Step [71/132], D Loss: 0.6931, G Loss: 3.8954\n",
      "Epoch [77/95], Step [76/132], D Loss: 0.6931, G Loss: 3.0140\n",
      "Epoch [77/95], Step [81/132], D Loss: 0.6931, G Loss: 3.1036\n",
      "Epoch [77/95], Step [86/132], D Loss: 0.6931, G Loss: 3.8977\n",
      "Epoch [77/95], Step [91/132], D Loss: 0.6931, G Loss: 3.4675\n",
      "Epoch [77/95], Step [96/132], D Loss: 0.6931, G Loss: 3.9228\n",
      "Epoch [77/95], Step [101/132], D Loss: 0.6931, G Loss: 3.3947\n",
      "Epoch [77/95], Step [106/132], D Loss: 0.6931, G Loss: 3.7265\n",
      "Epoch [77/95], Step [111/132], D Loss: 0.6931, G Loss: 3.9788\n",
      "Epoch [77/95], Step [116/132], D Loss: 0.6931, G Loss: 3.6563\n",
      "Epoch [77/95], Step [121/132], D Loss: 0.6931, G Loss: 3.4444\n",
      "Epoch [77/95], Step [126/132], D Loss: 0.6931, G Loss: 2.7825\n",
      "Epoch [77/95], Step [131/132], D Loss: 0.6931, G Loss: 3.2344\n",
      "Epoch [77/95], Training Loss: 3.6152\n",
      "1 epoch time: 7.027327620983124\n",
      "Epoch [78/95], Step [1/132], D Loss: 0.6931, G Loss: 3.9755\n",
      "Epoch [78/95], Step [6/132], D Loss: 0.6931, G Loss: 3.2861\n",
      "Epoch [78/95], Step [11/132], D Loss: 0.6931, G Loss: 3.4128\n",
      "Epoch [78/95], Step [16/132], D Loss: 0.6931, G Loss: 3.4483\n",
      "Epoch [78/95], Step [21/132], D Loss: 0.6931, G Loss: 3.7018\n",
      "Epoch [78/95], Step [26/132], D Loss: 0.6931, G Loss: 3.6455\n",
      "Epoch [78/95], Step [31/132], D Loss: 0.6931, G Loss: 3.6920\n",
      "Epoch [78/95], Step [36/132], D Loss: 0.6931, G Loss: 3.3920\n",
      "Epoch [78/95], Step [41/132], D Loss: 0.6931, G Loss: 4.1170\n",
      "Epoch [78/95], Step [46/132], D Loss: 0.6931, G Loss: 3.1992\n",
      "Epoch [78/95], Step [51/132], D Loss: 0.6931, G Loss: 3.9567\n",
      "Epoch [78/95], Step [56/132], D Loss: 0.6931, G Loss: 3.4751\n",
      "Epoch [78/95], Step [61/132], D Loss: 0.6931, G Loss: 3.1906\n",
      "Epoch [78/95], Step [66/132], D Loss: 0.6931, G Loss: 3.4057\n",
      "Epoch [78/95], Step [71/132], D Loss: 0.6931, G Loss: 3.6321\n",
      "Epoch [78/95], Step [76/132], D Loss: 0.6931, G Loss: 3.2025\n",
      "Epoch [78/95], Step [81/132], D Loss: 0.6931, G Loss: 3.9978\n",
      "Epoch [78/95], Step [86/132], D Loss: 0.6931, G Loss: 3.5951\n",
      "Epoch [78/95], Step [91/132], D Loss: 0.6931, G Loss: 3.5180\n",
      "Epoch [78/95], Step [96/132], D Loss: 0.6931, G Loss: 3.5941\n",
      "Epoch [78/95], Step [101/132], D Loss: 0.6931, G Loss: 3.3026\n",
      "Epoch [78/95], Step [106/132], D Loss: 0.6931, G Loss: 3.5123\n",
      "Epoch [78/95], Step [111/132], D Loss: 0.6931, G Loss: 4.0127\n",
      "Epoch [78/95], Step [116/132], D Loss: 0.6931, G Loss: 3.6580\n",
      "Epoch [78/95], Step [121/132], D Loss: 0.6931, G Loss: 4.1101\n",
      "Epoch [78/95], Step [126/132], D Loss: 0.6931, G Loss: 4.1950\n",
      "Epoch [78/95], Step [131/132], D Loss: 0.6931, G Loss: 4.1565\n",
      "Epoch [78/95], Training Loss: 3.5774\n",
      "1 epoch time: 7.019680976867676\n",
      "Epoch [79/95], Step [1/132], D Loss: 0.6931, G Loss: 3.7578\n",
      "Epoch [79/95], Step [6/132], D Loss: 0.6931, G Loss: 3.2770\n",
      "Epoch [79/95], Step [11/132], D Loss: 0.6931, G Loss: 3.3428\n",
      "Epoch [79/95], Step [16/132], D Loss: 0.6931, G Loss: 3.6696\n",
      "Epoch [79/95], Step [21/132], D Loss: 0.6931, G Loss: 3.4791\n",
      "Epoch [79/95], Step [26/132], D Loss: 0.6931, G Loss: 3.3904\n",
      "Epoch [79/95], Step [31/132], D Loss: 0.6931, G Loss: 3.3269\n",
      "Epoch [79/95], Step [36/132], D Loss: 0.6931, G Loss: 3.7888\n",
      "Epoch [79/95], Step [41/132], D Loss: 0.6931, G Loss: 4.5042\n",
      "Epoch [79/95], Step [46/132], D Loss: 0.6931, G Loss: 3.4155\n",
      "Epoch [79/95], Step [51/132], D Loss: 0.6931, G Loss: 3.5950\n",
      "Epoch [79/95], Step [56/132], D Loss: 0.6931, G Loss: 3.9025\n",
      "Epoch [79/95], Step [61/132], D Loss: 0.6931, G Loss: 3.4304\n",
      "Epoch [79/95], Step [66/132], D Loss: 0.6931, G Loss: 3.6686\n",
      "Epoch [79/95], Step [71/132], D Loss: 0.6931, G Loss: 3.7016\n",
      "Epoch [79/95], Step [76/132], D Loss: 0.6931, G Loss: 3.4852\n",
      "Epoch [79/95], Step [81/132], D Loss: 0.6931, G Loss: 3.8016\n",
      "Epoch [79/95], Step [86/132], D Loss: 0.6931, G Loss: 2.9718\n",
      "Epoch [79/95], Step [91/132], D Loss: 0.6931, G Loss: 3.0029\n",
      "Epoch [79/95], Step [96/132], D Loss: 0.6931, G Loss: 3.2109\n",
      "Epoch [79/95], Step [101/132], D Loss: 0.6931, G Loss: 3.2899\n",
      "Epoch [79/95], Step [106/132], D Loss: 0.6931, G Loss: 3.0834\n",
      "Epoch [79/95], Step [111/132], D Loss: 0.6931, G Loss: 4.4592\n",
      "Epoch [79/95], Step [116/132], D Loss: 0.6931, G Loss: 3.7492\n",
      "Epoch [79/95], Step [121/132], D Loss: 0.6931, G Loss: 3.9563\n",
      "Epoch [79/95], Step [126/132], D Loss: 0.6931, G Loss: 3.5682\n",
      "Epoch [79/95], Step [131/132], D Loss: 0.6931, G Loss: 3.3068\n",
      "Epoch [79/95], Training Loss: 3.5803\n",
      "1 epoch time: 7.04715633392334\n",
      "Epoch [80/95], Step [1/132], D Loss: 0.6931, G Loss: 3.8981\n",
      "Epoch [80/95], Step [6/132], D Loss: 0.6931, G Loss: 3.9528\n",
      "Epoch [80/95], Step [11/132], D Loss: 0.6931, G Loss: 3.4243\n",
      "Epoch [80/95], Step [16/132], D Loss: 0.6931, G Loss: 2.9792\n",
      "Epoch [80/95], Step [21/132], D Loss: 0.6930, G Loss: 3.3636\n",
      "Epoch [80/95], Step [26/132], D Loss: 0.6931, G Loss: 4.1119\n",
      "Epoch [80/95], Step [31/132], D Loss: 0.6931, G Loss: 4.2674\n",
      "Epoch [80/95], Step [36/132], D Loss: 0.6931, G Loss: 3.3540\n",
      "Epoch [80/95], Step [41/132], D Loss: 0.6931, G Loss: 3.3209\n",
      "Epoch [80/95], Step [46/132], D Loss: 0.6931, G Loss: 3.6140\n",
      "Epoch [80/95], Step [51/132], D Loss: 0.6931, G Loss: 3.7263\n",
      "Epoch [80/95], Step [56/132], D Loss: 0.6931, G Loss: 3.7277\n",
      "Epoch [80/95], Step [61/132], D Loss: 0.6931, G Loss: 3.5539\n",
      "Epoch [80/95], Step [66/132], D Loss: 0.6931, G Loss: 3.1996\n",
      "Epoch [80/95], Step [71/132], D Loss: 0.6931, G Loss: 3.1746\n",
      "Epoch [80/95], Step [76/132], D Loss: 0.6931, G Loss: 3.1791\n",
      "Epoch [80/95], Step [81/132], D Loss: 0.6931, G Loss: 3.8794\n",
      "Epoch [80/95], Step [86/132], D Loss: 0.6931, G Loss: 4.3592\n",
      "Epoch [80/95], Step [91/132], D Loss: 0.6931, G Loss: 3.0378\n",
      "Epoch [80/95], Step [96/132], D Loss: 0.6931, G Loss: 3.0281\n",
      "Epoch [80/95], Step [101/132], D Loss: 0.6931, G Loss: 3.7855\n",
      "Epoch [80/95], Step [106/132], D Loss: 0.6931, G Loss: 3.7129\n",
      "Epoch [80/95], Step [111/132], D Loss: 0.6931, G Loss: 3.0525\n",
      "Epoch [80/95], Step [116/132], D Loss: 0.6931, G Loss: 3.4033\n",
      "Epoch [80/95], Step [121/132], D Loss: 0.6931, G Loss: 3.8603\n",
      "Epoch [80/95], Step [126/132], D Loss: 0.6931, G Loss: 3.8800\n",
      "Epoch [80/95], Step [131/132], D Loss: 0.6931, G Loss: 3.1161\n",
      "Epoch [80/95], Training Loss: 3.5743\n",
      "1 epoch time: 7.042804586887359\n",
      "Epoch [81/95], Step [1/132], D Loss: 0.6931, G Loss: 3.5781\n",
      "Epoch [81/95], Step [6/132], D Loss: 0.6931, G Loss: 3.8357\n",
      "Epoch [81/95], Step [11/132], D Loss: 0.6931, G Loss: 3.1457\n",
      "Epoch [81/95], Step [16/132], D Loss: 0.6931, G Loss: 3.4767\n",
      "Epoch [81/95], Step [21/132], D Loss: 0.6931, G Loss: 3.9395\n",
      "Epoch [81/95], Step [26/132], D Loss: 0.6931, G Loss: 3.7123\n",
      "Epoch [81/95], Step [31/132], D Loss: 0.6931, G Loss: 3.2091\n",
      "Epoch [81/95], Step [36/132], D Loss: 0.6931, G Loss: 3.7755\n",
      "Epoch [81/95], Step [41/132], D Loss: 0.6931, G Loss: 3.7683\n",
      "Epoch [81/95], Step [46/132], D Loss: 0.6931, G Loss: 3.1183\n",
      "Epoch [81/95], Step [51/132], D Loss: 0.6931, G Loss: 3.6215\n",
      "Epoch [81/95], Step [56/132], D Loss: 0.6931, G Loss: 3.6318\n",
      "Epoch [81/95], Step [61/132], D Loss: 0.6931, G Loss: 3.3584\n",
      "Epoch [81/95], Step [66/132], D Loss: 0.6931, G Loss: 4.1462\n",
      "Epoch [81/95], Step [71/132], D Loss: 0.6931, G Loss: 4.0008\n",
      "Epoch [81/95], Step [76/132], D Loss: 0.6931, G Loss: 3.5945\n",
      "Epoch [81/95], Step [81/132], D Loss: 0.6931, G Loss: 3.5305\n",
      "Epoch [81/95], Step [86/132], D Loss: 0.6931, G Loss: 3.5153\n",
      "Epoch [81/95], Step [91/132], D Loss: 0.6931, G Loss: 3.3806\n",
      "Epoch [81/95], Step [96/132], D Loss: 0.6931, G Loss: 3.3140\n",
      "Epoch [81/95], Step [101/132], D Loss: 0.6931, G Loss: 3.6534\n",
      "Epoch [81/95], Step [106/132], D Loss: 0.6928, G Loss: 4.2626\n",
      "Epoch [81/95], Step [111/132], D Loss: 0.6931, G Loss: 3.4772\n",
      "Epoch [81/95], Step [116/132], D Loss: 0.6931, G Loss: 4.2302\n",
      "Epoch [81/95], Step [121/132], D Loss: 0.6931, G Loss: 3.3903\n",
      "Epoch [81/95], Step [126/132], D Loss: 0.6930, G Loss: 3.7453\n",
      "Epoch [81/95], Step [131/132], D Loss: 0.6931, G Loss: 3.5177\n",
      "Epoch [81/95], Training Loss: 3.5834\n",
      "1 epoch time: 7.02483643690745\n",
      "Epoch [82/95], Step [1/132], D Loss: 0.6931, G Loss: 3.6762\n",
      "Epoch [82/95], Step [6/132], D Loss: 0.6929, G Loss: 3.4314\n",
      "Epoch [82/95], Step [11/132], D Loss: 0.6930, G Loss: 3.4331\n",
      "Epoch [82/95], Step [16/132], D Loss: 0.6929, G Loss: 3.6624\n",
      "Epoch [82/95], Step [21/132], D Loss: 0.6899, G Loss: 3.9511\n",
      "Epoch [82/95], Step [26/132], D Loss: 0.6702, G Loss: 3.9094\n",
      "Epoch [82/95], Step [31/132], D Loss: 0.6315, G Loss: 3.3739\n",
      "Epoch [82/95], Step [36/132], D Loss: 0.5056, G Loss: 3.5485\n",
      "Epoch [82/95], Step [41/132], D Loss: 0.5071, G Loss: 4.0320\n",
      "Epoch [82/95], Step [46/132], D Loss: 0.5032, G Loss: 3.6422\n",
      "Epoch [82/95], Step [51/132], D Loss: 0.5032, G Loss: 3.3901\n",
      "Epoch [82/95], Step [56/132], D Loss: 0.5032, G Loss: 3.4927\n",
      "Epoch [82/95], Step [61/132], D Loss: 0.5033, G Loss: 3.3828\n",
      "Epoch [82/95], Step [66/132], D Loss: 0.5033, G Loss: 4.0101\n",
      "Epoch [82/95], Step [71/132], D Loss: 0.5032, G Loss: 4.3385\n",
      "Epoch [82/95], Step [76/132], D Loss: 0.5032, G Loss: 3.5048\n",
      "Epoch [82/95], Step [81/132], D Loss: 0.5159, G Loss: 3.6557\n",
      "Epoch [82/95], Step [86/132], D Loss: 0.5032, G Loss: 3.8299\n",
      "Epoch [82/95], Step [91/132], D Loss: 0.5036, G Loss: 3.2822\n",
      "Epoch [82/95], Step [96/132], D Loss: 0.5032, G Loss: 3.9591\n",
      "Epoch [82/95], Step [101/132], D Loss: 0.5032, G Loss: 3.2949\n",
      "Epoch [82/95], Step [106/132], D Loss: 0.5032, G Loss: 2.7105\n",
      "Epoch [82/95], Step [111/132], D Loss: 0.8130, G Loss: 2.9847\n",
      "Epoch [82/95], Step [116/132], D Loss: 0.5291, G Loss: 3.3348\n",
      "Epoch [82/95], Step [121/132], D Loss: 0.5074, G Loss: 3.6668\n",
      "Epoch [82/95], Step [126/132], D Loss: 0.5032, G Loss: 3.1056\n",
      "Epoch [82/95], Step [131/132], D Loss: 0.5034, G Loss: 3.2077\n",
      "Epoch [82/95], Training Loss: 3.5337\n",
      "1 epoch time: 7.035881916681926\n",
      "Epoch [83/95], Step [1/132], D Loss: 0.7731, G Loss: 3.7295\n",
      "Epoch [83/95], Step [6/132], D Loss: 0.7239, G Loss: 3.6955\n",
      "Epoch [83/95], Step [11/132], D Loss: 0.5771, G Loss: 4.1493\n",
      "Epoch [83/95], Step [16/132], D Loss: 0.6446, G Loss: 3.4388\n",
      "Epoch [83/95], Step [21/132], D Loss: 0.6931, G Loss: 3.5040\n",
      "Epoch [83/95], Step [26/132], D Loss: 0.5390, G Loss: 3.8828\n",
      "Epoch [83/95], Step [31/132], D Loss: 0.5058, G Loss: 3.5239\n",
      "Epoch [83/95], Step [36/132], D Loss: 0.5032, G Loss: 3.9902\n",
      "Epoch [83/95], Step [41/132], D Loss: 0.5121, G Loss: 3.1750\n",
      "Epoch [83/95], Step [46/132], D Loss: 0.5105, G Loss: 4.0906\n",
      "Epoch [83/95], Step [51/132], D Loss: 0.5043, G Loss: 3.1965\n",
      "Epoch [83/95], Step [56/132], D Loss: 0.5032, G Loss: 3.5162\n",
      "Epoch [83/95], Step [61/132], D Loss: 0.7855, G Loss: 2.9329\n",
      "Epoch [83/95], Step [66/132], D Loss: 0.7247, G Loss: 3.9344\n",
      "Epoch [83/95], Step [71/132], D Loss: 0.6914, G Loss: 3.1542\n",
      "Epoch [83/95], Step [76/132], D Loss: 0.6121, G Loss: 3.6445\n",
      "Epoch [83/95], Step [81/132], D Loss: 0.5034, G Loss: 3.5406\n",
      "Epoch [83/95], Step [86/132], D Loss: 0.6240, G Loss: 3.6434\n",
      "Epoch [83/95], Step [91/132], D Loss: 0.8131, G Loss: 3.6531\n",
      "Epoch [83/95], Step [96/132], D Loss: 0.8132, G Loss: 2.6590\n",
      "Epoch [83/95], Step [101/132], D Loss: 0.7632, G Loss: 3.4393\n",
      "Epoch [83/95], Step [106/132], D Loss: 0.6452, G Loss: 3.5666\n",
      "Epoch [83/95], Step [111/132], D Loss: 0.6935, G Loss: 3.5928\n",
      "Epoch [83/95], Step [116/132], D Loss: 0.5877, G Loss: 3.3881\n",
      "Epoch [83/95], Step [121/132], D Loss: 0.5044, G Loss: 3.9202\n",
      "Epoch [83/95], Step [126/132], D Loss: 0.6660, G Loss: 3.6230\n",
      "Epoch [83/95], Step [131/132], D Loss: 0.6531, G Loss: 4.0436\n",
      "Epoch [83/95], Training Loss: 3.5705\n",
      "1 epoch time: 7.022846412658692\n",
      "Epoch [84/95], Step [1/132], D Loss: 0.8132, G Loss: 3.3010\n",
      "Epoch [84/95], Step [6/132], D Loss: 0.5213, G Loss: 3.7009\n",
      "Epoch [84/95], Step [11/132], D Loss: 0.5032, G Loss: 3.9449\n",
      "Epoch [84/95], Step [16/132], D Loss: 0.5032, G Loss: 3.2081\n",
      "Epoch [84/95], Step [21/132], D Loss: 0.5048, G Loss: 4.1434\n",
      "Epoch [84/95], Step [26/132], D Loss: 0.5034, G Loss: 4.1367\n",
      "Epoch [84/95], Step [31/132], D Loss: 0.5216, G Loss: 3.8252\n",
      "Epoch [84/95], Step [36/132], D Loss: 0.5032, G Loss: 3.6874\n",
      "Epoch [84/95], Step [41/132], D Loss: 0.5032, G Loss: 3.2070\n",
      "Epoch [84/95], Step [46/132], D Loss: 0.5032, G Loss: 3.2203\n",
      "Epoch [84/95], Step [51/132], D Loss: 0.5032, G Loss: 3.7075\n",
      "Epoch [84/95], Step [56/132], D Loss: 0.5903, G Loss: 3.8624\n",
      "Epoch [84/95], Step [61/132], D Loss: 0.5032, G Loss: 3.1725\n",
      "Epoch [84/95], Step [66/132], D Loss: 0.5251, G Loss: 3.6170\n",
      "Epoch [84/95], Step [71/132], D Loss: 0.5043, G Loss: 3.4797\n",
      "Epoch [84/95], Step [76/132], D Loss: 0.5040, G Loss: 3.5268\n",
      "Epoch [84/95], Step [81/132], D Loss: 0.5032, G Loss: 3.6429\n",
      "Epoch [84/95], Step [86/132], D Loss: 0.5032, G Loss: 3.6168\n",
      "Epoch [84/95], Step [91/132], D Loss: 0.5977, G Loss: 3.2920\n",
      "Epoch [84/95], Step [96/132], D Loss: 0.5032, G Loss: 4.0247\n",
      "Epoch [84/95], Step [101/132], D Loss: 0.5032, G Loss: 3.5793\n",
      "Epoch [84/95], Step [106/132], D Loss: 0.6135, G Loss: 3.4258\n",
      "Epoch [84/95], Step [111/132], D Loss: 0.5032, G Loss: 3.4813\n",
      "Epoch [84/95], Step [116/132], D Loss: 0.5056, G Loss: 3.1287\n",
      "Epoch [84/95], Step [121/132], D Loss: 0.5844, G Loss: 3.3659\n",
      "Epoch [84/95], Step [126/132], D Loss: 0.5052, G Loss: 3.1029\n",
      "Epoch [84/95], Step [131/132], D Loss: 0.6995, G Loss: 3.4148\n",
      "Epoch [84/95], Training Loss: 3.5780\n",
      "1 epoch time: 7.043276449044545\n",
      "Epoch [85/95], Step [1/132], D Loss: 0.6045, G Loss: 3.6891\n",
      "Epoch [85/95], Step [6/132], D Loss: 0.7886, G Loss: 2.5174\n",
      "Epoch [85/95], Step [11/132], D Loss: 0.5032, G Loss: 3.5060\n",
      "Epoch [85/95], Step [16/132], D Loss: 0.5047, G Loss: 3.6993\n",
      "Epoch [85/95], Step [21/132], D Loss: 0.5032, G Loss: 3.2082\n",
      "Epoch [85/95], Step [26/132], D Loss: 0.5032, G Loss: 3.4929\n",
      "Epoch [85/95], Step [31/132], D Loss: 0.5071, G Loss: 3.7176\n",
      "Epoch [85/95], Step [36/132], D Loss: 0.6030, G Loss: 3.5274\n",
      "Epoch [85/95], Step [41/132], D Loss: 0.8133, G Loss: 3.0845\n",
      "Epoch [85/95], Step [46/132], D Loss: 0.8133, G Loss: 3.1950\n",
      "Epoch [85/95], Step [51/132], D Loss: 0.8133, G Loss: 3.1406\n",
      "Epoch [85/95], Step [56/132], D Loss: 0.8132, G Loss: 3.6785\n",
      "Epoch [85/95], Step [61/132], D Loss: 0.8130, G Loss: 3.1270\n",
      "Epoch [85/95], Step [66/132], D Loss: 0.7503, G Loss: 3.0601\n",
      "Epoch [85/95], Step [71/132], D Loss: 0.5171, G Loss: 3.3471\n",
      "Epoch [85/95], Step [76/132], D Loss: 0.5072, G Loss: 3.4734\n",
      "Epoch [85/95], Step [81/132], D Loss: 0.5032, G Loss: 3.7337\n",
      "Epoch [85/95], Step [86/132], D Loss: 0.5035, G Loss: 3.0545\n",
      "Epoch [85/95], Step [91/132], D Loss: 0.5032, G Loss: 3.3447\n",
      "Epoch [85/95], Step [96/132], D Loss: 0.5032, G Loss: 3.2949\n",
      "Epoch [85/95], Step [101/132], D Loss: 0.5806, G Loss: 3.5576\n",
      "Epoch [85/95], Step [106/132], D Loss: 0.5050, G Loss: 3.7623\n",
      "Epoch [85/95], Step [111/132], D Loss: 0.8132, G Loss: 3.9857\n",
      "Epoch [85/95], Step [116/132], D Loss: 0.8008, G Loss: 3.1315\n",
      "Epoch [85/95], Step [121/132], D Loss: 0.6804, G Loss: 3.8105\n",
      "Epoch [85/95], Step [126/132], D Loss: 0.5118, G Loss: 3.7499\n",
      "Epoch [85/95], Step [131/132], D Loss: 0.8712, G Loss: 3.4549\n",
      "Epoch [85/95], Training Loss: 3.4594\n",
      "1 epoch time: 7.037855315208435\n",
      "Epoch [86/95], Step [1/132], D Loss: 0.7222, G Loss: 3.4826\n",
      "Epoch [86/95], Step [6/132], D Loss: 0.7260, G Loss: 3.4434\n",
      "Epoch [86/95], Step [11/132], D Loss: 0.6918, G Loss: 4.2145\n",
      "Epoch [86/95], Step [16/132], D Loss: 0.5057, G Loss: 3.3096\n",
      "Epoch [86/95], Step [21/132], D Loss: 0.5054, G Loss: 3.6856\n",
      "Epoch [86/95], Step [26/132], D Loss: 0.5042, G Loss: 2.9299\n",
      "Epoch [86/95], Step [31/132], D Loss: 0.5034, G Loss: 3.3418\n",
      "Epoch [86/95], Step [36/132], D Loss: 0.5119, G Loss: 3.2404\n",
      "Epoch [86/95], Step [41/132], D Loss: 0.5089, G Loss: 4.1475\n",
      "Epoch [86/95], Step [46/132], D Loss: 0.5032, G Loss: 4.2025\n",
      "Epoch [86/95], Step [51/132], D Loss: 0.5033, G Loss: 3.5150\n",
      "Epoch [86/95], Step [56/132], D Loss: 0.5962, G Loss: 2.8475\n",
      "Epoch [86/95], Step [61/132], D Loss: 0.5032, G Loss: 3.4424\n",
      "Epoch [86/95], Step [66/132], D Loss: 0.5053, G Loss: 3.2906\n",
      "Epoch [86/95], Step [71/132], D Loss: 0.5033, G Loss: 2.9636\n",
      "Epoch [86/95], Step [76/132], D Loss: 0.5032, G Loss: 3.3378\n",
      "Epoch [86/95], Step [81/132], D Loss: 0.5035, G Loss: 2.9186\n",
      "Epoch [86/95], Step [86/132], D Loss: 0.5032, G Loss: 3.3647\n",
      "Epoch [86/95], Step [91/132], D Loss: 0.5032, G Loss: 4.7360\n",
      "Epoch [86/95], Step [96/132], D Loss: 0.5032, G Loss: 3.1847\n",
      "Epoch [86/95], Step [101/132], D Loss: 0.5032, G Loss: 3.7109\n",
      "Epoch [86/95], Step [106/132], D Loss: 0.5042, G Loss: 3.4151\n",
      "Epoch [86/95], Step [111/132], D Loss: 0.5032, G Loss: 3.5017\n",
      "Epoch [86/95], Step [116/132], D Loss: 0.5032, G Loss: 3.6871\n",
      "Epoch [86/95], Step [121/132], D Loss: 0.5032, G Loss: 3.7547\n",
      "Epoch [86/95], Step [126/132], D Loss: 0.5032, G Loss: 3.4631\n",
      "Epoch [86/95], Step [131/132], D Loss: 0.5032, G Loss: 3.7106\n",
      "Epoch [86/95], Training Loss: 3.5415\n",
      "1 epoch time: 7.012589041392008\n",
      "Epoch [87/95], Step [1/132], D Loss: 0.5033, G Loss: 3.7278\n",
      "Epoch [87/95], Step [6/132], D Loss: 0.5032, G Loss: 3.4882\n",
      "Epoch [87/95], Step [11/132], D Loss: 0.5033, G Loss: 3.4115\n",
      "Epoch [87/95], Step [16/132], D Loss: 0.5032, G Loss: 3.4408\n",
      "Epoch [87/95], Step [21/132], D Loss: 0.5032, G Loss: 3.4887\n",
      "Epoch [87/95], Step [26/132], D Loss: 0.5032, G Loss: 3.3582\n",
      "Epoch [87/95], Step [31/132], D Loss: 0.5032, G Loss: 4.4678\n",
      "Epoch [87/95], Step [36/132], D Loss: 0.5032, G Loss: 3.5107\n",
      "Epoch [87/95], Step [41/132], D Loss: 0.5032, G Loss: 3.3988\n",
      "Epoch [87/95], Step [46/132], D Loss: 0.5032, G Loss: 3.4834\n",
      "Epoch [87/95], Step [51/132], D Loss: 0.5032, G Loss: 3.3366\n",
      "Epoch [87/95], Step [56/132], D Loss: 0.5032, G Loss: 3.5535\n",
      "Epoch [87/95], Step [61/132], D Loss: 0.5035, G Loss: 4.1802\n",
      "Epoch [87/95], Step [66/132], D Loss: 0.5045, G Loss: 3.7674\n",
      "Epoch [87/95], Step [71/132], D Loss: 0.5032, G Loss: 3.2054\n",
      "Epoch [87/95], Step [76/132], D Loss: 0.5035, G Loss: 2.9410\n",
      "Epoch [87/95], Step [81/132], D Loss: 0.5032, G Loss: 3.6559\n",
      "Epoch [87/95], Step [86/132], D Loss: 0.5032, G Loss: 3.3539\n",
      "Epoch [87/95], Step [91/132], D Loss: 0.5032, G Loss: 3.4470\n",
      "Epoch [87/95], Step [96/132], D Loss: 0.5032, G Loss: 3.6059\n",
      "Epoch [87/95], Step [101/132], D Loss: 0.5032, G Loss: 3.3983\n",
      "Epoch [87/95], Step [106/132], D Loss: 0.5032, G Loss: 3.7684\n",
      "Epoch [87/95], Step [111/132], D Loss: 0.5033, G Loss: 3.0122\n",
      "Epoch [87/95], Step [116/132], D Loss: 0.5033, G Loss: 3.6245\n",
      "Epoch [87/95], Step [121/132], D Loss: 0.5032, G Loss: 3.7422\n",
      "Epoch [87/95], Step [126/132], D Loss: 0.5116, G Loss: 3.6882\n",
      "Epoch [87/95], Step [131/132], D Loss: 0.5032, G Loss: 3.1867\n",
      "Epoch [87/95], Training Loss: 3.5179\n",
      "1 epoch time: 7.0166772365570065\n",
      "Epoch [88/95], Step [1/132], D Loss: 0.5033, G Loss: 3.8882\n",
      "Epoch [88/95], Step [6/132], D Loss: 0.5032, G Loss: 3.4681\n",
      "Epoch [88/95], Step [11/132], D Loss: 0.5056, G Loss: 3.5007\n",
      "Epoch [88/95], Step [16/132], D Loss: 0.5032, G Loss: 3.4369\n",
      "Epoch [88/95], Step [21/132], D Loss: 0.5032, G Loss: 3.2078\n",
      "Epoch [88/95], Step [26/132], D Loss: 0.5049, G Loss: 3.1508\n",
      "Epoch [88/95], Step [31/132], D Loss: 0.5032, G Loss: 3.7843\n",
      "Epoch [88/95], Step [36/132], D Loss: 0.5032, G Loss: 3.1457\n",
      "Epoch [88/95], Step [41/132], D Loss: 0.5048, G Loss: 3.0937\n",
      "Epoch [88/95], Step [46/132], D Loss: 0.5032, G Loss: 3.0216\n",
      "Epoch [88/95], Step [51/132], D Loss: 0.5032, G Loss: 3.2321\n",
      "Epoch [88/95], Step [56/132], D Loss: 0.5041, G Loss: 3.4084\n",
      "Epoch [88/95], Step [61/132], D Loss: 0.5032, G Loss: 3.4957\n",
      "Epoch [88/95], Step [66/132], D Loss: 0.5032, G Loss: 3.6737\n",
      "Epoch [88/95], Step [71/132], D Loss: 0.5032, G Loss: 3.4894\n",
      "Epoch [88/95], Step [76/132], D Loss: 0.5032, G Loss: 3.6716\n",
      "Epoch [88/95], Step [81/132], D Loss: 0.5032, G Loss: 3.1778\n",
      "Epoch [88/95], Step [86/132], D Loss: 0.5032, G Loss: 3.4303\n",
      "Epoch [88/95], Step [91/132], D Loss: 0.5032, G Loss: 3.0126\n",
      "Epoch [88/95], Step [96/132], D Loss: 0.5032, G Loss: 3.6467\n",
      "Epoch [88/95], Step [101/132], D Loss: 0.5086, G Loss: 3.3250\n",
      "Epoch [88/95], Step [106/132], D Loss: 0.5032, G Loss: 3.8298\n",
      "Epoch [88/95], Step [111/132], D Loss: 0.5034, G Loss: 3.3906\n",
      "Epoch [88/95], Step [116/132], D Loss: 0.5032, G Loss: 3.3299\n",
      "Epoch [88/95], Step [121/132], D Loss: 0.5032, G Loss: 3.4622\n",
      "Epoch [88/95], Step [126/132], D Loss: 0.5032, G Loss: 3.7993\n",
      "Epoch [88/95], Step [131/132], D Loss: 0.5032, G Loss: 4.1156\n",
      "Epoch [88/95], Training Loss: 3.5326\n",
      "1 epoch time: 6.999684711297353\n",
      "Epoch [89/95], Step [1/132], D Loss: 0.5032, G Loss: 4.1877\n",
      "Epoch [89/95], Step [6/132], D Loss: 0.5984, G Loss: 3.7468\n",
      "Epoch [89/95], Step [11/132], D Loss: 0.7082, G Loss: 3.6368\n",
      "Epoch [89/95], Step [16/132], D Loss: 0.5036, G Loss: 3.2650\n",
      "Epoch [89/95], Step [21/132], D Loss: 0.5032, G Loss: 3.5476\n",
      "Epoch [89/95], Step [26/132], D Loss: 0.8133, G Loss: 2.9738\n",
      "Epoch [89/95], Step [31/132], D Loss: 0.8131, G Loss: 2.8735\n",
      "Epoch [89/95], Step [36/132], D Loss: 0.8125, G Loss: 3.4493\n",
      "Epoch [89/95], Step [41/132], D Loss: 0.7341, G Loss: 3.2445\n",
      "Epoch [89/95], Step [46/132], D Loss: 0.6751, G Loss: 3.3806\n",
      "Epoch [89/95], Step [51/132], D Loss: 0.6930, G Loss: 3.5253\n",
      "Epoch [89/95], Step [56/132], D Loss: 0.6931, G Loss: 3.3650\n",
      "Epoch [89/95], Step [61/132], D Loss: 0.6931, G Loss: 3.2616\n",
      "Epoch [89/95], Step [66/132], D Loss: 0.6931, G Loss: 3.2854\n",
      "Epoch [89/95], Step [71/132], D Loss: 0.6931, G Loss: 3.7059\n",
      "Epoch [89/95], Step [76/132], D Loss: 0.6931, G Loss: 3.0454\n",
      "Epoch [89/95], Step [81/132], D Loss: 0.6931, G Loss: 3.5964\n",
      "Epoch [89/95], Step [86/132], D Loss: 0.6931, G Loss: 3.6082\n",
      "Epoch [89/95], Step [91/132], D Loss: 0.6931, G Loss: 3.9126\n",
      "Epoch [89/95], Step [96/132], D Loss: 0.6931, G Loss: 3.6191\n",
      "Epoch [89/95], Step [101/132], D Loss: 0.6931, G Loss: 3.3795\n",
      "Epoch [89/95], Step [106/132], D Loss: 0.6931, G Loss: 3.8795\n",
      "Epoch [89/95], Step [111/132], D Loss: 0.6931, G Loss: 3.0977\n",
      "Epoch [89/95], Step [116/132], D Loss: 0.6931, G Loss: 3.2581\n",
      "Epoch [89/95], Step [121/132], D Loss: 0.6931, G Loss: 3.2690\n",
      "Epoch [89/95], Step [126/132], D Loss: 0.6931, G Loss: 3.5375\n",
      "Epoch [89/95], Step [131/132], D Loss: 0.6931, G Loss: 3.5127\n",
      "Epoch [89/95], Training Loss: 3.4569\n",
      "1 epoch time: 7.101560445626577\n",
      "Epoch [90/95], Step [1/132], D Loss: 0.6929, G Loss: 3.2907\n",
      "Epoch [90/95], Step [6/132], D Loss: 0.6913, G Loss: 3.1538\n",
      "Epoch [90/95], Step [11/132], D Loss: 0.6832, G Loss: 3.5586\n",
      "Epoch [90/95], Step [16/132], D Loss: 0.6293, G Loss: 3.6930\n",
      "Epoch [90/95], Step [21/132], D Loss: 0.6751, G Loss: 3.6045\n",
      "Epoch [90/95], Step [26/132], D Loss: 0.5274, G Loss: 3.8034\n",
      "Epoch [90/95], Step [31/132], D Loss: 0.5380, G Loss: 3.3648\n",
      "Epoch [90/95], Step [36/132], D Loss: 0.5101, G Loss: 3.9398\n",
      "Epoch [90/95], Step [41/132], D Loss: 0.5032, G Loss: 3.6691\n",
      "Epoch [90/95], Step [46/132], D Loss: 0.5032, G Loss: 3.5956\n",
      "Epoch [90/95], Step [51/132], D Loss: 0.5033, G Loss: 3.0924\n",
      "Epoch [90/95], Step [56/132], D Loss: 0.5035, G Loss: 3.7286\n",
      "Epoch [90/95], Step [61/132], D Loss: 0.5035, G Loss: 2.6548\n",
      "Epoch [90/95], Step [66/132], D Loss: 0.5032, G Loss: 3.7210\n",
      "Epoch [90/95], Step [71/132], D Loss: 0.5032, G Loss: 3.3178\n",
      "Epoch [90/95], Step [76/132], D Loss: 0.5032, G Loss: 3.0930\n",
      "Epoch [90/95], Step [81/132], D Loss: 0.5032, G Loss: 5.1988\n",
      "Epoch [90/95], Step [86/132], D Loss: 0.5032, G Loss: 3.5177\n",
      "Epoch [90/95], Step [91/132], D Loss: 0.5032, G Loss: 3.2772\n",
      "Epoch [90/95], Step [96/132], D Loss: 0.5260, G Loss: 3.2503\n",
      "Epoch [90/95], Step [101/132], D Loss: 0.7151, G Loss: 3.3502\n",
      "Epoch [90/95], Step [106/132], D Loss: 0.6931, G Loss: 3.0116\n",
      "Epoch [90/95], Step [111/132], D Loss: 0.6808, G Loss: 4.0158\n",
      "Epoch [90/95], Step [116/132], D Loss: 0.6663, G Loss: 3.1633\n",
      "Epoch [90/95], Step [121/132], D Loss: 0.5538, G Loss: 4.0422\n",
      "Epoch [90/95], Step [126/132], D Loss: 0.6050, G Loss: 3.4063\n",
      "Epoch [90/95], Step [131/132], D Loss: 0.5432, G Loss: 3.5864\n",
      "Epoch [90/95], Training Loss: 3.5194\n",
      "1 epoch time: 7.323728895187378\n",
      "Epoch [91/95], Step [1/132], D Loss: 0.8132, G Loss: 3.1313\n",
      "Epoch [91/95], Step [6/132], D Loss: 0.5033, G Loss: 3.6436\n",
      "Epoch [91/95], Step [11/132], D Loss: 0.5032, G Loss: 3.5528\n",
      "Epoch [91/95], Step [16/132], D Loss: 0.5797, G Loss: 3.7693\n",
      "Epoch [91/95], Step [21/132], D Loss: 0.5040, G Loss: 3.1641\n",
      "Epoch [91/95], Step [26/132], D Loss: 0.5032, G Loss: 3.2530\n",
      "Epoch [91/95], Step [31/132], D Loss: 0.5032, G Loss: 3.3932\n",
      "Epoch [91/95], Step [36/132], D Loss: 0.5032, G Loss: 3.6665\n",
      "Epoch [91/95], Step [41/132], D Loss: 0.5032, G Loss: 3.6245\n",
      "Epoch [91/95], Step [46/132], D Loss: 0.5032, G Loss: 3.4022\n",
      "Epoch [91/95], Step [51/132], D Loss: 0.5096, G Loss: 3.5577\n",
      "Epoch [91/95], Step [56/132], D Loss: 0.5032, G Loss: 3.8546\n",
      "Epoch [91/95], Step [61/132], D Loss: 0.5032, G Loss: 4.6601\n",
      "Epoch [91/95], Step [66/132], D Loss: 0.5032, G Loss: 3.5644\n",
      "Epoch [91/95], Step [71/132], D Loss: 0.5032, G Loss: 3.2886\n",
      "Epoch [91/95], Step [76/132], D Loss: 0.5032, G Loss: 3.3164\n",
      "Epoch [91/95], Step [81/132], D Loss: 0.5032, G Loss: 3.0091\n",
      "Epoch [91/95], Step [86/132], D Loss: 0.5032, G Loss: 3.7584\n",
      "Epoch [91/95], Step [91/132], D Loss: 0.5032, G Loss: 4.1385\n",
      "Epoch [91/95], Step [96/132], D Loss: 0.5032, G Loss: 4.3672\n",
      "Epoch [91/95], Step [101/132], D Loss: 0.5032, G Loss: 4.3891\n",
      "Epoch [91/95], Step [106/132], D Loss: 0.5032, G Loss: 3.2812\n",
      "Epoch [91/95], Step [111/132], D Loss: 0.5032, G Loss: 3.1880\n",
      "Epoch [91/95], Step [116/132], D Loss: 0.5032, G Loss: 3.5909\n",
      "Epoch [91/95], Step [121/132], D Loss: 0.5032, G Loss: 3.6595\n",
      "Epoch [91/95], Step [126/132], D Loss: 0.5032, G Loss: 3.0967\n",
      "Epoch [91/95], Step [131/132], D Loss: 0.5049, G Loss: 2.9815\n",
      "Epoch [91/95], Training Loss: 3.5066\n",
      "1 epoch time: 7.168714165687561\n",
      "Epoch [92/95], Step [1/132], D Loss: 0.5032, G Loss: 4.2541\n",
      "Epoch [92/95], Step [6/132], D Loss: 0.5032, G Loss: 3.6608\n",
      "Epoch [92/95], Step [11/132], D Loss: 0.5032, G Loss: 2.9915\n",
      "Epoch [92/95], Step [16/132], D Loss: 0.5032, G Loss: 3.9106\n",
      "Epoch [92/95], Step [21/132], D Loss: 0.5032, G Loss: 3.8149\n",
      "Epoch [92/95], Step [26/132], D Loss: 0.5045, G Loss: 3.7010\n",
      "Epoch [92/95], Step [31/132], D Loss: 0.5090, G Loss: 3.2310\n",
      "Epoch [92/95], Step [36/132], D Loss: 0.5032, G Loss: 3.4082\n",
      "Epoch [92/95], Step [41/132], D Loss: 0.5057, G Loss: 3.1579\n",
      "Epoch [92/95], Step [46/132], D Loss: 0.5033, G Loss: 3.4322\n",
      "Epoch [92/95], Step [51/132], D Loss: 0.5032, G Loss: 3.1109\n",
      "Epoch [92/95], Step [56/132], D Loss: 0.8133, G Loss: 2.6207\n",
      "Epoch [92/95], Step [61/132], D Loss: 0.5043, G Loss: 3.8640\n",
      "Epoch [92/95], Step [66/132], D Loss: 0.5032, G Loss: 3.0902\n",
      "Epoch [92/95], Step [71/132], D Loss: 0.5032, G Loss: 3.4126\n",
      "Epoch [92/95], Step [76/132], D Loss: 0.5032, G Loss: 3.4255\n",
      "Epoch [92/95], Step [81/132], D Loss: 0.5045, G Loss: 3.6782\n",
      "Epoch [92/95], Step [86/132], D Loss: 0.5032, G Loss: 3.5958\n",
      "Epoch [92/95], Step [91/132], D Loss: 0.5032, G Loss: 3.4851\n",
      "Epoch [92/95], Step [96/132], D Loss: 0.5032, G Loss: 3.8682\n",
      "Epoch [92/95], Step [101/132], D Loss: 0.5032, G Loss: 3.7331\n",
      "Epoch [92/95], Step [106/132], D Loss: 0.5032, G Loss: 2.7246\n",
      "Epoch [92/95], Step [111/132], D Loss: 0.5032, G Loss: 3.6845\n",
      "Epoch [92/95], Step [116/132], D Loss: 0.5032, G Loss: 3.4306\n",
      "Epoch [92/95], Step [121/132], D Loss: 0.5032, G Loss: 3.6209\n",
      "Epoch [92/95], Step [126/132], D Loss: 0.5032, G Loss: 3.1476\n",
      "Epoch [92/95], Step [131/132], D Loss: 0.5044, G Loss: 3.1825\n",
      "Epoch [92/95], Training Loss: 3.4871\n",
      "1 epoch time: 7.18046586116155\n",
      "Epoch [93/95], Step [1/132], D Loss: 0.5032, G Loss: 3.1240\n",
      "Epoch [93/95], Step [6/132], D Loss: 0.5032, G Loss: 3.6028\n",
      "Epoch [93/95], Step [11/132], D Loss: 0.5032, G Loss: 3.5198\n",
      "Epoch [93/95], Step [16/132], D Loss: 0.5032, G Loss: 3.5722\n",
      "Epoch [93/95], Step [21/132], D Loss: 0.5032, G Loss: 3.2427\n",
      "Epoch [93/95], Step [26/132], D Loss: 0.5032, G Loss: 3.8076\n",
      "Epoch [93/95], Step [31/132], D Loss: 0.5032, G Loss: 4.1546\n",
      "Epoch [93/95], Step [36/132], D Loss: 0.5032, G Loss: 3.0886\n",
      "Epoch [93/95], Step [41/132], D Loss: 0.5032, G Loss: 3.9665\n",
      "Epoch [93/95], Step [46/132], D Loss: 0.5032, G Loss: 4.0515\n",
      "Epoch [93/95], Step [51/132], D Loss: 0.5046, G Loss: 2.9994\n",
      "Epoch [93/95], Step [56/132], D Loss: 0.5032, G Loss: 3.3316\n",
      "Epoch [93/95], Step [61/132], D Loss: 0.5032, G Loss: 3.2902\n",
      "Epoch [93/95], Step [66/132], D Loss: 0.5033, G Loss: 4.0064\n",
      "Epoch [93/95], Step [71/132], D Loss: 0.5032, G Loss: 4.1102\n",
      "Epoch [93/95], Step [76/132], D Loss: 0.5032, G Loss: 3.6515\n",
      "Epoch [93/95], Step [81/132], D Loss: 0.5032, G Loss: 4.1502\n",
      "Epoch [93/95], Step [86/132], D Loss: 0.5032, G Loss: 3.3978\n",
      "Epoch [93/95], Step [91/132], D Loss: 0.5032, G Loss: 3.6911\n",
      "Epoch [93/95], Step [96/132], D Loss: 0.5032, G Loss: 2.9954\n",
      "Epoch [93/95], Step [101/132], D Loss: 0.5032, G Loss: 3.2920\n",
      "Epoch [93/95], Step [106/132], D Loss: 0.5032, G Loss: 2.7592\n",
      "Epoch [93/95], Step [111/132], D Loss: 0.5032, G Loss: 3.3054\n",
      "Epoch [93/95], Step [116/132], D Loss: 0.5032, G Loss: 3.3514\n",
      "Epoch [93/95], Step [121/132], D Loss: 0.5032, G Loss: 3.5938\n",
      "Epoch [93/95], Step [126/132], D Loss: 0.5032, G Loss: 4.0490\n",
      "Epoch [93/95], Step [131/132], D Loss: 0.5032, G Loss: 4.1718\n",
      "Epoch [93/95], Training Loss: 3.5219\n",
      "1 epoch time: 7.302182491620382\n",
      "Epoch [94/95], Step [1/132], D Loss: 0.5032, G Loss: 3.4284\n",
      "Epoch [94/95], Step [6/132], D Loss: 0.5032, G Loss: 3.2627\n",
      "Epoch [94/95], Step [11/132], D Loss: 0.5032, G Loss: 3.1380\n",
      "Epoch [94/95], Step [16/132], D Loss: 0.5032, G Loss: 3.5376\n",
      "Epoch [94/95], Step [21/132], D Loss: 0.5032, G Loss: 3.4022\n",
      "Epoch [94/95], Step [26/132], D Loss: 0.5032, G Loss: 3.3233\n",
      "Epoch [94/95], Step [31/132], D Loss: 0.5032, G Loss: 3.3012\n",
      "Epoch [94/95], Step [36/132], D Loss: 0.5032, G Loss: 3.2887\n",
      "Epoch [94/95], Step [41/132], D Loss: 0.5032, G Loss: 3.5127\n",
      "Epoch [94/95], Step [46/132], D Loss: 0.5032, G Loss: 3.6572\n",
      "Epoch [94/95], Step [51/132], D Loss: 0.5032, G Loss: 3.0427\n",
      "Epoch [94/95], Step [56/132], D Loss: 0.5032, G Loss: 4.2309\n",
      "Epoch [94/95], Step [61/132], D Loss: 0.5032, G Loss: 3.2675\n",
      "Epoch [94/95], Step [66/132], D Loss: 0.5032, G Loss: 3.4280\n",
      "Epoch [94/95], Step [71/132], D Loss: 0.5032, G Loss: 3.6365\n",
      "Epoch [94/95], Step [76/132], D Loss: 0.5032, G Loss: 3.0230\n",
      "Epoch [94/95], Step [81/132], D Loss: 0.5032, G Loss: 3.4442\n",
      "Epoch [94/95], Step [86/132], D Loss: 0.5032, G Loss: 3.6655\n",
      "Epoch [94/95], Step [91/132], D Loss: 0.5032, G Loss: 3.9543\n",
      "Epoch [94/95], Step [96/132], D Loss: 0.5032, G Loss: 2.9262\n",
      "Epoch [94/95], Step [101/132], D Loss: 0.5032, G Loss: 3.4281\n",
      "Epoch [94/95], Step [106/132], D Loss: 0.5032, G Loss: 3.2457\n",
      "Epoch [94/95], Step [111/132], D Loss: 0.5032, G Loss: 3.3696\n",
      "Epoch [94/95], Step [116/132], D Loss: 0.5032, G Loss: 3.3534\n",
      "Epoch [94/95], Step [121/132], D Loss: 0.5032, G Loss: 3.9914\n",
      "Epoch [94/95], Step [126/132], D Loss: 0.5291, G Loss: 3.3749\n",
      "Epoch [94/95], Step [131/132], D Loss: 0.5245, G Loss: 3.5585\n",
      "Epoch [94/95], Training Loss: 3.4724\n",
      "1 epoch time: 7.247833887736003\n",
      "Epoch [95/95], Step [1/132], D Loss: 0.5032, G Loss: 3.7242\n",
      "Epoch [95/95], Step [6/132], D Loss: 0.5032, G Loss: 3.3812\n",
      "Epoch [95/95], Step [11/132], D Loss: 0.5033, G Loss: 2.7308\n",
      "Epoch [95/95], Step [16/132], D Loss: 0.5906, G Loss: 2.8862\n",
      "Epoch [95/95], Step [21/132], D Loss: 0.5032, G Loss: 3.2921\n",
      "Epoch [95/95], Step [26/132], D Loss: 0.5066, G Loss: 3.5684\n",
      "Epoch [95/95], Step [31/132], D Loss: 0.5032, G Loss: 3.1336\n",
      "Epoch [95/95], Step [36/132], D Loss: 0.5032, G Loss: 3.7276\n",
      "Epoch [95/95], Step [41/132], D Loss: 0.5032, G Loss: 3.7297\n",
      "Epoch [95/95], Step [46/132], D Loss: 0.5032, G Loss: 3.3416\n",
      "Epoch [95/95], Step [51/132], D Loss: 0.5032, G Loss: 3.3468\n",
      "Epoch [95/95], Step [56/132], D Loss: 0.5032, G Loss: 3.4547\n",
      "Epoch [95/95], Step [61/132], D Loss: 0.5032, G Loss: 3.0353\n",
      "Epoch [95/95], Step [66/132], D Loss: 0.5032, G Loss: 3.4630\n",
      "Epoch [95/95], Step [71/132], D Loss: 0.5032, G Loss: 3.9943\n",
      "Epoch [95/95], Step [76/132], D Loss: 0.6736, G Loss: 2.9178\n",
      "Epoch [95/95], Step [81/132], D Loss: 0.5037, G Loss: 3.2666\n",
      "Epoch [95/95], Step [86/132], D Loss: 0.5032, G Loss: 3.2434\n",
      "Epoch [95/95], Step [91/132], D Loss: 0.5032, G Loss: 3.4359\n",
      "Epoch [95/95], Step [96/132], D Loss: 0.5032, G Loss: 3.0908\n",
      "Epoch [95/95], Step [101/132], D Loss: 0.5032, G Loss: 3.2380\n",
      "Epoch [95/95], Step [106/132], D Loss: 0.5032, G Loss: 3.5557\n",
      "Epoch [95/95], Step [111/132], D Loss: 0.5032, G Loss: 2.8205\n",
      "Epoch [95/95], Step [116/132], D Loss: 0.5032, G Loss: 3.6390\n",
      "Epoch [95/95], Step [121/132], D Loss: 0.5032, G Loss: 3.3084\n",
      "Epoch [95/95], Step [126/132], D Loss: 0.5032, G Loss: 3.7789\n",
      "Epoch [95/95], Step [131/132], D Loss: 0.5032, G Loss: 3.2988\n",
      "Epoch [95/95], Training Loss: 3.4851\n",
      "1 epoch time: 7.181275737285614\n",
      "Training complete!\n",
      "Best generator checkpoint: /kaggle/working/generatorepoch89.pth with Training Loss: 3.4569\n",
      "Best discriminator checkpoint: /kaggle/working/discriminatorepoch89.pth\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"üñ•Ô∏è Using {torch.cuda.device_count()} GPUs!\")\n",
    "    generator = nn.DataParallel(generator)\n",
    "    discriminator = nn.DataParallel(discriminator)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "train_dataset = DeblurDataset(root_dir='/kaggle/input/gopro/GOPRO_Large/train')\n",
    "test_dataset = DeblurDataset(root_dir='/kaggle/input/gopro/GOPRO_Large/test')  \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "pixel_loss = nn.L1Loss()\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "training_losses = []\n",
    "num_epochs = 95\n",
    "best_checkpoint_generator = None\n",
    "lowest_train_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    generator.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (blurred, sharp) in enumerate(train_loader):\n",
    "        blurred, sharp = blurred.to(device), sharp.to(device)\n",
    "\n",
    "        real_output = discriminator(sharp)\n",
    "        fake_images = generator(blurred)\n",
    "        fake_output = discriminator(fake_images.detach())\n",
    "\n",
    "        real_labels = torch.ones_like(real_output, device=device)\n",
    "        fake_labels = torch.zeros_like(fake_output, device=device)\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss = (adversarial_loss(real_output, real_labels) +\n",
    "                  adversarial_loss(fake_output, fake_labels)) / 2\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        fake_output = discriminator(fake_images)\n",
    "        g_adv_loss = adversarial_loss(fake_output, real_labels)\n",
    "        g_pix_loss = pixel_loss(fake_images, sharp)\n",
    "        g_loss = g_adv_loss + 100 * g_pix_loss\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        train_loss += g_loss.item()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], \"\n",
    "                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = train_loss / len(train_loader)\n",
    "    training_losses.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    end_time = time.time()  \n",
    "    elapsed_time = (end_time - start_time) / 60  \n",
    "    print(f\"1 epoch time: {elapsed_time}\")\n",
    "\n",
    "    checkpoint_generator = f\"/kaggle/working/generatorepoch{epoch+1}.pth\"\n",
    "    torch.save(generator.state_dict(), checkpoint_generator)\n",
    "\n",
    "    checkpoint_discriminator = f\"/kaggle/working/discriminatorepoch{epoch+1}.pth\"\n",
    "    torch.save(discriminator.state_dict(), checkpoint_discriminator)\n",
    "    \n",
    "    if epoch_loss < lowest_train_loss:\n",
    "        lowest_train_loss = epoch_loss\n",
    "        best_checkpoint_generator = checkpoint_generator\n",
    "        best_checkpoint_discriminator = checkpoint_discriminator\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best generator checkpoint: {best_checkpoint_generator} with Training Loss: {lowest_train_loss:.4f}\")\n",
    "print(f\"Best discriminator checkpoint: {best_checkpoint_discriminator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "517629a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T18:43:13.005927Z",
     "iopub.status.busy": "2025-04-28T18:43:13.005556Z",
     "iopub.status.idle": "2025-04-28T18:43:13.428661Z",
     "shell.execute_reply": "2025-04-28T18:43:13.427819Z"
    },
    "papermill": {
     "duration": 0.556171,
     "end_time": "2025-04-28T18:43:13.430884",
     "exception": false,
     "start_time": "2025-04-28T18:43:12.874713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"training_loss_plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb4d91c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T18:43:13.698063Z",
     "iopub.status.busy": "2025-04-28T18:43:13.697674Z",
     "iopub.status.idle": "2025-04-28T18:46:40.964524Z",
     "shell.execute_reply": "2025-04-28T18:46:40.963495Z"
    },
    "papermill": {
     "duration": 207.537538,
     "end_time": "2025-04-28T18:46:41.101163",
     "exception": false,
     "start_time": "2025-04-28T18:43:13.563625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2905787750.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(best_checkpoint_generator, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR: 27.24\n",
      "Average SSIM: 0.8568\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    generator = nn.DataParallel(generator)\n",
    "generator = generator.to(device)\n",
    "generator.load_state_dict(torch.load(best_checkpoint_generator, map_location=device))\n",
    "generator.eval()\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "\n",
    "def calculate_ssim(img1, img2, win_size=7):\n",
    "    return ssim(img1, img2, channel_axis=2, win_size=win_size, data_range=1.0)\n",
    "\n",
    "total_psnr = 0.0\n",
    "total_ssim = 0.0\n",
    "count = 0\n",
    "\n",
    "for blurred, sharp in test_loader:\n",
    "    blurred = blurred.to(device)\n",
    "    sharp = sharp.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = generator(blurred)\n",
    "\n",
    "    output_resized = np.transpose(output.cpu().numpy()[0], (1, 2, 0))\n",
    "    sharp_resized = np.transpose(sharp.cpu().numpy()[0], (1, 2, 0))\n",
    "    output_resized = np.clip(output_resized, 0, 1)\n",
    "    sharp_resized = np.clip(sharp_resized, 0, 1)\n",
    "\n",
    "    psnr_val = calculate_psnr(output_resized, sharp_resized)\n",
    "    win_size = min(11, min(output_resized.shape[0], output_resized.shape[1]) - 1)\n",
    "    ssim_val = calculate_ssim(output_resized, sharp_resized, win_size=win_size)\n",
    "\n",
    "    total_psnr += psnr_val\n",
    "    total_ssim += ssim_val\n",
    "    count += 1\n",
    "\n",
    "print(f\"Average PSNR: {total_psnr / count:.2f}\")\n",
    "print(f\"Average SSIM: {total_ssim / count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb799458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T18:46:41.367058Z",
     "iopub.status.busy": "2025-04-28T18:46:41.366669Z",
     "iopub.status.idle": "2025-04-28T18:46:41.991720Z",
     "shell.execute_reply": "2025-04-28T18:46:41.990635Z"
    },
    "papermill": {
     "duration": 0.760781,
     "end_time": "2025-04-28T18:46:41.993875",
     "exception": false,
     "start_time": "2025-04-28T18:46:41.233094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1019843964.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(best_checkpoint_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = Generator()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    generator = torch.nn.DataParallel(generator)\n",
    "generator = generator.to(device)\n",
    "\n",
    "best_checkpoint_path = best_checkpoint_generator  \n",
    "generator.load_state_dict(torch.load(best_checkpoint_path, map_location=device))\n",
    "generator.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((360, 640)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "img_path = \"/kaggle/input/gopro/GOPRO_Large/test/GOPR0384_11_00/blur/000001.png\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "input_tensor = transform(img).unsqueeze(0).to(device)  \n",
    "\n",
    "with torch.no_grad():\n",
    "    output_tensor = generator(input_tensor)\n",
    "\n",
    "output_tensor = output_tensor.squeeze(0).cpu()  \n",
    "output_img = transforms.ToPILImage()(output_tensor.clamp(0, 1))\n",
    "\n",
    "output_img.save(\"deblurred_output.png\")\n",
    "output_img.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c59dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T18:46:42.279473Z",
     "iopub.status.busy": "2025-04-28T18:46:42.279128Z",
     "iopub.status.idle": "2025-04-28T18:46:42.870180Z",
     "shell.execute_reply": "2025-04-28T18:46:42.869231Z"
    },
    "papermill": {
     "duration": 0.728983,
     "end_time": "2025-04-28T18:46:42.873882",
     "exception": false,
     "start_time": "2025-04-28T18:46:42.144899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_for_show = transforms.ToTensor()(img.resize((640, 360)))\n",
    "input_for_show = input_for_show * 0.5 + 0.5\n",
    "\n",
    "input_img = transforms.ToPILImage()(input_for_show)\n",
    "combined = Image.new('RGB', (640 * 2, 360))\n",
    "combined.paste(input_img, (0, 0))\n",
    "combined.paste(output_img, (640, 0))\n",
    "combined.save(\"compare_blur_vs_deblur.png\")\n",
    "combined.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 627736,
     "sourceId": 1118216,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1899266,
     "sourceId": 3111677,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6287659,
     "sourceId": 10179289,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6425456,
     "sourceId": 10373010,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40451.367292,
   "end_time": "2025-04-28T18:46:46.415038",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-28T07:32:35.047746",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
